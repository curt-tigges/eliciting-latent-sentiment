{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bb195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t    miniconda.sh  quick_start_pytorch.ipynb   wandb\n",
      "eliciting-latent-sentiment  miniconda3\t  quick_start_pytorch_images\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a140d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc336ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://github.com/ojh31/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/ojh31/CircuitsVis.git to /tmp/pip-req-build-pj1dvx7q\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ojh31/CircuitsVis.git /tmp/pip-req-build-pj1dvx7q\n",
      "  Resolved https://github.com/ojh31/CircuitsVis.git to commit c6dcf2ed4755e5c6bcd5897582bf42baf0a1fbac\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (5.2.0)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->circuitsvis==0.0.0) (4.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: fancy_einsum==0.0.3 in /usr/local/lib/python3.9/dist-packages (0.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.9/dist-packages (1.6.1)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.12.1+cu116)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.14.5)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.15.11)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.23.4)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.6.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.33.2)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.2.13)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.17.3)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.1.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (66.1.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (5.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (3.11.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: jaxtyping==0.2.13 in /usr/local/lib/python3.9/dist-packages (0.2.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.23.4)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping==0.2.13) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping==0.2.13) (3.11.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.9/dist-packages (3.20.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (5.17.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torchtyping in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (1.12.1+cu116)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (4.1.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.11.1->torchtyping) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.11.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-mlxc_sqm\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-mlxc_sqm\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.17.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.9/dist-packages (0.0.0)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.12.1+cu116)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.23.4)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->circuitsvis) (4.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m\u001b[31m================================================================================\u001b[m\n",
      "\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n",
      "\u001b[1m\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "  \u001b[1m\u001b[33m                         \u001b[4mSCRIPT DEPRECATION WARNING\u001b[m                    \u001b[m\n",
      "\n",
      "  \n",
      "  This script, located at \u001b[1mhttps://deb.nodesource.com/setup_X\u001b[m, used to\n",
      "  install Node.js is deprecated now and will eventually be made inactive.\n",
      "\n",
      "  Please visit the NodeSource \u001b[1mdistributions\u001b[m Github and follow the\n",
      "  instructions to migrate your repo.\n",
      "  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
      "\n",
      "  The \u001b[1mNodeSource\u001b[m Node.js Linux distributions GitHub repository contains\n",
      "  information about which versions of Node.js and which Linux distributions\n",
      "  are supported and how to install it.\n",
      "  \u001b[4m\u001b[32m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
      "\n",
      "\n",
      "                          \u001b[4m\u001b[1m\u001b[33mSCRIPT DEPRECATION WARNING\u001b[m\n",
      "\n",
      "\u001b[1m\u001b[31m================================================================================\u001b[m\n",
      "\u001b[1m\u001b[31m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[m\n",
      "\u001b[1m\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "\u001b[36m\u001b[1mTO AVOID THIS WAIT MIGRATE THE SCRIPT\u001b[m\n",
      "Continuing in 60 seconds (press Ctrl-C to abort) ...\n",
      "\n",
      "\n",
      "## Installing the NodeSource Node.js 16.x repo...\n",
      "\n",
      "\n",
      "## Populating apt-get cache...\n",
      "\n",
      "+ apt-get update\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3065 kB]\n",
      "Get:7 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]            \n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2826 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3552 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1110 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1414 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2977 kB]\n",
      "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1185 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.1 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:18 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [776 B]  \n",
      "Get:19 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
      "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
      "Fetched 16.7 MB in 3s (5660 kB/s)    \n",
      "Reading package lists... Done\n",
      "\n",
      "## Confirming \"focal\" is supported...\n",
      "\n",
      "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/focal/Release'\n",
      "\n",
      "## Adding the NodeSource signing key to your keyring...\n",
      "\n",
      "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n",
      "\n",
      "## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n",
      "\n",
      "+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' > /etc/apt/sources.list.d/nodesource.list\n",
      "+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' >> /etc/apt/sources.list.d/nodesource.list\n",
      "\n",
      "## Running `apt-get update` for you...\n",
      "\n",
      "+ apt-get update\n",
      "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \n",
      "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:6 https://deb.nodesource.com/node_16.x focal InRelease                     \n",
      "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Reading package lists... Done\n",
      "\n",
      "## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n",
      "## You may also need development tools to build native addons:\n",
      "     sudo apt-get install gcc g++ make\n",
      "## To install the Yarn package manager, run:\n",
      "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n",
      "     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
      "     sudo apt-get update && sudo apt-get install yarn\n",
      "\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages will be upgraded:\n",
      "  nodejs\n",
      "1 upgraded, 0 newly installed, 0 to remove and 151 not upgraded.\n",
      "Need to get 27.2 MB of archives.\n",
      "After this operation, 554 kB disk space will be freed.\n",
      "Get:1 https://deb.nodesource.com/node_16.x focal/main amd64 nodejs amd64 16.20.2-deb-1nodesource1 [27.2 MB]\n",
      "Fetched 27.2 MB in 1s (53.5 MB/s)\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../nodejs_16.20.2-deb-1nodesource1_amd64.deb ...\n",
      "Unpacking nodejs (16.20.2-deb-1nodesource1) over (16.19.0-deb-1nodesource1) ...\n",
      "Setting up nodejs (16.20.2-deb-1nodesource1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ojh31/CircuitsVis.git#subdirectory=python\n",
    "!pip install fancy_einsum==0.0.3\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "!pip install circuitsvis\n",
    "!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "# %pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ee7590e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import einops\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.evals import make_owt_data_loader\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from fancy_einsum import einsum\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text, to_csv, get_csv\n",
    "from utils.neuroscope import (\n",
    "    plot_neuroscope, get_dataloader, get_projections_for_text, plot_top_p, plot_topk, \n",
    "    harry_potter_start, harry_potter_fr_start, get_batch_pos_mask, extract_text_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20d3f7a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fdc277c4fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfed67e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2754ad1ac49f4e8bb032d09e431b378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c373406e07974849afd6b9ace81092d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9d3b9d1b2745edaedc9046e4912582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ecd566395841e2aede58f4dec55ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028fe5a64e46404593acdc695846a158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "MODEL_NAME = \"pythia-2.8b\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")\n",
    "model.name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371f9a91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sentiment_dir = load_array(\"das_simple_train_ADJ_layer16\", model)[:,0]\n",
    "sentiment_dir: Float[Tensor, \"d_model\"] = torch.tensor(sentiment_dir).to(device=device, dtype=torch.float32)\n",
    "sentiment_dir /= sentiment_dir.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28af74d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2560])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ee99e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/usr/local/lib/python3.9/dist-packages/circuitsvis/dist/cdn/iife.js'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/projection_neuroscope2.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/projection_neuroscope2.ipynb#X12sanVweXRleHQ%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/projection_neuroscope2.ipynb#X12sanVweXRleHQ%3D?line=1'>2</a>\u001b[0m \u001b[39mAmidst a serene landscape, a group of volunteers worked together to rescue injured animals. Their dedication showcased the true value of compassion and empathy. However, as they tirelessly carried out their mission, they faced an pressing issue: the rapid decline of the local wildlife\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms natural habitat. Determined to subdue this crisis, they organized campaigns to raise awareness and funds. Their efforts reminded us that even in the face of adversity, unity and determination can make a significant impact.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/projection_neuroscope2.ipynb#X12sanVweXRleHQ%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/projection_neuroscope2.ipynb#X12sanVweXRleHQ%3D?line=3'>4</a>\u001b[0m plot_neuroscope(text, model, centred\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, special_dir\u001b[39m=\u001b[39;49msentiment_dir)\n",
      "File \u001b[0;32m/notebooks/eliciting-latent-sentiment/utils/neuroscope.py:170\u001b[0m, in \u001b[0;36mplot_neuroscope\u001b[0;34m(text, model, centred, activations, special_dir, verbose)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    161\u001b[0m     activations\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m    162\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactivations must be of shape [tokens x layers x neurons], found \u001b[39m\u001b[39m{\u001b[39;00mactivations\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(str_tokens) \u001b[39m==\u001b[39m activations\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], (\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtokens and activations must have the same length, found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokens=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(str_tokens)\u001b[39m}\u001b[39;00m\u001b[39m and acts=\u001b[39m\u001b[39m{\u001b[39;00mactivations\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m )\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m text_neuron_activations(\n\u001b[1;32m    171\u001b[0m     tokens\u001b[39m=\u001b[39;49mstr_tokens, \n\u001b[1;32m    172\u001b[0m     activations\u001b[39m=\u001b[39;49mactivations,\n\u001b[1;32m    173\u001b[0m     first_dimension_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLayer (resid_pre)\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    174\u001b[0m     second_dimension_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mModel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    175\u001b[0m     second_dimension_labels\u001b[39m=\u001b[39;49m[model\u001b[39m.\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmodel_name],\n\u001b[1;32m    176\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/circuitsvis/activations.py:52\u001b[0m, in \u001b[0;36mtext_neuron_activations\u001b[0;34m(tokens, activations, first_dimension_name, second_dimension_name, first_dimension_labels, second_dimension_labels, first_dimension_default, second_dimension_default, show_selectors)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactivations must be of type np.ndarray, torch.Tensor, or list, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(activations)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m render(\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mTextNeuronActivations\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m     tokens\u001b[39m=\u001b[39;49mtokens,\n\u001b[1;32m     55\u001b[0m     activations\u001b[39m=\u001b[39;49mactivations_list,\n\u001b[1;32m     56\u001b[0m     firstDimensionName\u001b[39m=\u001b[39;49mfirst_dimension_name,\n\u001b[1;32m     57\u001b[0m     secondDimensionName\u001b[39m=\u001b[39;49msecond_dimension_name,\n\u001b[1;32m     58\u001b[0m     firstDimensionLabels\u001b[39m=\u001b[39;49mfirst_dimension_labels,\n\u001b[1;32m     59\u001b[0m     secondDimensionLabels\u001b[39m=\u001b[39;49msecond_dimension_labels,\n\u001b[1;32m     60\u001b[0m     firstDimensionDefault\u001b[39m=\u001b[39;49mfirst_dimension_default,\n\u001b[1;32m     61\u001b[0m     secondDimensionDefault\u001b[39m=\u001b[39;49msecond_dimension_default,\n\u001b[1;32m     62\u001b[0m     showSelectors\u001b[39m=\u001b[39;49mshow_selectors,\n\u001b[1;32m     63\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/circuitsvis/utils/render.py:214\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(react_element_name, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    199\u001b[0m     react_element_name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    200\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: PythonProperty\n\u001b[1;32m    201\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderedHTML:\n\u001b[1;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Render a visualization to HTML\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[39m    This will show the visualization in Jupyter Lab/Colab by default, and show a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m        Html: HTML for the visualization\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     local_src \u001b[39m=\u001b[39m render_local(react_element_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m     cdn_src \u001b[39m=\u001b[39m render_cdn(react_element_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m RenderedHTML(local_src, cdn_src)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/circuitsvis/utils/render.py:149\u001b[0m, in \u001b[0;36mrender_local\u001b[0;34m(react_element_name, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m# Load the JS\u001b[39;00m\n\u001b[1;32m    148\u001b[0m filename \u001b[39m=\u001b[39m Path(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdist\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcdn\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miife.js\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 149\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    150\u001b[0m     inline_js \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m    151\u001b[0m     \u001b[39m# Remove any closing script tags (as this breaks inline code)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.9/dist-packages/circuitsvis/dist/cdn/iife.js'"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Amidst a serene landscape, a group of volunteers worked together to rescue injured animals. Their dedication showcased the true value of compassion and empathy. However, as they tirelessly carried out their mission, they faced an pressing issue: the rapid decline of the local wildlife's natural habitat. Determined to subdue this crisis, they organized campaigns to raise awareness and funds. Their efforts reminded us that even in the face of adversity, unity and determination can make a significant impact.\n",
    "\"\"\"\n",
    "plot_neuroscope(text, model, centred=True, verbose=False, special_dir=sentiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Harry Potter example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a6448",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# harry_potter_neuroscope = plot_neuroscope(harry_potter_start, model, centred=True, verbose=False, special_dir=sentiment_dir)\n",
    "# save_html(harry_potter_neuroscope, \"harry_potter_neuroscope\", model)\n",
    "# harry_potter_neuroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5b71c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "\n",
    "# harry_potter_fr_neuroscope = plot_neuroscope(harry_potter_fr_start, model, centred=True, verbose=False, special_dir=sentiment_dir)\n",
    "# save_html(harry_potter_fr_neuroscope, \"harry_potter_fr_neuroscope\", model)\n",
    "# harry_potter_fr_neuroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6142dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Mandarin example\n",
    "# mandarin_text = \"\"\"\n",
    "# 這是可能發生的最糟糕的事情。 我討厭你這麼說。 你所做的事情太可怕了。\n",
    "\n",
    "# 然而，你的兄弟卻做了一些了不起的事情。 他非常好，非常令人欽佩，非常善良。 我很愛他。\n",
    "# \"\"\"\n",
    "# plot_neuroscope(mandarin_text, model, centred=True, verbose=False, special_dir=sentiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4e37a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Steering and generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725adb0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def steering_hook(\n",
    "    input: Float[Tensor, \"batch pos d_model\"], hook: HookPoint, coef: float, direction: Float[Tensor, \"d_model\"]\n",
    "):\n",
    "    assert 'resid_post' in hook.name\n",
    "    input += coef * direction\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e67d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def steer_and_test_prompt(\n",
    "    coef: float,\n",
    "    direction: Float[Tensor, \"d_model\"],\n",
    "    prompt: str,\n",
    "    answer: str,\n",
    "    model: HookedTransformer,\n",
    "    prepend_space_to_answer: bool = True,\n",
    "):\n",
    "    model.reset_hooks()\n",
    "    hook = partial(steering_hook, coef=coef, direction=direction)\n",
    "    model.add_hook(\n",
    "        get_act_name(\"resid_post\", 0),\n",
    "        hook,\n",
    "        dir=\"fwd\",\n",
    "    )\n",
    "    test_prompt(prompt, answer, model, prepend_space_to_answer=prepend_space_to_answer)\n",
    "    model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9482f40",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def steer_and_generate(\n",
    "    coef: float,\n",
    "    direction: Float[Tensor, \"d_model\"],\n",
    "    prompt: str,\n",
    "    model: HookedTransformer,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    model.reset_hooks()\n",
    "    hook = partial(steering_hook, coef=coef, direction=direction)\n",
    "    model.add_hook(\n",
    "        get_act_name(\"resid_post\", 0),\n",
    "        hook,\n",
    "        dir=\"fwd\",\n",
    "    )\n",
    "    input = model.to_tokens(prompt)\n",
    "    output = model.generate(input, **kwargs)\n",
    "    model.reset_hooks()\n",
    "    return model.to_string(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3477174",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def run_steering_search(\n",
    "    coefs: Iterable[int], samples: int, sentiment_dir: Float[Tensor, \"d_model\"], model: HookedTransformer, \n",
    "    top_k: int = 10, temperature: float = 1.0, max_new_tokens: int = 20, do_sample: bool = True,\n",
    "    seed: int = 0,\n",
    "    prompt: str = \"I really enjoyed the movie, in fact I loved it. I thought the movie was just very\",\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    out = \"\"\n",
    "    for coef, sample in tqdm(itertools.product(coefs, range(samples)), total=len(coefs) * samples):\n",
    "        if sample == 0:\n",
    "            out += f\"Coef: {coef}\\n\"\n",
    "        gen = steer_and_generate(\n",
    "            coef,\n",
    "            sentiment_dir,\n",
    "            prompt,\n",
    "            model,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        out += gen.replace(prompt, \"\") + \"\\n\"\n",
    "    return out.replace(\"<|endoftext|>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5e12f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# steering_text = run_steering_search(\n",
    "#     coefs=[-20, -10,  0],\n",
    "#     samples=10,\n",
    "#     sentiment_dir=sentiment_dir,\n",
    "#     model=model,\n",
    "#     top_k=10,\n",
    "#     temperature=1.0,\n",
    "#     max_new_tokens=30,\n",
    "#     do_sample=True,\n",
    "#     seed=0,\n",
    "#     prompt=\"I really enjoyed the movie, in fact I loved it. I thought the movie was just very\",\n",
    "# )\n",
    "# #%%\n",
    "# plot_neuroscope(steering_text, centred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23444bd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53493e1d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def test_prefixes(fragment: str, prefixes: List[str], model: HookedTransformer):\n",
    "    single_tokens = []\n",
    "    for word in prefixes:\n",
    "        if model.to_str_tokens(word, prepend_bos=False)[0] == fragment:\n",
    "            single_tokens.append(word)\n",
    "    single_tokens = list(set(single_tokens))\n",
    "    text = \"\\n\".join(single_tokens)\n",
    "    return plot_neuroscope(text, centred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4052f3d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# test_prefixes(\n",
    "#     \" cr\",\n",
    "#     [' crony', ' crump', ' crinkle', ' craggy', ' cramp', ' crumb', ' crayon', ' cringing', ' cramping'],\n",
    "#     model\n",
    "# )\n",
    "# #%%\n",
    "# test_prefixes(\n",
    "#     \" clo\",\n",
    "#     [' clopped', ' cloze', ' cloistered', ' clopping', ' cloacal', ' cloister', ' cloaca',],\n",
    "#     model\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac7d29",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a4710",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# negating_positive_text = \"Here are my honest thoughts. You're not a good person. I don't like you. I hope that you don't succeed.\"\n",
    "# plot_neuroscope(negating_positive_text, centred=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f1900",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# negating_negative_text = \"Here are my honest thoughts. You never fail. You're not bad at all. \"\n",
    "# plot_neuroscope(negating_negative_text, centred=True, verbose=False)\n",
    "# #%%\n",
    "# plot_neuroscope(\n",
    "#     \"Here are my honest thoughts. You never fail. You're not bad at all.\", \n",
    "#     centred=True, \n",
    "#     verbose=False,\n",
    "# )\n",
    "# #%%\n",
    "# plot_neuroscope(\n",
    "#     \"Here are my honest thoughts. Don't doubt yourself. You need not fear. You are not wrong. You are very much\", \n",
    "#     centred=True, \n",
    "#     verbose=False,\n",
    "# )\n",
    "# #%%\n",
    "# plot_neuroscope(\n",
    "#     \"Don't be sad. You should not feel ashamed. You are a truly\", \n",
    "#     centred=True, \n",
    "#     verbose=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa6895",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# test_prompt(\n",
    "#     \"Here are my honest thoughts. You never fail. You're not bad at all. You will always\", \n",
    "#     \"\", \n",
    "#     model\n",
    "# )\n",
    "# #%%\n",
    "# test_prompt(\n",
    "#     \"Don't be sad. You have nothing to be ashamed of. You are a truly\", \n",
    "#     \"\", \n",
    "#     model,\n",
    "#     top_k=20,\n",
    "# )\n",
    "# #%%\n",
    "# test_prompt(\n",
    "#     \"Here are my honest thoughts. You are not a good person. Your behaviour is not okay. You are very\", \n",
    "#     \"\", \n",
    "#     model,\n",
    "#     top_k=20\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373aeb2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# negating_weird_text = \"Here are my honest thoughts. You are disgustingly beautiful. I hate how much I love you. Stop being so good at everything.\"\n",
    "# plot_neuroscope(negating_weird_text, centred=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877c090",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "multi_token_negative_text = \"\"\"\n",
    "Alas, it is with a regretful sigh that I endeavor to convey my cogitations regarding the cinematic offering that is \"Oppenheimer,\" a motion picture that sought to render an illuminating portrayal of the eponymous historical figure, yet found itself ensnared within a quagmire of ponderous pacing, desultory character delineations, and an ostentatious predilection for pretentious verbosity, thereby culminating in an egregious amalgamation of celluloid that fails egregiously to coalesce into a coherent and engaging opus.\n",
    "\n",
    "From its inception, one is greeted with a superfluous indulgence in visual rhapsodies, replete with panoramic vistas and artistic tableaux that appear, ostensibly, to strive for profundity but instead devolve into a grandiloquent spectacle that serves naught but to obfuscate the underlying narrative. The esoteric nature of the cinematographic composition, while intended to convey a sense of erudition, inadvertently estranges the audience, stifling any vestige of emotional resonance that might have been evoked by the thematic elements.\n",
    "\n",
    "Regrettably, the characters, ostensibly intended to be the vessels through which the audience navigates the tumultuous currents of historical transformation, emerge as little more than hollow archetypes, devoid of psychological nuance or relatable verisimilitude. Their interactions, laden with stilted dialogues and ponderous monologues, meander aimlessly in the midst of a ponderous expanse, rendering their ostensibly profound endeavors an exercise in vapid verbosity rather than poignant engagement.\n",
    "\n",
    "The directorial predilection for intellectual acrobatics is manifest in the labyrinthine structure of the narrative, wherein chronology becomes a malleable construct, flitting whimsically between past and present without discernible rhyme or reason. While this narrative elasticity might have been wielded as a potent tool of thematic resonance, it instead metastasizes into an obfuscating force that imparts a sense of disjointed incoherence upon the cinematic proceedings, leaving the viewer to grapple with a puzzling tapestry of events that resist cohesive assimilation.\n",
    "\n",
    "Moreover, the fervent desire to imbue the proceedings with a veneer of intellectual profundity is acutely palpable within the film's verbiage-laden script. Dialogue, often comprising polysyllabic words of labyrinthine complexity, becomes an exercise in linguistic gymnastics that strays perilously close to the precipice of unintentional self-parody. This quixotic dalliance with ostentatious vocabulary serves only to erect an insurmountable barrier between the audience and the narrative, relegating the viewer to a state of befuddled detachment.\n",
    "\n",
    "In summation, \"Oppenheimer,\" for all its aspirations to ascend the cinematic pantheon as an erudite exploration of historical gravitas, falters egregiously beneath the weight of its own ponderous ambitions. With an overarching penchant for verbal ostentation over emotional resonance, a narrative structure that veers perilously into the realm of disjointed incoherence, and characters bereft of authentic vitality, this cinematic endeavor sadly emerges as an exercise in cinematic misdirection that regrettably fails to ignite the intellectual or emotional faculties of its audience.\n",
    "\"\"\"\n",
    "# plot_neuroscope(multi_token_negative_text, centred=True, verbose=False, model=model, special_dir=sentiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798db09",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Openwebtext-10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b22645",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(model, \"stas/openwebtext-10k\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed96e4b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def get_activations_from_dataloader(\n",
    "    data: torch.utils.data.dataloader.DataLoader,\n",
    "    max_batches: int = None,\n",
    ") -> Float[Tensor, \"row pos\"]:\n",
    "    all_acts = []\n",
    "    for batch_idx, batch_value in tqdm(enumerate(data), total=len(data)):\n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "        batch_acts: Float[Tensor, \"batch pos layer\"] = get_projections_for_text(batch_tokens, sentiment_dir, model)\n",
    "        all_acts.append(batch_acts)\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "    # Concatenate the activations into a single tensor\n",
    "    all_acts: Float[Tensor, \"row pos layer\"] = torch.cat(all_acts, dim=0)\n",
    "    return all_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8c7d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class ClearCache:\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        model.cuda()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        model.cpu()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95fab4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if is_file(\"sentiment_activations.npy\", model):\n",
    "    sentiment_activations = load_array(\"sentiment_activations\", model)\n",
    "    sentiment_activations: Float[Tensor, \"row pos layer\"]  = torch.tensor(\n",
    "        sentiment_activations, device=device, dtype=torch.float32\n",
    "    )\n",
    "else:\n",
    "    with ClearCache():\n",
    "        sentiment_activations: Float[Tensor, \"row pos layer\"]  = get_activations_from_dataloader(dataloader)\n",
    "    save_array(sentiment_activations, \"sentiment_activations\", model)\n",
    "sentiment_activations.shape, sentiment_activations.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f427655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Anthropic Graph 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c06dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def sample_by_bin(\n",
    "    data: Float[Tensor, \"batch pos\"],\n",
    "    bins: int = 20,\n",
    "    samples_per_bin: int = 20,\n",
    "    seed: int = 0,\n",
    "    window_size: int = 10,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    flat = data.flatten()\n",
    "    hist, bin_edges = np.histogram(flat.cpu().numpy(), bins=bins)\n",
    "    bin_indices: Int[np.ndarray, \"batch pos\"] = np.digitize(data.cpu().numpy(), bin_edges)\n",
    "    if verbose:\n",
    "        print(bin_edges)\n",
    "    indices = []\n",
    "    for bin_idx in range(1, bins + 1):\n",
    "        lb = bin_edges[bin_idx - 1]\n",
    "        ub = bin_edges[bin_idx]\n",
    "        bin_batches, bin_positions = np.where(bin_indices == bin_idx)\n",
    "        bin_samples = np.random.randint(0, len(bin_batches), samples_per_bin)\n",
    "        indices += [\n",
    "            (bin_idx, lb, ub, bin_batches[bin_sample], bin_positions[bin_sample])\n",
    "            for bin_sample in bin_samples\n",
    "        ]\n",
    "    df =  pd.DataFrame(indices, columns=[\"bin\", \"lb\", \"ub\", \"batch\", \"position\"])\n",
    "    tokens = []\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = extract_text_window(\n",
    "            int(row.batch), int(row.position), dataloader, model, window_size=window_size\n",
    "        )\n",
    "        tokens.append(text[window_size])\n",
    "        texts.append(\"\".join(text))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['token'] = tokens\n",
    "    df['text'] = texts\n",
    "    return df.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710fda2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "bin_samples = sample_by_bin(\n",
    "    sentiment_activations[:, :, 1], verbose=False\n",
    ")\n",
    "to_csv(bin_samples, \"bin_samples\", model)\n",
    "bin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18de9c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "labelled_bin_samples = get_csv(\n",
    "    \"labelled_bin_samples\", model\n",
    ")\n",
    "labelled_bin_samples.sentiment = labelled_bin_samples.sentiment.str.replace('negative', 'Negative').str.replace('positive', 'Positive')\n",
    "assert labelled_bin_samples.sentiment.isin(['Positive', 'Negative', 'Neutral', 'Somewhat Positive', 'Somewhat Negative']).all()\n",
    "labelled_bin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1f8fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sampled_activations = []\n",
    "for idx, row in labelled_bin_samples.iterrows():\n",
    "    sampled_activations.append(sentiment_activations[row.batch, row.position, 1].detach().cpu().numpy())\n",
    "labelled_bin_samples['activation'] = sampled_activations\n",
    "labelled_bin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1d26d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    labelled_bin_samples,\n",
    "    x=\"activation\",\n",
    "    color=\"sentiment\",\n",
    "    nbins=200,\n",
    "    title=\"Histogram of sentiment activations by label\",\n",
    "    barmode=\"overlay\",\n",
    "    marginal=\"rug\",\n",
    "    histnorm=\"probability density\",\n",
    "    hover_data=[\"token\", \"text\"]\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_x=0.5,\n",
    "    showlegend=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b13c03",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_bin_proportions(df: pd.DataFrame, nbins=50):\n",
    "    sentiments = df['sentiment'].unique()\n",
    "    df = df.sort_values(by='activation').reset_index(drop=True)\n",
    "    df['activation_cut'] = pd.cut(df.activation, bins=nbins)\n",
    "    df.activation_cut = df.activation_cut.apply(lambda x: 0.5 * (x.left + x.right))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    data = []\n",
    "    \n",
    "    for x, bin_df in df.groupby('activation_cut'):\n",
    "        label_props = bin_df.value_counts('sentiment', normalize=True, sort=False)\n",
    "        data.append([label_props.get(sentiment, 0) for sentiment in sentiments])\n",
    "    \n",
    "    data = pd.DataFrame(data, columns=sentiments)\n",
    "    cumulative_data = data.cumsum(axis=1)  # Cumulative sum along columns\n",
    "    \n",
    "    x_values = df['activation_cut'].unique()\n",
    "    \n",
    "    # Adding traces for the rest of the sentiments\n",
    "    for idx, sentiment in enumerate(sentiments):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_values, y=cumulative_data[sentiment], name=sentiment,\n",
    "            hovertemplate='<br>'.join([\n",
    "                'Sentiment: ' + sentiment,\n",
    "                'Activation: %{x}',\n",
    "                'Cum. Label proportion: %{y:.4f}',\n",
    "            ]),\n",
    "            fill='tonexty',\n",
    "            mode='lines',\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Anthropic Graph 1: Proportion of Sentiment by Activation\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        xaxis_title=\"Activation\",\n",
    "        yaxis_title=\"Cum. Label proportion\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facfaa7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot_bin_proportions(labelled_bin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d369f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_stacked_histogram(labelled_bin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772961c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Anthropic Graph 2\n",
    "ecdf = stats.ecdf(sentiment_activations[:, :, 1].flatten().cpu().numpy())\n",
    "ecdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8cf79",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_weighted_histogram(df: pd.DataFrame, nbins: int = 100):\n",
    "    sentiments = df['sentiment'].unique()\n",
    "    df = df.sort_values(by='activation').reset_index(drop=True)\n",
    "    df['activation_cut'] = pd.cut(df.activation, bins=nbins)\n",
    "    fig = go.Figure()\n",
    "    data = []\n",
    "    for x, bin_df in df.groupby('activation_cut'):\n",
    "        prob_x = ecdf.cdf.evaluate(x.right) - ecdf.cdf.evaluate(x.left)\n",
    "        label_props = bin_df.value_counts('sentiment', normalize=True, sort=False)\n",
    "        data.append([prob_x * label_props.get(sentiment, 0) for sentiment in sentiments])\n",
    "    data = pd.DataFrame(data, columns=sentiments)\n",
    "    # Adding bar traces for each sentiment\n",
    "    x_values = df['activation_cut'].apply(lambda x: 0.5 * (x.left + x.right)).unique()\n",
    "    for sentiment in sentiments:\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=x_values, y=data[sentiment], name=sentiment,\n",
    "            hovertemplate='<br>'.join([\n",
    "                'Sentiment: ' + sentiment,\n",
    "                'Activation: %{x}',\n",
    "                'Probability density: %{y:.4f}',\n",
    "            ]),\n",
    "            xaxis='x1', yaxis='y1',\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\", title=\"Anthropic Graph 2: Stacked Histogram of Sentiment by Activation\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        xaxis_title=\"Activation\",\n",
    "        yaxis_title=\"Probability density\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cf70d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot_weighted_histogram(labelled_bin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae758ab1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Anthropic Graph 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6874d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_ev_histogram(df: pd.DataFrame, nbins: int = 100):\n",
    "    sentiments = df['sentiment'].unique()\n",
    "    df = df.sort_values(by='activation').reset_index(drop=True)\n",
    "    df['activation_cut'] = pd.cut(df.activation, bins=nbins)\n",
    "    fig = go.Figure()\n",
    "    data = []\n",
    "    for x_interval, bin_df in df.groupby('activation_cut'):\n",
    "        x_mid = 0.5 * (x_interval.left + x_interval.right)\n",
    "        prob_x = ecdf.cdf.evaluate(x_interval.right) - ecdf.cdf.evaluate(x_interval.left)\n",
    "        ev = x_mid * prob_x\n",
    "        label_props = bin_df.value_counts('sentiment', normalize=True, sort=False)\n",
    "        data.append([ev * label_props.get(sentiment, 0) for sentiment in sentiments])\n",
    "    data = pd.DataFrame(data, columns=sentiments)\n",
    "    # Adding bar traces for each sentiment\n",
    "    x_values = df['activation_cut'].apply(lambda x: 0.5 * (x.left + x.right)).unique()\n",
    "    for sentiment in sentiments:\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=x_values, y=data[sentiment], name=sentiment,\n",
    "            hovertemplate='<br>'.join([\n",
    "                'Sentiment: ' + sentiment,\n",
    "                'Activation: %{x}',\n",
    "                'EV Contribution: %{y:.4f}',\n",
    "            ]),\n",
    "            xaxis='x1', yaxis='y1',\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        barmode=\"stack\", title=\"Anthropic Graph 3: Stacked Histogram of EV contribution\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True,\n",
    "        xaxis_title=\"Activation\",\n",
    "        yaxis_title=\"EV Contribution\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4c19a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot_ev_histogram(labelled_bin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ca97e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Top k max activating examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857b823",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_topk(sentiment_activations, dataloader, model, k=50, layer=6, window_size=20, centred=True)\n",
    "# # %%\n",
    "# plot_topk(sentiment_activations, k=50, layer=12, window_size=20, centred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e57f12",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Top p sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb99d7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_top_p(sentiment_activations, k=50, layer=1, p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb906820",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b3ceb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def expand_exclusions(exclusions: Iterable[str]):\n",
    "    expanded_exclusions = []\n",
    "    for exclusion in exclusions:\n",
    "        exclusion = exclusion.strip().lower()\n",
    "        expanded_exclusions.append(exclusion)\n",
    "        expanded_exclusions.append(exclusion + \" \")\n",
    "        expanded_exclusions.append(\" \" + exclusion)\n",
    "        expanded_exclusions.append(\" \" + exclusion + \" \")\n",
    "        expanded_exclusions.append(exclusion.capitalize())\n",
    "        expanded_exclusions.append(\" \" + exclusion.capitalize())\n",
    "        expanded_exclusions.append(exclusion.capitalize() + \" \")\n",
    "        expanded_exclusions.append(exclusion.upper())\n",
    "        expanded_exclusions.append(\" \" + exclusion.upper())\n",
    "        expanded_exclusions.append(exclusion.upper() + \" \")\n",
    "    return list(set(expanded_exclusions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d1340",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "exclusions = [\n",
    "    # proper nouns\n",
    "    # 'Flint', 'Fukushima', 'Obama', 'Assad', 'Gaza',\n",
    "    # 'CIA', 'BP', 'istan', 'VICE', 'TSA', 'Mitt', 'Romney', 'Afghanistan', 'Kurd', 'Molly',\n",
    "    # 'DoS', 'Medicaid', 'Kissinger',\n",
    "    'ISIS', 'GOP',\n",
    "    # the rest\n",
    "    'adequate', 'truly', 'mis', 'dys', 'provides', 'offers', 'fully',  'migraine',  \n",
    "    'really', 'considerable', 'reasonably', 'substantial', 'additional', 'STD', \n",
    "    'Narcolepsy', 'Tooth', 'RUDE', 'Diagnostic',  '!', \n",
    "    'agoraphobia', 'greenhouse', \n",
    "    'stars', 'star',\n",
    "    ' perfect', ' fantastic',' marvelous',' good',' remarkable',' wonderful',\n",
    "    ' fabulous',' outstanding',' awesome',' exceptional',' incredible',' extraordinary',\n",
    "    ' amazing',' lovely',' brilliant',' terrific',' superb',' spectacular',' great',\n",
    "    ' beautiful'\n",
    "    ' dreadful',' bad',' miserable',' horrific',' terrible',\n",
    "    ' disgusting',' disastrous',' horrendous',' offensive',' wretched',\n",
    "    ' awful',' unpleasant',' horrible',' mediocre',' disappointing',\n",
    "    'excellent', 'opportunity', 'success', 'generous', 'harmful', 'plaguing', 'derailed', 'unwanted',\n",
    "    'stigma', 'burdened', 'stereotypes', 'hurts', 'burdens', 'harming', 'winning', 'smooth', \n",
    "    'shameful', 'hurting', 'nightmare', 'inflicted', 'disadvantaging', 'stigmatized', 'stigmatizing',\n",
    "    'stereotyped', 'forced', 'confidence', 'senseless', 'wrong', 'hurt', 'stereotype', 'sexist',\n",
    "    'unnecesarily', 'horribly',  'impressive', 'fraught', 'brute', 'blight',\n",
    "    'unnecessary', 'unnecessarily', 'fraught','deleterious', 'scrapped', 'intrusive',\n",
    "    'unhealthy', 'plague', 'hated', 'burden', 'vilified', 'afflicted', 'polio', 'inaction',\n",
    "    'condemned', 'crippled', 'unrestrained', 'derail', 'cliché', \n",
    "    'toxicity', 'bastard', 'clich', 'politicized', 'overedit', 'curse', 'choked', 'politicize',\n",
    "    'frowned', 'sorry', 'slurs', 'taboo', 'bullshit', 'painfully', 'premature', 'worsened',\n",
    "    'pathogens', 'Domestic', 'Violence', 'painful',\n",
    "    'splendid', 'magnificent', 'beautifully', 'gorgeous', 'nice', 'phenomenal',\n",
    "    'finest', 'splendid', 'wonderfully',\n",
    "    'ugly', 'dehuman', 'negatively', 'degrading', 'rotten', 'traumatic', 'crying',\n",
    "    'criticized', 'dire', 'best', 'exceptionally', 'negative', 'dirty',\n",
    "    'rotting','enjoy', 'amazingly', 'brilliantly', 'traumatic', 'hinder', 'hindered',\n",
    "    'depressing', 'diseased', 'depressing', 'bleary', 'carcinogenic', 'demoralizing',\n",
    "    'traumatizing', 'injustice', 'blemish', 'nausea', 'peeing', 'abhorred', \n",
    "    'appreciate', 'perfectly', 'elegant', 'supreme', 'excellence', 'sufficient', 'toxic', 'hazardous',\n",
    "    'muddy', 'hinder', 'derelict', 'disparaged', 'sour', 'disgraced', 'degenerate',\n",
    "    'disapproved', 'annoy', 'nicely', 'stellar', 'charming', 'cool', 'handsome', 'exquisite',\n",
    "    'sufficient', 'cool', 'brilliance', 'flawless', 'delightful', 'impeccable', 'fascinating',\n",
    "    'decent', 'genius', 'appreciated', 'remarkably', 'greatest', 'humiliating', \n",
    "    'embarassing', 'saddening', 'injustice', 'hinders', 'annihilate', 'waste', 'unliked',\n",
    "    'stunning', 'glorious', 'deft', 'enjoyed', 'ideal', 'stylish', 'sublime', 'admirable',\n",
    "    'embarass', 'injustices', 'disapproval', 'misery', 'sore', 'prejudice', 'disgrace',\n",
    "    'messed', 'capable', 'breathtaking', 'suffered', 'poisoned', 'ill', 'unsafe', \n",
    "    'morbid', 'irritated', 'irritable', 'contaiminate', 'derogatory',\n",
    "    'prejudging', 'inconvenienced', 'embarrassingly', 'embarrass', 'embarassed', 'embarrassment',\n",
    "    'fine', 'better', 'unparalleled', 'astonishing', 'neat', 'embarrassing', 'doom',\n",
    "    'inconvenient', 'boring', 'conatiminate', 'contaminated', 'contaminating', 'contaminates',\n",
    "    'penalty', 'tarnish', 'disenfranchised', 'disenfranchising', 'disenfranchisement',\n",
    "    'super', 'marvel', 'enjoys', 'talented', 'clever', 'enhanced', 'ample',\n",
    "    'love', 'expert', 'gifted', 'loved', 'enjoying', 'enjoyable', 'enjoyed', 'enjoyable',\n",
    "    'tremendous', 'confident', 'confidently', 'love', 'harms', 'jeapordize', 'jeapordized',\n",
    "    'depress', 'penalize', 'penalized', 'penalizes', 'penalizing', 'penalty', 'penalties',\n",
    "    'tarred', 'nauseating', 'harms', 'lethality', 'loves', 'unique', 'appreciated', 'appreciates',\n",
    "    'appreciating', 'appreciation', 'appreciative', 'appreciates', 'appreciated', 'appreciating',\n",
    "    'favorite', 'greatness', 'goodness', 'suitable', 'prowess', 'masterpiece', 'ingenious', 'strong',\n",
    "    'versatile', 'well', 'effective', 'scare', 'shaming', 'worse', 'bleak', 'hate', 'tainted',\n",
    "    'destructive', 'doomed', 'celebrated', 'gracious', 'worthy', 'interesting', 'coolest', \n",
    "    'intriguing', 'enhance', 'enhances', 'celebrated', 'genuine', 'smoothly', 'greater', 'astounding',\n",
    "    'classic', 'successful', 'innovative', 'plenty', 'competent', 'noteworthy', 'treasures',\n",
    "    'adore', 'adores', 'adored', 'adoring', 'adorable', 'adoration', 'adore', 'grim',\n",
    "    'displeased', 'mismanagement', 'jeopardizes', 'garbage', 'mangle', 'stale',\n",
    "    'excel', 'wonders', 'faithful', 'extraordinarily', 'inspired', 'vibrant', 'faithful', 'compelling',\n",
    "    'standout', 'exemplary', 'vibrant', 'toxic', 'contaminate', 'antagonistic', 'terminate',\n",
    "    'detrimental', 'unpopular', 'fear', 'outdated', 'adept', 'charisma', 'popular', 'popularly',\n",
    "    'humiliation', 'sick', 'nasty', 'fatal', 'distress', 'unfavorable', 'foul', \n",
    "    'bureaucratic', 'dying', 'nasty', 'worst', 'destabilising', 'unforgiving', 'vandalized',\n",
    "    'polluted', 'poisonous', 'dirt', 'original', 'incredibly', 'invaluable', 'acclaimed',\n",
    "    'successfully', 'able', 'reliable', 'loving', 'beauty', 'famous', 'solid', 'rich',\n",
    "    'famous', 'thoughtful', 'enhancement', 'sufficiently', 'robust', 'bestselling', 'renowned',\n",
    "    'impressed', 'elegence', 'thrilled', 'hostile', 'scar', 'piss', 'danger', 'inflammatory',\n",
    "    'diseases', 'disillusion', 'depressive', 'bum', 'disgust', 'aggravates', 'pissy',\n",
    "    'dangerous', 'urinary', 'pissing', 'nihilism', 'nihilistic', 'disillusioned', 'depressive', \n",
    "    'dismal', 'trustworthy', 'unjust', 'enthusiastic', 'seamlesslly', 'seamless', 'liked',\n",
    "    'enthusiasm', 'superior', 'useful', 'master', 'heavenly', 'enthusiastic', 'effortlessly',\n",
    "    'adequately', 'powerful', 'seamlessly', 'dumb', 'dishonors', 'traitor',\n",
    "    'bleed', 'invalid', 'horror', 'reprehensible', 'die', 'petty', 'lame', 'fouling', 'foul',\n",
    "    'racist', 'elegance', 'top', 'waste', 'wasteful', 'wasted', 'wasting', 'wastes', 'wastefulness',\n",
    "    'trample', 'trampled', 'vexing', 'vitriol', 'stangate', 'stagnant', 'stagnate', 'stagnated',\n",
    "    'crisis', 'vex', 'corroded', 'sad', 'bitter', 'insults', 'impres', 'cringe', 'humilate', 'humiliates',\n",
    "    'humiliated', 'humiliating', 'humiliation', 'humiliations', 'humiliates', 'humiliatingly',\n",
    "    'corrosive', 'corrosion', 'corroded', 'corrodes', 'corroding', 'corrosive', 'corrosively',\n",
    "    'inhospitable', 'waste', 'wastes', 'wastefulness', 'wasteful', 'wasted', 'wasting',\n",
    "    'unintended', 'stressful', 'trash', 'unhappy', 'unhappily', 'unhappiness', 'unhappier',\n",
    "    'unholy', 'peril', 'perilous', 'perils', 'perilously', 'perilousness', 'perilousnesses',\n",
    "    'faulty', 'damaging', 'damages', 'damaged', 'damagingly', 'damages', 'damaging', 'damaged',\n",
    "    'trashy', 'punitive', 'punish', 'punished', 'punishes', 'punishing', 'punishment', 'punishments',\n",
    "    'pessimistic', 'pessimism', 'inspiring', 'impress', 'coward', 'tired', 'empty',\n",
    "    'trauma', 'torn', 'unease', 'gloomy', 'gloom', 'gloomily', 'gloominess', 'gloomier',\n",
    "    'hideous', 'embarrassed', 'wastes', 'wasteful', 'misdemeanour', 'nuisance',\n",
    "    'dilemma',' dilemmas', 'sewage', 'bogie', 'postponed', 'backward', 'paralyze',\n",
    "    'very', 'special', 'important', 'more', 'nervous', 'awkward', 'problem', 'pain', 'loss',\n",
    "    'melancholy', 'dismissing', 'complain', 'stomp', 'terrorist', 'racism', 'criminal',\n",
    "    'colder', 'nuclear', 'divided', 'death', 'chlorine', 'illegal', 'risks',\n",
    "    'prisons', 'villain', 'incinerate', 'dead', 'lonely', 'mistakes', 'biased', 'illicit',\n",
    "    'defeat', 'lose', 'unbearable', 'presure', 'desperation', \n",
    "    'osteoarthritis', 'Medicating', 'Medications', 'Medication', 'depressed', 'crimes',\n",
    "    'suck', 'hemorrhage', 'crap', 'dull', 'headaches', 'turbulent', 'intolerant',\n",
    "    'vulnerable', 'insignificant', 'insignificance', 'blame', 'Lie', 'jail', 'abuse',\n",
    "    'reputable', 'slave', 'harm', 'died', 'viruses', 'homeless', 'blind', 'mistake',\n",
    "    'war', 'accident', 'incidents', 'radiation', 'cursed', 'scorn', 'deaths', 'slow',\n",
    "    'crashing', 'warning', 'hypocritical', 'hypocrisy', 'problems', 'disappointment',\n",
    "    'blood', 'slut', 'skewer', 'vaguely', 'riots', 'unclear', 'charm', 'disease', 'creepy',\n",
    "    'burning', 'lack', 'guilty', 'glaring', 'failed', 'indoctrination', 'incoherent',\n",
    "    'hospital', 'syphilis', 'guilty', 'infection', 'faux', 'burning', 'creepy',\n",
    "    'disease', 'welts', 'trojans', 'trojan', 'makeshift', 'cant', 'tragic', 'stupid',\n",
    "    'vulgar', 'horrors', 'ugliness', 'miseries', 'loathing', 'hatred', 'dread', 'brutal',\n",
    "    'satisfactory', 'okay', 'ok', 'satisfying', 'filthy', 'crash', 'cynical', 'mourning',\n",
    "    'messy', 'tragedies', 'satisfied', 'cruelty', 'sadness', 'brutality', 'worsening',\n",
    "    'suicidal', 'despair', 'neatly', 'appropriately', 'handy', 'significant',\n",
    "    \"'kay\", 'aogny', 'sadly', 'hates', 'disaster', 'atrocities', 'effectively',\n",
    "    'worth', 'capability', 'ability', 'optimum', 'agony', 'tragedy', 'desperate',\n",
    "    'satisfy', 'optimal', 'helpful', 'definitely', 'cruel', 'crashed', 'ignorant',\n",
    "    'wrongful', 'imprisonment', 'cheap', 'severe', 'contamination', 'worried',\n",
    "    'anxiety', 'complaining', 'Hurricane', 'threat',\n",
    "\n",
    "]\n",
    "exclusions = expand_exclusions(exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677daee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_topk(\n",
    "#     sentiment_activations, dataloader, model,\n",
    "#     k=20, layer=1, window_size=20, centred=True,\n",
    "#     exclusions=exclusions,\n",
    "# )\n",
    "# #%%\n",
    "# plot_top_p(\n",
    "#     sentiment_activations, dataloader, model,\n",
    "#     p=.02,\n",
    "#     k=20, layer=1, window_size=20, centred=True,\n",
    "#     exclusions=exclusions,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb328944",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "save_text('\\n'.join(exclusions), 'sentiment_exclusions', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e612a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8098ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_histogram(\n",
    "    tokens: Union[str, List[str]], \n",
    "    all_activations: Float[Tensor, \"row pos layer\"], \n",
    "    name: str = None,\n",
    "    layer: int = 0, \n",
    "    nbins: int = 100,\n",
    "):\n",
    "    if name is None:\n",
    "        assert isinstance(tokens, str)\n",
    "        name = tokens\n",
    "    assert isinstance(name, str)\n",
    "    activations: Float[Tensor, \"row pos\"] = all_activations[:, :, layer]\n",
    "    mask: Bool[Tensor, \"row pos\"] = get_batch_pos_mask(tokens, dataloader, model, all_activations)\n",
    "    assert mask.shape == activations.shape\n",
    "    activations_to_plot = activations[mask].flatten()\n",
    "    fig = go.Histogram(x=activations_to_plot.cpu().numpy(), nbinsx=nbins, name=name)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1d0a3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_histograms(\n",
    "    tokens: Dict[str, List[str]], all_activations: Float[Tensor, \"row pos layer\"], layer: int = 0,\n",
    "    nbins: int = 100,\n",
    "):\n",
    "    fig = make_subplots(rows=len(tokens), cols=1, shared_xaxes=True, shared_yaxes=True)\n",
    "    for idx, (name, token) in enumerate(tokens.items()):\n",
    "        hist = plot_histogram(token, all_activations, name, layer, nbins)\n",
    "        fig.add_trace(hist, row=idx+1, col=1)\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Layer {layer} resid_pre sentiment cosine sims\",\n",
    "        height=200 * (idx + 1)\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bbc65",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pos_list = [\n",
    "    \" amazing\", \" great\", \" excellent\", \" good\", \" wonderful\", \" fantastic\", \" awesome\", \n",
    "    \" nice\", \" superb\", \" perfect\", \" incredible\", \" beautiful\"\n",
    "]\n",
    "neg_list = [\n",
    "    \" terrible\", \" bad\", \" awful\", \" horrible\", \" disgusting\", \" awful\", \n",
    "    \" evil\", \" scary\",\n",
    "]\n",
    "neutral_list = [\n",
    "    \" okay\", \" alright\", \" decent\", \" acceptable\", \" satisfactory\",\n",
    "]\n",
    "pos_neg_dict = {\n",
    "    \"positive\": pos_list,\n",
    "    \"negative\": neg_list,\n",
    "    \"neutral\": neutral_list,\n",
    "    # \"surprising_proper_nouns\": [\" Trek\", \" Yorkshire\", \" Linux\", \" Reuters\"],\n",
    "    # \"trek\": [\" Trek\"],\n",
    "    # \"yorkshire\": [\" Yorkshire\"],\n",
    "    # \"linux\": [\" Linux\"],\n",
    "    # \"reuters\": [\" Reuters\"],\n",
    "    # \"first_names\": [\" John\", \" Mary\", \" Bob\", \" Alice\"],\n",
    "    # \"places\": [\" London\", \" Paris\", \" Tokyo\"],\n",
    "    # \"exclamation_mark\": [\"!\"],\n",
    "    # \"other_punctuation\": [\".\", \",\", \"?\", \":\", \";\"],\n",
    "}\n",
    "# plot_histograms(\n",
    "#     pos_neg_dict, \n",
    "#     all_activations=sentiment_activations, \n",
    "#     layer=1, \n",
    "#     nbins=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962a280",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_topk(sentiment_activations, dataloader, model, k=20, layer=1, inclusions=pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf88010",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_topk(sentiment_activations, k=50, layer=1, inclusions=[\".\", \",\", \"?\", \":\", \";\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4e51e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# Means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07208a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def compute_mean_variance(\n",
    "    all_activations: Float[Tensor, \"row pos layer\"], layer: int, model: HookedTransformer,\n",
    "):\n",
    "    activations: Float[pd.Series, \"batch_and_pos\"] = pd.Series(all_activations[:, :, layer].flatten().cpu().numpy())\n",
    "    tokens: Int[pd.DataFrame, \"batch_and_pos\"] = dataloader.dataset.data.to_pandas().tokens.explode(ignore_index=True)\n",
    "    counts = tokens.value_counts()\n",
    "    means = activations.groupby(tokens).mean()\n",
    "    std_devs = activations.groupby(tokens).std()\n",
    "    return counts, means, std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae8910",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "token_counts, token_means, token_std_devs = compute_mean_variance(sentiment_activations, 1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e4156",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_top_mean_variance(\n",
    "    counts: pd.Series, means: pd.Series, std_devs: pd.Series, model: HookedTransformer, k: int = 10, \n",
    "    min_count: int = 10,\n",
    "):\n",
    "    means = means[counts >= min_count].dropna().sort_values()\n",
    "    std_devs = std_devs[counts >= min_count].dropna().sort_values()\n",
    "    means_top_and_bottom = pd.concat([means.head(k), means.tail(k)]).reset_index()\n",
    "    means_top_and_bottom['valence'] = [\"negative\"] * k + [\"positive\"] * k\n",
    "    means_top_and_bottom.columns = ['token', 'mean', 'valence']\n",
    "    means_top_and_bottom.token = [\n",
    "        f\"{i}:{tok}\" \n",
    "        for i, tok in zip(\n",
    "            means_top_and_bottom.token, \n",
    "            model.to_str_tokens(torch.tensor(means_top_and_bottom.token))\n",
    "        )\n",
    "    ]\n",
    "    fig = px.bar(data_frame=means_top_and_bottom, x='token', y='mean', color='valence')\n",
    "    fig.update_layout(title_text=\"Most extreme means\", title_x=0.5)\n",
    "    fig.show()\n",
    "    std_devs_top_and_bottom = pd.concat([std_devs.head(k), std_devs.tail(k)]).reset_index()\n",
    "    std_devs_top_and_bottom['variation'] = [\"consistent\"] * k + [\"variable\"] * k\n",
    "    std_devs_top_and_bottom.columns = ['token', 'std_dev', 'variation']\n",
    "    std_devs_top_and_bottom.token = [\n",
    "        f\"{i}:{tok}\" \n",
    "        for i, tok in zip(\n",
    "            std_devs_top_and_bottom.token, \n",
    "            model.to_str_tokens(torch.tensor(std_devs_top_and_bottom.token))\n",
    "        )\n",
    "    ]\n",
    "    fig = px.bar(data_frame=std_devs_top_and_bottom, x='token', y='std_dev', color='variation')\n",
    "    fig.update_layout(title_text=\"Most extreme standard deviations\", title_x=0.5)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9500a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_top_mean_variance(token_counts, token_means, token_std_devs, model=model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4e4e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plot_topk(sentiment_activations, k=10, layer=1, inclusions=[\" Yorkshire\"], window_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41885a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def resample_hook(\n",
    "    input: Float[Tensor, \"batch pos d_model\"], \n",
    "    hook: HookPoint, \n",
    "    direction: Float[Tensor, \"d_model\"],\n",
    "):\n",
    "    assert 'resid' in hook.name\n",
    "    assert direction.shape == (model.cfg.d_model,)\n",
    "    assert direction.norm().item() == 1.0\n",
    "    # shuffle input tensor along the batch dimension\n",
    "    shuffled = input[torch.randperm(input.shape[0])]\n",
    "    orig_proj: Float[Tensor, \"batch pos\"] = einops.einsum(\n",
    "        input, direction, 'batch pos d_model, d_model -> batch pos'\n",
    "    )\n",
    "    new_proj: Float[Tensor, \"batch pos\"] = einops.einsum(\n",
    "        shuffled, direction, 'batch pos d_model, d_model -> batch pos'\n",
    "    )\n",
    "    return (\n",
    "        input + (new_proj - orig_proj).unsqueeze(-1) * direction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe304d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def get_resample_ablated_loss_diffs(\n",
    "    direction: Float[Tensor, \"d_model\"],\n",
    "    model: HookedTransformer,\n",
    "    dataloader: DataLoader,\n",
    "    k: int = 10,\n",
    "    window_size: int = 10,\n",
    "    layer: int = 0,\n",
    "    seed: int = 0,\n",
    "    max_batch: int = None\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    model.reset_hooks()\n",
    "    hook = partial(resample_hook, direction=direction)\n",
    "    loss_diffs = []\n",
    "\n",
    "    bar = tqdm(dataloader, total=len(dataloader))\n",
    "    for batch_idx, batch_value in bar:\n",
    "        bar.set_description(f\"Batch {batch_idx}\")\n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "        model.reset_hooks()\n",
    "        orig_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        model.add_hook(\n",
    "            get_act_name('resid_post', layer),\n",
    "            hook,\n",
    "            dir=\"fwd\",\n",
    "        )\n",
    "        new_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        loss_diff: Float[Tensor, \"mb pos\"] = new_loss - orig_loss\n",
    "        loss_diffs.append(loss_diff)\n",
    "        model.reset_hooks()\n",
    "        if max_batch is not None and batch_idx + 1 >= max_batch:\n",
    "            break\n",
    "    loss_diffs = torch.cat(loss_diffs, dim=0)\n",
    "\n",
    "    return plot_topk(\n",
    "        loss_diffs, dataloader, model, k=k, layer=layer, window_size=window_size, centred=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b3032",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "loss_diff_text = get_resample_ablated_loss_diffs(sentiment_dir, model, dataloader, k=50, window_size=10)\n",
    "plot_neuroscope(''.join(loss_diff_text), centred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8e4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
