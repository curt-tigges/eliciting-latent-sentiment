{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df58c6a",
   "metadata": {},
   "source": [
    "# Mood Inference Circuit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04addc8d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed7069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attribution_Patching_Demo.ipynb\n",
      " CircuitsVis\n",
      " Dockerfile\n",
      " LICENSE\n",
      " README.md\n",
      " __pycache__\n",
      " adjective_token_lengths.txt\n",
      " ccs.py\n",
      " ccs_act_patching.py\n",
      " ccs_circuit_analysis.py\n",
      " ccs_circuit_attribution.py\n",
      " ccs_circuit_path_patching.py\n",
      " circuit.md\n",
      " circuit_analysis_classification_prompt_experimentation_pythia2_8b.ipynb\n",
      " circuit_analysis_contrastive_sentiment_gpt2_small.py\n",
      " circuit_analysis_restaurant_review_classification_pythia1_4b.ipynb\n",
      "'circuit_analysis_sentiment continuation_pythia1_4b.py'\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.py\n",
      " circuit_analysis_sentiment_continuation_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_contradiction_pythia1_4b.ipynb\n",
      "'circuit_analysis_simple single sentiment_gpt2_small.py'\n",
      " circuit_analysis_simple_sentiment_gpt2_small.ipynb\n",
      " circuit_analysis_simple_sentiment_gpt2_small.py\n",
      "'circuit_analysis_task comparison_pythia1_4b.ipynb'\n",
      " circuit_for_mood_binding_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_characteristic.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas_alt_prompt.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_situation.ipynb\n",
      " circuit_for_sentiment_classification_pythia1_4b.ipynb\n",
      " circuits\n",
      " classification.py\n",
      " classifier_accuracy.py\n",
      " comma_ablation_experiments.ipynb\n",
      " compare_lines.py\n",
      " data\n",
      " dataset_stats.py\n",
      " derivative_log_prob.py\n",
      " direct_linear_attribution.py\n",
      " direction_patching_results.py\n",
      " direction_patching_suite.py\n",
      " dlk\n",
      " environment.yml\n",
      " finetuning-imdb.ipynb\n",
      " fit_directions.py\n",
      " gpt2-simple_sentiment-act_patching_at_resid_stream_and_layer_outputs.pdf\n",
      " gpt2_imdb_classifier\n",
      " gpt2_imdb_test\n",
      " head_cosine_sim.py\n",
      " hp.txt\n",
      " imdb_balanced_subset\n",
      " imdb_neg_subset\n",
      " imdb_pos_subset\n",
      " imdb_zero_shot\n",
      " initial_exploration.py\n",
      " leace.py\n",
      " localising_by_direction.py\n",
      " mood_inference_names.txt\n",
      " movie_review_finetuning.ipynb\n",
      " negation_experiment.py\n",
      " neuron_directions.py\n",
      " openai_api_labels.py\n",
      " openai_api_labels_steering.py\n",
      " openwebtext_performance.py\n",
      " ov_unembed.py\n",
      " patching_at_resid_stream_and_layer_outputs.pdf\n",
      " path_patching.py\n",
      " plot_gpu_memory.py\n",
      " projection_neuroscope.py\n",
      " projection_neuroscope2.ipynb\n",
      " prompt_utils.py\n",
      " prompts.yaml\n",
      " pythia_classification_experiments.ipynb\n",
      " random_directions.py\n",
      " record_gpu_memory.sh\n",
      " resample_ablation.py\n",
      " resample_ablation_mood_inference.py\n",
      " sentiment_ablated_act_patching.py\n",
      " sst2\n",
      " sst_balanced_subset\n",
      " sst_neg_subset\n",
      " sst_pos_subset\n",
      " sst_zero_shot\n",
      " sst_zero_shot_EleutherAI_pythia-1.4b\n",
      " sst_zero_shot_EleutherAI_pythia-6.9b\n",
      " sst_zero_shot_gpt2-xl\n",
      " stanfordSentimentTreebank\n",
      " test_prompt.py\n",
      " tests\n",
      " treebank_data_gen.py\n",
      " utils\n",
      " wandb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745a76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'eliciting-latent-sentiment'\n",
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af695c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-xv5h08f4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-xv5h08f4\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit df9bfc252807e8b1c3a26c3c4796c18342c7fc71\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<3.0,>=2.0\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.9.0)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.2)\n",
      "Collecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (0.35.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-17.0.1.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: circuitsvis, lit\n",
      "  Building wheel for circuitsvis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuitsvis: filename=circuitsvis-0.0.0-py3-none-any.whl size=6170923 sha256=7101b262d0720ea697e8ca0933183e55c9d6b28a592d89e9c7f49c863ad572c6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4f4iz7kv/wheels/94/79/66/781b85e0732736078188d905010db6471f2787826da308336a\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.1-py3-none-any.whl size=93254 sha256=84ac8fe7caef55b2f2661a72c7060fe8f6e7d52bf8c9f2b81fb485a47c40ab49\n",
      "  Stored in directory: /root/.cache/pip/wheels/cb/cb/3e/f4a695a76daf22029d463b6ddcd3a58357c5fbf011b6aeb989\n",
      "Successfully built circuitsvis lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, importlib-metadata, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, circuitsvis\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed circuitsvis-0.0.0 cmake-3.27.5 importlib-metadata-5.2.0 lit-17.0.1 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformer_lens\n",
      "  Downloading transformer_lens-1.6.1-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.9/109.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops>=0.6.0\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Collecting transformers>=4.25.1\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.0.1)\n",
      "Collecting fancy-einsum>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting wandb>=0.13.5\n",
      "  Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jaxtyping>=0.2.11\n",
      "  Downloading jaxtyping-0.2.22-py3-none-any.whl (25 kB)\n",
      "Collecting datasets>=2.7.1\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.4.0)\n",
      "Collecting typeguard>=2.13.3\n",
      "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (66.1.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (17.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.27.5)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.19.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (5.2.0)\n",
      "Collecting typing-extensions>=3.7.4.1\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (3.11.0)\n",
      "Installing collected packages: safetensors, appdirs, typing-extensions, fancy-einsum, einops, beartype, typeguard, huggingface-hub, wandb, transformers, jaxtyping, datasets, transformer_lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 beartype-0.14.1 datasets-2.14.5 einops-0.6.1 fancy-einsum-0.0.3 huggingface-hub-0.17.3 jaxtyping-0.2.22 safetensors-0.3.3 transformer_lens-1.6.1 transformers-4.33.3 typeguard-4.1.5 typing-extensions-4.8.0 wandb-0.15.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jaxtyping==0.2.13\n",
      "  Downloading jaxtyping-0.2.13-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.8.0)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping==0.2.13) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping==0.2.13) (3.11.0)\n",
      "Installing collected packages: jaxtyping\n",
      "  Attempting uninstall: jaxtyping\n",
      "    Found existing installation: jaxtyping 0.2.22\n",
      "    Uninstalling jaxtyping-0.2.22:\n",
      "      Successfully uninstalled jaxtyping-0.2.22\n",
      "Successfully installed jaxtyping-0.2.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-5.17.0-py2.py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.17.0 tenacity-8.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtyping\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (4.1.5)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (2.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (66.1.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (17.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (3.27.5)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.11.1->torchtyping) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
      "Installing collected packages: torchtyping\n",
      "Successfully installed torchtyping-0.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-ej11fi36\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-ej11fi36\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (2.0.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.17.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (1.12)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.101)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (3.27.5)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (17.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->neel-plotly==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->neel-plotly==0.0.0) (1.3.0)\n",
      "Building wheels for collected packages: neel-plotly\n",
      "  Building wheel for neel-plotly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neel-plotly: filename=neel_plotly-0.0.0-py3-none-any.whl size=10186 sha256=4521b03c4082e2532341a0c18d55bc6cc1e65f682e953baf8c05055c68f186f6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlecotuo/wheels/e1/3c/c0/b5897c402b85e7fc329feb205ad5948b518f0423d891a79f7f\n",
      "Successfully built neel-plotly\n",
      "Installing collected packages: neel-plotly\n",
      "Successfully installed neel-plotly-0.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting kaleido\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "#!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "!pip install kaleido\n",
    "#%pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "#%pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67acf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6327938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "\n",
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set\n",
    "from rich import print as rprint\n",
    "\n",
    "from typing import List, Union\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching import Node, IterNode, path_patch, act_patch\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "from utils.visualization import get_attn_head_patterns, imshow_p, plot_attention_heads, scatter_attention_and_contribution_simple\n",
    "from utils.visualization import get_attn_pattern, plot_attention\n",
    "\n",
    "from utils.prompts import get_dataset\n",
    "from utils.circuit_analysis import get_logit_diff, logit_diff_denoising, logit_diff_noising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504aeb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f89a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly\n",
    "#plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbd9fe8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def two_lines(tensor1, tensor2, renderer=None, **kwargs):\n",
    "    px.line(y=[utils.to_numpy(tensor1), utils.to_numpy(tensor2)], **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4117f8",
   "metadata": {},
   "source": [
    "## Circuit Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41247d7a",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1fe429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name == \"EleutherAI/pythia-6.9b\" or model_name == \"StabilityAI/stablelm-tuned-alpha-7b\":\n",
    "        source_model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"model_cache\").to('cpu').bfloat16()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            model_name,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False,\n",
    "            tokenizer=tokenizer,\n",
    "            hf_model=source_model,\n",
    "        )\n",
    "    else:\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            model_name,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "438c880b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-1.4b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"EleutherAI/pythia-1.4b\")\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f53a6c5",
   "metadata": {},
   "source": [
    "### Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "158ccfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompts import CircularList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0d99afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \" John, Anne, Mark, Mary, Peter, Paul, James, Sarah, Mike, Tom, Carl, Sam, Sarah, Carl, Jack\"\n",
    "names = [n[1:] for n in model.to_str_tokens(names)[1::2]]\n",
    "names = CircularList(names)\n",
    "# for n in names:\n",
    "#     print(n)\n",
    "#     print(model.to_str_tokens(n))\n",
    "#     print(model.to_str_tokens(\" \" + n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42cf330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' John', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' Anne', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' Anne', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' John', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' quiet', ' private', ' library', '.', ' John', ' feels', ' very'] 36\n"
     ]
    }
   ],
   "source": [
    "orig_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. {names[i]} feels very\" for i in range(len(names))]\n",
    "name_flip_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. {names[i+1]} feels very\" for i in range(len(names))]\n",
    "\n",
    "ch_flip_prompts_a = [f\"{names[i]} hates parties, and avoids them whenever possible. {names[i+1]} loves parties, and joins them whenever possible. One day, they were invited to a grand gala. {names[i]} feels very\" for i in range(len(names))]\n",
    "ch_flip_prompts_b = [f\"{names[i]} hates parties, and avoids them whenever possible. {names[i+1]} loves parties, and joins them whenever possible. One day, they were invited to a grand gala. {names[i+1]} feels very\" for i in range(len(names))]\n",
    "\n",
    "sit_flip_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a quiet private library. {names[i]} feels very\" for i in range(len(names))]\n",
    "\n",
    "print(model.to_str_tokens(orig_prompts[0]), len(model.to_str_tokens(orig_prompts[0])))\n",
    "print(model.to_str_tokens(name_flip_prompts[0]), len(model.to_str_tokens(name_flip_prompts[0])))\n",
    "print(model.to_str_tokens(ch_flip_prompts_a[0]), len(model.to_str_tokens(ch_flip_prompts_a[0])))\n",
    "print(model.to_str_tokens(sit_flip_prompts[0]), len(model.to_str_tokens(sit_flip_prompts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fd51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_answers = [\" excited\", \" honored\"] #, \" amazing\", \" good\"]\n",
    "neg_answers = [\" nervous\", \" uneasy\"] #, \" terrible\", \" bad\"]\n",
    "batch_size = 4 * len(orig_prompts)\n",
    "n_pairs = 1\n",
    "\n",
    "def create_dataset_4way(orig_prompts_1, orig_prompts_2, flip_prompts_1, flip_prompts_2):\n",
    "    clean_prompts = []\n",
    "    corrupt_prompts = []\n",
    "    answer_tokens = torch.empty(\n",
    "            (batch_size, n_pairs, 2), \n",
    "            device=device, \n",
    "            dtype=torch.long\n",
    "        )\n",
    "    for i in range(len(orig_prompts)):\n",
    "        clean_prompts.append(orig_prompts_1[i])\n",
    "        clean_prompts.append(flip_prompts_1[i])\n",
    "        clean_prompts.append(orig_prompts_2[i])\n",
    "        clean_prompts.append(flip_prompts_2[i])\n",
    "\n",
    "        corrupt_prompts.append(flip_prompts_1[i])\n",
    "        corrupt_prompts.append(orig_prompts_1[i])\n",
    "        corrupt_prompts.append(flip_prompts_2[i])\n",
    "        corrupt_prompts.append(orig_prompts_2[i])\n",
    "\n",
    "        for pair_idx in range(n_pairs):\n",
    "                pos_token = model.to_single_token(pos_answers[pair_idx])\n",
    "                neg_token = model.to_single_token(neg_answers[pair_idx])\n",
    "                tokens_dict = {\n",
    "                    'positive': pos_token, \n",
    "                    'negative': neg_token, \n",
    "                }\n",
    "                answer_tokens[i * 4, pair_idx, 0] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4, pair_idx, 1] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 1, pair_idx, 0] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 1, pair_idx, 1] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 2, pair_idx, 0] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 2, pair_idx, 1] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 3, pair_idx, 0] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 3, pair_idx, 1] = tokens_dict['negative']\n",
    "\n",
    "    prompts_tokens: Float[Tensor, \"batch pos\"] = model.to_tokens(\n",
    "            clean_prompts, prepend_bos=True\n",
    "        )\n",
    "    clean_tokens = prompts_tokens.to(device)\n",
    "    corrupted_tokens = model.to_tokens(\n",
    "        corrupt_prompts, prepend_bos=True\n",
    "    ).to(device)\n",
    "\n",
    "    return clean_prompts, clean_tokens, corrupted_tokens, answer_tokens\n",
    "\n",
    "all_prompts, clean_tokens, corrupted_tokens, answer_tokens = create_dataset_4way(orig_prompts, name_flip_prompts, ch_flip_prompts_a, ch_flip_prompts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd5ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(clean_tokens.shape[0]):\n",
    "    print(model.to_string(clean_tokens[i]))\n",
    "    print(model.to_string(corrupted_tokens[i]))\n",
    "    print(model.to_str_tokens(answer_tokens[i][0]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dcd3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, torch.Size([60, 1, 2]), torch.Size([60, 36]), torch.Size([60, 36]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prompts), answer_tokens.shape, clean_tokens.shape, corrupted_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afef9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(1.0039, device='cuda:0') \n",
      "\n",
      "John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.1777, device='cuda:0') \n",
      "\n",
      "John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.0159, device='cuda:0') \n",
      "\n",
      "John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.3234, device='cuda:0') \n",
      "\n",
      "Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.6389, device='cuda:0') \n",
      "\n",
      "Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.2409, device='cuda:0') \n",
      "\n",
      "Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.2561, device='cuda:0') \n",
      "\n",
      "Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.3514, device='cuda:0') \n",
      "\n",
      "Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(1.1934, device='cuda:0') \n",
      "\n",
      "Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.1098, device='cuda:0') \n",
      "\n",
      "Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(0.4599, device='cuda:0') \n",
      "\n",
      "Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.9919, device='cuda:0') \n",
      "\n",
      "Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(1.0778, device='cuda:0') \n",
      "\n",
      "Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.1582, device='cuda:0') \n",
      "\n",
      "Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.0492, device='cuda:0') \n",
      "\n",
      "Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.6231, device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 16, 1):\n",
    "    logits, _ = model.run_with_cache(all_prompts[i])\n",
    "    log_diff = get_logit_diff(logits, answer_tokens[i].unsqueeze(0))\n",
    "    #if log_diff < 0.1:\n",
    "    print(all_prompts[i])\n",
    "    print(model.to_str_tokens(answer_tokens[i][0]))\n",
    "    print(log_diff, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4765b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = all_prompts[:16]\n",
    "answer_tokens = answer_tokens[:16]\n",
    "clean_tokens = clean_tokens[:16]\n",
    "corrupted_tokens = corrupted_tokens[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e609123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very',\n",
       " '<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(clean_tokens[3]), model.to_string(corrupted_tokens[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9e84c0",
   "metadata": {},
   "source": [
    "#### Logit Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50c19669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indices = [0, 3, 4, 7, 8, 11, 12, 15]\n",
    "neg_indices = [1, 2, 5, 6, 9, 10, 13, 14]\n",
    "name_1_indices = [0, 1, 4, 5, 8, 9, 12, 13]\n",
    "name_2_indices = [2, 3, 6, 7, 10, 11, 14, 15]\n",
    "pos_first_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "neg_first_indices = [1, 3, 5, 7, 9, 11, 13, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8adb4951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7755, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logits, pos_cache = model.run_with_cache(clean_tokens[pos_indices,:])\n",
    "pos_logit_diff = get_logit_diff(pos_logits, answer_tokens[pos_indices,:])\n",
    "pos_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c41dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0585, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_logits, neg_cache = model.run_with_cache(clean_tokens[neg_indices,:])\n",
    "neg_logit_diff = get_logit_diff(neg_logits, answer_tokens[neg_indices,:])\n",
    "neg_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d422344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0751, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_1_logits, name_1_cache = model.run_with_cache(clean_tokens[name_1_indices, :])\n",
    "name_1_logit_diff = get_logit_diff(name_1_logits, answer_tokens[name_1_indices, :])\n",
    "name_1_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fda79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7589, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_2_logits, name_2_cache = model.run_with_cache(clean_tokens[name_2_indices, :])\n",
    "name_2_logit_diff = get_logit_diff(name_2_logits, answer_tokens[name_2_indices, :])\n",
    "name_2_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c93a5dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9619, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_first_logits, pos_first_cache = model.run_with_cache(clean_tokens[pos_first_indices,:])\n",
    "pos_first_logit_diff = get_logit_diff(pos_first_logits, answer_tokens[pos_first_indices,:])\n",
    "pos_first_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b43a89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8721, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_first_logits, neg_first_cache = model.run_with_cache(clean_tokens[neg_first_indices,:])\n",
    "neg_first_logit_diff = get_logit_diff(neg_first_logits, answer_tokens[neg_first_indices,:])\n",
    "neg_first_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ce996af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9170, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_tokens, per_prompt=False)\n",
    "clean_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31adef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9170, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_tokens, per_prompt=False)\n",
    "corrupted_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89f27f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_denoising(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[Tensor, \"batch n_pairs 2\"] = answer_tokens,\n",
    "    flipped_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    return_tensor: bool = False,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is\n",
    "    same as on flipped input, and 1 when performance is same as on clean input.\n",
    "    '''\n",
    "    patched_logit_diff = get_logit_diff(logits, answer_tokens)\n",
    "    ld = ((patched_logit_diff - flipped_logit_diff) / (clean_logit_diff  - flipped_logit_diff))\n",
    "    if return_tensor:\n",
    "        return ld\n",
    "    else:\n",
    "        return ld.item()\n",
    "\n",
    "\n",
    "def logit_diff_noising(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        clean_logit_diff: float = clean_logit_diff,\n",
    "        corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "        answer_tokens: Float[Tensor, \"batch n_pairs 2\"] = answer_tokens,\n",
    "        return_tensor: bool = False,\n",
    "    ) -> float:\n",
    "        '''\n",
    "        We calibrate this so that the value is 0 when performance isn't harmed (i.e. same as IOI dataset),\n",
    "        and -1 when performance has been destroyed (i.e. is same as ABC dataset).\n",
    "        '''\n",
    "        patched_logit_diff = get_logit_diff(logits, answer_tokens)\n",
    "        ld = ((patched_logit_diff - clean_logit_diff) / (clean_logit_diff - corrupted_logit_diff))\n",
    "\n",
    "        if return_tensor:\n",
    "            return ld\n",
    "        else:\n",
    "            return ld.item()\n",
    "\n",
    "logit_diff_denoising_tensor = partial(logit_diff_denoising, return_tensor=True)\n",
    "logit_diff_noising_tensor = partial(logit_diff_noising, return_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b093f49b",
   "metadata": {},
   "source": [
    "### Ablation & Swap Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8912f",
   "metadata": {},
   "source": [
    "#### Patching Comma Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98bbdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stop_patch_pos = [torch.tensor([4,14], dtype=torch.long) for _ in range(len(all_prompts))]\n",
    "full_stop_patch_pos = torch.stack(full_stop_patch_pos, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b343e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d13eaaaa4e1418aa1ce511519e9b1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results['v'].shape = (layer=32, head=32)\n",
      "results['k'].shape = (layer=32, head=32)\n"
     ]
    }
   ],
   "source": [
    "results = act_patch(\n",
    "    model=model,\n",
    "    orig_input=corrupted_tokens,\n",
    "    new_cache=clean_cache,\n",
    "    patching_nodes=IterNode([\"v\",\"k\"], seq_pos=full_stop_patch_pos), # iterating over all heads' output in all layers\n",
    "    patching_metric=logit_diff_denoising,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fecb5e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff variation: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           -0.0013260109117254615,
           0.0005460045067593455,
           0.00039325322723016143,
           -0.0020410167053341866,
           -0.002138517564162612,
           0.002645521890372038,
           0.0039422824047505856,
           -0.001735514379106462,
           0.003363777883350849,
           0.000520004250574857,
           0.0011505094589665532,
           0.001264260383322835,
           0.0056550465524196625,
           -0.00153076252900064,
           -0.000910007453057915,
           -0.0013780113076791167,
           -0.002375769428908825,
           -0.002434269990772009,
           0.0006662554806098342,
           -0.004927040543407202,
           -0.0003965032519772649,
           -0.0014950123149901628,
           -0.006597553845494986,
           0.003224026644602418,
           -0.0006532553234137595,
           0.0007832564879208803,
           0.0016607637517154217,
           -0.0005362543743103743,
           0.003877281676977873,
           0.004598787985742092,
           0.0067178052850067616,
           0.001612013322301209
          ],
          [
           -0.004748289007693529,
           0.007829314097762108,
           -0.003071275306865573,
           -0.006435052491724491,
           0.0032175262458622456,
           -0.002210017992183566,
           -0.0026877720374614,
           -0.0006305052083916962,
           -0.0007345060585066676,
           0.008583320304751396,
           -0.000042250347178196535,
           -0.00004875040031038225,
           0.0006727555301040411,
           -0.001186259789392352,
           0.005824048072099686,
           -0.002697522286325693,
           -0.012980606406927109,
           -0.002457020105794072,
           0.006607304327189922,
           -0.0010595086496323347,
           -0.00016900138871278614,
           0.0021515176631510258,
           0.005258542951196432,
           -0.0025707711465656757,
           0.00927557609975338,
           -0.0030160248279571533,
           -0.002044266788288951,
           0.002964024432003498,
           -0.010221333242952824,
           -0.000939257733989507,
           0.0009587578242644668,
           0.003077775239944458
          ],
          [
           -0.00927557609975338,
           -0.0001917515619425103,
           -0.008407819084823132,
           0.005850048270076513,
           -0.0023822695948183537,
           -0.00004875040031038225,
           -0.0004777539288625121,
           -0.0026942722033709288,
           -0.004156784154474735,
           0.0008190067019313574,
           -0.0004907540278509259,
           0.029877495020627975,
           0.0005265043582767248,
           0.0003445028269197792,
           -0.002658521756529808,
           -0.0030810253228992224,
           -0.0021417676471173763,
           0.003126525552943349,
           -0.0017095140647143126,
           0.005343043711036444,
           0.013526611030101776,
           0.0038025311660021544,
           0.0031980264466255903,
           -0.0036790301091969013,
           0.000022750187781639397,
           0.006695055402815342,
           0.001381261390633881,
           0.002340019214898348,
           0.0008060066611506045,
           0.01167734619230032,
           -0.0016575135523453355,
           -0.005554295610636473
          ],
          [
           -0.0021417676471173763,
           -0.0020507669541984797,
           0.001322760945186019,
           -0.004803539719432592,
           -0.000975007947999984,
           0.00210926728323102,
           0.002639021724462509,
           -0.0038610317278653383,
           0.0019597660284489393,
           0.0034417782444506884,
           -0.0009132574778050184,
           -0.00200851634144783,
           -0.004137284122407436,
           -0.001170009607449174,
           -0.014729120768606663,
           -0.001381261390633881,
           -0.0024472700897604227,
           0.008476069197058678,
           -0.019165407866239548,
           0.003708280622959137,
           -0.0008580069988965988,
           -0.006285551469773054,
           0.00024050197680480778,
           0.0007995066116563976,
           0.005495795048773289,
           0.0006597554311156273,
           0.0009132574778050184,
           -0.0070525580085814,
           -0.0010367585346102715,
           -0.0016380134038627148,
           -0.0022295184899121523,
           -0.0032597766257822514
          ],
          [
           0.00011700095637934282,
           0.01414086576551199,
           0.03249376639723778,
           -0.015002124011516571,
           0.0007182558765634894,
           0.10066957771778107,
           -0.0016575135523453355,
           0.002869773656129837,
           0.00507004139944911,
           0.00589554850012064,
           -0.0005232543335296214,
           0.002564270980656147,
           0.0007897564209997654,
           -0.001430011703632772,
           -0.003077775239944458,
           0.0035685293842107058,
           0.00006825056334491819,
           -0.0013000107137486339,
           0.0002827523276209831,
           0.00006500053132185712,
           -0.0006207550759427249,
           -0.013074858114123344,
           0.029269739985466003,
           -0.003295527072623372,
           -0.0045370371080935,
           0.018333401530981064,
           0.0005622546304948628,
           0.010006831958889961,
           -0.0014787621330469847,
           0.00505704153329134,
           0.0067145549692213535,
           0.006399302277714014
          ],
          [
           -0.00012025098840240389,
           -0.0007962565287016332,
           0.015193874016404152,
           -0.004693038295954466,
           0.004345285706222057,
           0.003900031791999936,
           -0.02277618832886219,
           0.026851721107959747,
           -0.00016575136396568269,
           -0.003438528161495924,
           0.0019207658478990197,
           -0.0002535020757932216,
           0.000042250347178196535,
           -0.0068478062748909,
           -0.0004290034994482994,
           -0.004621537867933512,
           0.002203518059104681,
           -0.012769355438649654,
           -0.011586345732212067,
           0.013130106963217258,
           -0.00252852076664567,
           -0.007917065173387527,
           -0.006669054739177227,
           -0.0036822801921516657,
           -0.0038057812489569187,
           -0.006500053219497204,
           -0.007169559132307768,
           0.04026133194565773,
           0.0028665235731750727,
           0.021667927503585815,
           -0.012580853886902332,
           -0.0004550037265289575
          ],
          [
           0.00003900032243109308,
           0.057284969836473465,
           -0.040024079382419586,
           -0.005541295278817415,
           -0.00003900032243109308,
           0.007302810437977314,
           0.071474589407444,
           -0.001306510646827519,
           0.006126300431787968,
           0.017998648807406425,
           0.0031460258178412914,
           -0.00010725087486207485,
           -0.00037700310349464417,
           0.006126300431787968,
           -0.009688329882919788,
           0.002210017992183566,
           0.00010400085739092901,
           -0.014547118917107582,
           0.01911015808582306,
           0.0019272658973932266,
           0.0018070148071274161,
           -0.03526603803038597,
           -0.0011310093104839325,
           0.00965907983481884,
           -0.007315810304135084,
           -0.009340576827526093,
           -0.0171893909573555,
           -0.0018265149556100368,
           -0.0006110050017014146,
           0.0012415101518854499,
           -0.011726096272468567,
           0.01793689653277397
          ],
          [
           0.0012480103177949786,
           -0.03295852243900299,
           -0.006574803963303566,
           -0.004468786530196667,
           0.026893971487879753,
           0.004231534898281097,
           -0.011199591681361198,
           0.03994932770729065,
           -0.0034320279955863953,
           -0.0007247559260576963,
           -0.0030290246941149235,
           0.008625570684671402,
           -0.011206092312932014,
           -0.016370385885238647,
           -0.01040983572602272,
           -0.01914265751838684,
           -0.013204858638346195,
           -0.0009880081051960588,
           -0.004962790757417679,
           -0.0035100288223475218,
           -0.0010530087165534496,
           0.008817322552204132,
           0.00029575242660939693,
           -0.0005135042010806501,
           -0.011735846288502216,
           -0.0006987557280808687,
           0.00883032288402319,
           -0.002765772631391883,
           -0.001316260895691812,
           -0.007094807922840118,
           0.0036237798631191254,
           0.0053527941927313805
          ],
          [
           0.007725313305854797,
           0.01040983572602272,
           0.019958414137363434,
           -0.0020182665903121233,
           0.013617612421512604,
           0.011622095480561256,
           0.017829647287726402,
           -0.002609771443530917,
           -0.002577271079644561,
           0.00970457959920168,
           0.04762263968586922,
           0.027436725795269012,
           -0.0853879526257515,
           0.00016900138871278614,
           -0.008212816901504993,
           0.008947324007749557,
           -0.015112623572349548,
           -0.002882773755118251,
           -0.003922782372683287,
           0.007059057708829641,
           -0.009847580455243587,
           0.00579479755833745,
           0.01095259003341198,
           -0.006337551865726709,
           -0.024303698912262917,
           0.00013975115143693984,
           -0.017072390764951706,
           -0.0004387536027934402,
           -0.03876306861639023,
           0.34613433480262756,
           0.0008352568256668746,
           -0.0020540168043226004
          ],
          [
           0.00022750186326447874,
           -0.0023530193138867617,
           0.0110403411090374,
           0.004290034994482994,
           -0.006175050511956215,
           0.011225592344999313,
           -0.00022100182832218707,
           0.009766330011188984,
           -0.0016282635042443871,
           -0.0018655153689906001,
           -0.009535578079521656,
           0.007150058634579182,
           -0.06474702805280685,
           -0.00035425293026492,
           -0.008544320240616798,
           -0.002037766622379422,
           0.00036075295065529644,
           0.004543537274003029,
           0.022997189313173294,
           -0.0007865064544603229,
           0.009545328095555305,
           -0.043014101684093475,
           0.00029575242660939693,
           -0.005879298318177462,
           0.031694259494543076,
           -0.009620078839361668,
           -0.09167350083589554,
           0.006360302213579416,
           0.006045049522072077,
           -0.22012430429458618,
           -0.007484811823815107,
           -0.008450069464743137
          ],
          [
           0.39871978759765625,
           0.002934774151071906,
           -0.30459901690483093,
           -0.000809256627690047,
           0.000013000107173866127,
           0.003815531497821212,
           0.032113511115312576,
           -0.0011667595244944096,
           -0.005125292111188173,
           -0.005859797820448875,
           0.01596412993967533,
           0.006656054873019457,
           0.0038480316288769245,
           0.0003835031238850206,
           -0.006682054605334997,
           -0.026926470920443535,
           -0.10716637969017029,
           0.03034224733710289,
           0.10436160862445831,
           0.00022750186326447874,
           0.0016217633383348584,
           -0.2846730947494507,
           0.12615303695201874,
           -0.002782022813335061,
           -0.025158457458019257,
           0.1744094341993332,
           0.01985766366124153,
           0.010169333778321743,
           0.00027300225337967277,
           -0.003181776264682412,
           -0.006844555959105492,
           0.002564270980656147
          ],
          [
           -0.024040447548031807,
           0.01182684674859047,
           -0.24678102135658264,
           0.0010790089145302773,
           0.6775980591773987,
           -0.13342659175395966,
           -0.0019695162773132324,
           -0.0024472700897604227,
           0.00923007633537054,
           -0.008804322220385075,
           0.0021027671173214912,
           -0.0010172583861276507,
           0.00867432076483965,
           0.001459261984564364,
           -0.0016315134707838297,
           0.005755797028541565,
           0.00891807395964861,
           0.04130784049630165,
           -0.0005037541268393397,
           -0.009204075671732426,
           -0.004797039553523064,
           -0.0007215059013105929,
           -0.003207776229828596,
           -0.006256301421672106,
           0.0006662554806098342,
           -0.02689722180366516,
           -0.0017647644272074103,
           0.00918782502412796,
           -0.017059391364455223,
           -0.0003835031238850206,
           0.00611330009996891,
           -0.0005622546304948628
          ],
          [
           -0.008378569036722183,
           0.002593521261587739,
           -0.06324227154254913,
           -0.006441553123295307,
           0.026871221140027046,
           0.0009977581212297082,
           -0.07394785434007645,
           0.005963799078017473,
           -0.004894540179520845,
           0.0013390110107138753,
           0.0005915048532187939,
           -0.00034775285166688263,
           -0.004143784288316965,
           0.007387310266494751,
           -0.008268067613244057,
           -0.020832670852541924,
           0.007237809710204601,
           0.001092009013518691,
           -0.0011277592275291681,
           0.05336868762969971,
           0.09059450030326843,
           0.006630054209381342,
           0.06274176388978958,
           0.003363777883350849,
           -0.005408044438809156,
           0.004962790757417679,
           0.00013975115143693984,
           -0.0008417568751610816,
           0.0005557545810006559,
           -0.014771371148526669,
           -0.141808420419693,
           0.0016965139657258987
          ],
          [
           -0.0011375093599781394,
           0.0370340533554554,
           0.17114640772342682,
           -0.0005817547789774835,
           0.15375226736068726,
           0.000390003202483058,
           -0.004946540575474501,
           -0.0003282527031842619,
           -0.004026783164590597,
           -0.003708280622959137,
           -0.0009165075607597828,
           0.005040791351348162,
           0.0017972649075090885,
           0.06270276755094528,
           -0.0030387749429792166,
           0.005778547376394272,
           0.0026195214595645666,
           -0.0029737744480371475,
           -0.0000747506128391251,
           -0.025246208533644676,
           -0.0006500053568743169,
           0.007189059630036354,
           -0.0018362650880590081,
           0.34660884737968445,
           -0.002635771641507745,
           -0.002216518158093095,
           0.013841863721609116,
           -0.0012772604823112488,
           0.006097050383687019,
           -0.38277512788772583,
           0.009538828395307064,
           -0.04916965216398239
          ],
          [
           0.009213825687766075,
           0.0000747506128391251,
           -0.04012482985854149,
           -0.02153792791068554,
           0.0115895951166749,
           0.00014625120093114674,
           0.02145342528820038,
           0.0032825267408043146,
           -0.0006987557280808687,
           -0.00000975008060777327,
           0.000968508014921099,
           -0.0005752547294832766,
           -0.000195001601241529,
           0.0009262575767934322,
           0.00044200365664437413,
           0.0007670062477700412,
           -0.0016347634373232722,
           -0.0008450069581158459,
           0.0010757588315755129,
           -0.05676496773958206,
           -0.010416335426270962,
           -0.0012805105652660131,
           -0.00040625332621857524,
           0.0009815080557018518,
           -0.0011570095084607601,
           -0.0019857664592564106,
           0.01405311468988657,
           -0.0023205189500004053,
           0.0014137616381049156,
           0.006451302673667669,
           -0.0004940040525980294,
           0.00003250026566092856
          ],
          [
           -0.0007930065039545298,
           -0.0005232543335296214,
           -0.008011315949261189,
           0.00021775178902316839,
           -0.0028567733243107796,
           -0.0003802531282417476,
           0.0019012655830010772,
           0.011794347316026688,
           0.002297768834978342,
           -0.00025675210054032505,
           -0.022275682538747787,
           -0.0003835031238850206,
           -0.0002925024018622935,
           -0.0004842540074605495,
           -0.006870556622743607,
           0.0011277592275291681,
           -0.0011830097064375877,
           -0.0015047623310238123,
           0.000968508014921099,
           -0.0006402552826330066,
           -0.004007282666862011,
           -0.00025675210054032505,
           -0.0006467553321272135,
           0.004283535294234753,
           0.41767069697380066,
           -0.0005005041020922363,
           -0.00170626409817487,
           -0.0012415101518854499,
           0.002411519642919302,
           0.0032337764278054237,
           0.0008580069988965988,
           0.00018200150225311518
          ],
          [
           -0.0008970073540695012,
           -0.0008580069988965988,
           -0.001105008996091783,
           -0.0007280060090124607,
           0.003077775239944458,
           -0.0003510028764139861,
           0.007933314889669418,
           0.050645165145397186,
           0.00008775071910349652,
           -0.0007507561822421849,
           -0.001771264593116939,
           -0.0004647538298740983,
           -0.000013000107173866127,
           0.00027625224902294576,
           -0.0003737530787475407,
           0.0002535020757932216,
           -0.0035067787393927574,
           -0.00009425077587366104,
           0.0007345060585066676,
           -0.06077225133776665,
           -0.0016932639991864562,
           0.0007475061574950814,
           -0.0004192534543108195,
           -0.00012025098840240389,
           -0.0013780113076791167,
           0.0011147592449560761,
           -0.010669837705790997,
           0.0032175262458622456,
           0.018362650647759438,
           -0.00027300225337967277,
           -0.0015762628754600883,
           -0.00037050305400043726
          ],
          [
           -0.0003802531282417476,
           0.004878289997577667,
           0.0019760162103921175,
           -0.0015372626949101686,
           -0.00021775178902316839,
           -0.0008905072463676333,
           -0.002080017002299428,
           -0.0002145017497241497,
           0.00012025098840240389,
           -0.000487503973999992,
           -0.0008255067514255643,
           -0.0011245092609897256,
           -0.0002860023523680866,
           0.0062238010577857494,
           0.0013422609772533178,
           0.0002145017497241497,
           0.003165526082739234,
           -0.00010075083264382556,
           0.00040300333057530224,
           0.0005525044980458915,
           0.000243751986999996,
           -0.000022750187781639397,
           -0.00044525362318381667,
           -0.0001722514134598896,
           -0.0010172583861276507,
           -0.004985541105270386,
           -0.00018525152700021863,
           -0.0007312560337595642,
           -0.001140759326517582,
           -0.0019207658478990197,
           -0.0005135042010806501,
           -0.00033150272793136537
          ],
          [
           0.056394461542367935,
           0.0002827523276209831,
           -0.0008645071065984666,
           0.00011050091416109353,
           -0.0001430011761840433,
           -0.0037927809171378613,
           0.0021710179280489683,
           0,
           0.0004777539288625121,
           0.00015925129991956055,
           -0.002983524464070797,
           -0.0005167542258277535,
           -0.0015697629423812032,
           -0.0002242518385173753,
           -0.0009457577834837139,
           -0.8505807518959045,
           -0.00003575029404601082,
           -0.0011960098054260015,
           -0.0002632521791383624,
           -0.003714780556038022,
           -0.0004647538298740983,
           -0.0004680038255173713,
           -0.0012740103993564844,
           -0.00005200042869546451,
           -0.00010725087486207485,
           -0.0007507561822421849,
           -0.0006565054063685238,
           -0.00039325322723016143,
           0.0002827523276209831,
           0.02209693193435669,
           0.004166534170508385,
           -0.004108033608645201
          ],
          [
           0.0002145017497241497,
           0.00025025205104611814,
           -0.0025675210636109114,
           0.00009425077587366104,
           -0.00012350101314950734,
           -0.0009425077005289495,
           -0.0001430011761840433,
           -0.0003445028269197792,
           -0.0016705136513337493,
           -0.00016575136396568269,
           0.3546331524848938,
           0.00006175050657475367,
           0.00032500267843715847,
           -0.0011082590790465474,
           -0.0020475168712437153,
           -0.000390003202483058,
           -0.00029575242660939693,
           -0.00007150058809202164,
           0.0004647538298740983,
           0.010832338593900204,
           -0.0005525044980458915,
           -0.00030225247610360384,
           0.000321752653690055,
           0.0003445028269197792,
           -0.00001950016121554654,
           -0.007413310930132866,
           -0.000022750187781639397,
           -0.002037766622379422,
           -0.00010725087486207485,
           -0.00013975115143693984,
           0.018700653687119484,
           -0.0006760055548511446
          ],
          [
           0.00027300225337967277,
           -0.00009100075112655759,
           -0.0007637562812305987,
           -0.0008190067019313574,
           -0.003607529681175947,
           -0.0005232543335296214,
           0.00015275125042535365,
           0.0004387536027934402,
           -0.0026877720374614,
           -0.00011375093163223937,
           -0.000045500375563278794,
           0.0001267510378966108,
           -0.0003152526041958481,
           -0.00012350101314950734,
           -0.00023400191275868565,
           -0.001618513255380094,
           0.011852847412228584,
           -0.00045175370178185403,
           -0.0002925024018622935,
           -0.0004290034994482994,
           -0.000679255579598248,
           0,
           0.000520004250574857,
           -0.0004647538298740983,
           0.0017647644272074103,
           -0.00039325322723016143,
           -0.0004582537803798914,
           0.0013032606802880764,
           -0.00021775178902316839,
           -0.00031200257944874465,
           0.004400536417961121,
           0.001595763023942709
          ],
          [
           0.0004095033509656787,
           -0.0016445135697722435,
           -0.00023725195205770433,
           -0.00003900032243109308,
           -0.00028925237711519003,
           -0.0013910114066675305,
           -0.0003802531282417476,
           -0.00015925129991956055,
           -0.0005817547789774835,
           0.0035685293842107058,
           0.0003380027774255723,
           0.00011050091416109353,
           -0.0005265043582767248,
           0.0011082590790465474,
           -0.0018882654840126634,
           0.01591538079082966,
           0.00007150058809202164,
           -0.0001787514629540965,
           0.00017550143820699304,
           0.0003380027774255723,
           -0.00004875040031038225,
           0.00035425293026492,
           0.0003282527031842619,
           -0.0000975008006207645,
           -0.0028080230113118887,
           -0.0005037541268393397,
           -0.000243751986999996,
           -0.0003185025998391211,
           0.00023075190256349742,
           -0.0003867531777359545,
           -0.0016607637517154217,
           -0.00031200257944874465
          ],
          [
           0.0001787514629540965,
           0.0006045049522072077,
           0.00001950016121554654,
           0.00027300225337967277,
           -0.000013000107173866127,
           -0.0003835031238850206,
           0.0000032500267934665317,
           0.00008450069435639307,
           -0.000055250457080546767,
           -0.00022100182832218707,
           -0.0029900246299803257,
           -0.00023075190256349742,
           0.0015275125624611974,
           -0.000042250347178196535,
           -0.001163509557954967,
           0.00020800171478185803,
           0.0005102541763335466,
           0.000029250239094835706,
           0.00027950230287387967,
           -0.00020150166528765112,
           0.0002600021252874285,
           0.001192759838886559,
           -0.00047125385026447475,
           0.00024050197680480778,
           0.00017550143820699304,
           0.0006240051588974893,
           0.000614255026448518,
           -0.00011375093163223937,
           -0.00013000106264371425,
           0.0003380027774255723,
           -0.0003055025008507073,
           0.00024050197680480778
          ],
          [
           0.00011700095637934282,
           0.0001787514629540965,
           -0.0004322535532992333,
           -0.00017550143820699304,
           -0.00015275125042535365,
           -0.00041600342956371605,
           -0.000006500053586933063,
           -0.00012350101314950734,
           -0.00020475167548283935,
           -0.00013650112668983638,
           -0.0001722514134598896,
           -0.0010725087486207485,
           0.00012025098840240389,
           -0.0008775072055868804,
           0.0003802531282417476,
           0.00040300333057530224,
           -0.0005720047047361732,
           0.00033150272793136537,
           0.00033150272793136537,
           -0.00027625224902294576,
           0,
           0.0007507561822421849,
           -0.00024050197680480778,
           -0.0008970073540695012,
           0.00015275125042535365,
           0.00040300333057530224,
           -0.000013000107173866127,
           -0.00015600128972437233,
           -0.0004485036770347506,
           -0.00010400085739092901,
           0.00008125066960928962,
           -0.0004582537803798914
          ],
          [
           -0.0001787514629540965,
           0.0017517644446343184,
           -0.00034775285166688263,
           0.00028925237711519003,
           0.000042250347178196535,
           -0.0004680038255173713,
           0.00027300225337967277,
           -0.0001495012256782502,
           0.00021775178902316839,
           0.0007962565287016332,
           0.00035425293026492,
           -0.0008450069581158459,
           0.003207776229828596,
           0.00006500053132185712,
           -0.002262018620967865,
           0.000045500375563278794,
           0.00003900032243109308,
           0.0005590046057477593,
           0.00000975008060777327,
           0.00003250026566092856,
           -0.000243751986999996,
           0.00012350101314950734,
           -0.00007800064486218616,
           0.00008775071910349652,
           -0.00020150166528765112,
           -0.0003672530292533338,
           -0.0000032500267934665317,
           -0.00022100182832218707,
           -0.00025025205104611814,
           -0.000006500053586933063,
           -0.00036075295065529644,
           0.00023725195205770433
          ],
          [
           0.007046057842671871,
           0.0002990024513565004,
           -0.000357502925908193,
           -0.00023725195205770433,
           -0.00011375093163223937,
           -0.0011245092609897256,
           -0.00025025205104611814,
           -0.0001267510378966108,
           -0.00007150058809202164,
           -0.00036075295065529644,
           -0.00008775071910349652,
           -0.0002827523276209831,
           -0.00003575029404601082,
           -0.00018525152700021863,
           0.0007312560337595642,
           -0.0001430011761840433,
           -0.0001917515619425103,
           -0.00011375093163223937,
           -0.00016250133921857923,
           -0.00017550143820699304,
           0.0000975008006207645,
           0.0004680038255173713,
           -0.00020150166528765112,
           -0.0002665021747816354,
           0.0001917515619425103,
           -0.000013000107173866127,
           -0.00011050091416109353,
           -0.0004972540773451328,
           0.0003510028764139861,
           -0.00001625013283046428,
           -0.00001625013283046428,
           -0.00046150380512699485
          ],
          [
           -0.0001332510873908177,
           -0.0001787514629540965,
           -0.00012350101314950734,
           0.0001722514134598896,
           -0.0001430011761840433,
           -0.00023400191275868565,
           -0.00015600128972437233,
           -0.0006175050511956215,
           0.004709288477897644,
           -0.000042250347178196535,
           -0.00001625013283046428,
           -0.0011310093104839325,
           -0.00005850047818967141,
           -0.003337777452543378,
           0.09147850424051285,
           0.00013650112668983638,
           -0.00015925129991956055,
           -0.00041275337571278214,
           -0.0006630054558627307,
           -0.000614255026448518,
           -0.0008970073540695012,
           0.0017485142452642322,
           -0.00030225247610360384,
           -0.0005915048532187939,
           0.00022750186326447874,
           -0.00037050305400043726,
           -0.00030875252559781075,
           -0.002632521791383624,
           -0.00036400300450623035,
           0.00013975115143693984,
           -0.000042250347178196535,
           -0.00022750186326447874
          ],
          [
           0.000013000107173866127,
           0.00021775178902316839,
           -0.00008775071910349652,
           -0.00022750186326447874,
           -0.00013975115143693984,
           0.0002535020757932216,
           0.0000032500267934665317,
           -0.0008970073540695012,
           0.000055250457080546767,
           -0.00013000106264371425,
           -0.00017550143820699304,
           -0.000022750187781639397,
           -0.0000032500267934665317,
           -0.0009360076510347426,
           0.00001950016121554654,
           -0.0004907540278509259,
           0.00006825056334491819,
           -0.00016250133921857923,
           -0.00023725195205770433,
           0.001235010102391243,
           -0.0001722514134598896,
           0.00008775071910349652,
           0,
           0.00006825056334491819,
           -0.00001625013283046428,
           0.0001332510873908177,
           -0.0003445028269197792,
           0.00042250347905792296,
           -0.0001787514629540965,
           0.00012025098840240389,
           0.00001950016121554654,
           0.00007150058809202164
          ],
          [
           -0.0007117558852769434,
           -0.00016575136396568269,
           0.00007150058809202164,
           0.0001332510873908177,
           -0.0003737530787475407,
           0.00011700095637934282,
           0.0000747506128391251,
           -0.000357502925908193,
           0.0001787514629540965,
           -0.0002470020262990147,
           -0.00041600342956371605,
           -0.0001722514134598896,
           -0.0007540062069892883,
           -0.00236926949582994,
           0.00011700095637934282,
           -0.000055250457080546767,
           -0.0006110050017014146,
           -0.00003250026566092856,
           -0.000013000107173866127,
           -0.0004680038255173713,
           0.00014625120093114674,
           0,
           0.00011050091416109353,
           0.000013000107173866127,
           -0.00006500053132185712,
           -0.0002827523276209831,
           0.0000032500267934665317,
           0.0025837712455540895,
           -0.0007832564879208803,
           0.00005850047818967141,
           -0.0006597554311156273,
           -0.0001787514629540965
          ],
          [
           -0.00001950016121554654,
           -0.00010725087486207485,
           0.000022750187781639397,
           0.0004940040525980294,
           -0.0002632521791383624,
           -0.0012610104167833924,
           -0.000013000107173866127,
           -0.00027950230287387967,
           -0.00005850047818967141,
           0.00004875040031038225,
           0.00011700095637934282,
           -0.00020150166528765112,
           -0.0001787514629540965,
           -0.000055250457080546767,
           0.00001950016121554654,
           -0.000029250239094835706,
           -0.00025025205104611814,
           -0.00006825056334491819,
           -0.0005330043495632708,
           0.00007150058809202164,
           0.00010075083264382556,
           -0.00009100075112655759,
           -0.0008807572303339839,
           0.00027300225337967277,
           0.0001495012256782502,
           -0.00019825162598863244,
           -0.00005850047818967141,
           -0.00001950016121554654,
           -0.00006500053132185712,
           0.00015600128972437233,
           -0.010013332590460777,
           0.0000032500267934665317
          ],
          [
           0.0000032500267934665317,
           0.0000032500267934665317,
           -0.00010400085739092901,
           -0.00005850047818967141,
           -0.000029250239094835706,
           -0.00004875040031038225,
           -0.00005200042869546451,
           -0.000022750187781639397,
           -0.00001625013283046428,
           -0.0003737530787475407,
           -0.00001625013283046428,
           -0.00009100075112655759,
           -0.00018200150225311518,
           0.00005200042869546451,
           -0.00000975008060777327,
           0.00015600128972437233,
           0,
           -0.0009717579814605415,
           -0.0000032500267934665317,
           -0.00018525152700021863,
           -0.00005850047818967141,
           0.00009425077587366104,
           -0.00013650112668983638,
           -0.00010075083264382556,
           0,
           -0.00018200150225311518,
           -0.00015275125042535365,
           -0.000013000107173866127,
           -0.00035425293026492,
           0.00013000106264371425,
           -0.00005850047818967141,
           -0.0008970073540695012
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "ticksuffix": "%",
          "title": {
           "text": "Logit diff variation"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Patching output of attention heads (corrupted -> clean)"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"e3f68124-9f8a-4199-a4d4-2c26376d4794\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e3f68124-9f8a-4199-a4d4-2c26376d4794\")) {                    Plotly.newPlot(                        \"e3f68124-9f8a-4199-a4d4-2c26376d4794\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[-0.0013260109117254615,0.0005460045067593455,0.00039325322723016143,-0.0020410167053341866,-0.002138517564162612,0.002645521890372038,0.0039422824047505856,-0.001735514379106462,0.003363777883350849,0.000520004250574857,0.0011505094589665532,0.001264260383322835,0.0056550465524196625,-0.00153076252900064,-0.000910007453057915,-0.0013780113076791167,-0.002375769428908825,-0.002434269990772009,0.0006662554806098342,-0.004927040543407202,-0.0003965032519772649,-0.0014950123149901628,-0.006597553845494986,0.003224026644602418,-0.0006532553234137595,0.0007832564879208803,0.0016607637517154217,-0.0005362543743103743,0.003877281676977873,0.004598787985742092,0.0067178052850067616,0.001612013322301209],[-0.004748289007693529,0.007829314097762108,-0.003071275306865573,-0.006435052491724491,0.0032175262458622456,-0.002210017992183566,-0.0026877720374614,-0.0006305052083916962,-0.0007345060585066676,0.008583320304751396,-4.2250347178196535e-05,-4.875040031038225e-05,0.0006727555301040411,-0.001186259789392352,0.005824048072099686,-0.002697522286325693,-0.012980606406927109,-0.002457020105794072,0.006607304327189922,-0.0010595086496323347,-0.00016900138871278614,0.0021515176631510258,0.005258542951196432,-0.0025707711465656757,0.00927557609975338,-0.0030160248279571533,-0.002044266788288951,0.002964024432003498,-0.010221333242952824,-0.000939257733989507,0.0009587578242644668,0.003077775239944458],[-0.00927557609975338,-0.0001917515619425103,-0.008407819084823132,0.005850048270076513,-0.0023822695948183537,-4.875040031038225e-05,-0.0004777539288625121,-0.0026942722033709288,-0.004156784154474735,0.0008190067019313574,-0.0004907540278509259,0.029877495020627975,0.0005265043582767248,0.0003445028269197792,-0.002658521756529808,-0.0030810253228992224,-0.0021417676471173763,0.003126525552943349,-0.0017095140647143126,0.005343043711036444,0.013526611030101776,0.0038025311660021544,0.0031980264466255903,-0.0036790301091969013,2.2750187781639397e-05,0.006695055402815342,0.001381261390633881,0.002340019214898348,0.0008060066611506045,0.01167734619230032,-0.0016575135523453355,-0.005554295610636473],[-0.0021417676471173763,-0.0020507669541984797,0.001322760945186019,-0.004803539719432592,-0.000975007947999984,0.00210926728323102,0.002639021724462509,-0.0038610317278653383,0.0019597660284489393,0.0034417782444506884,-0.0009132574778050184,-0.00200851634144783,-0.004137284122407436,-0.001170009607449174,-0.014729120768606663,-0.001381261390633881,-0.0024472700897604227,0.008476069197058678,-0.019165407866239548,0.003708280622959137,-0.0008580069988965988,-0.006285551469773054,0.00024050197680480778,0.0007995066116563976,0.005495795048773289,0.0006597554311156273,0.0009132574778050184,-0.0070525580085814,-0.0010367585346102715,-0.0016380134038627148,-0.0022295184899121523,-0.0032597766257822514],[0.00011700095637934282,0.01414086576551199,0.03249376639723778,-0.015002124011516571,0.0007182558765634894,0.10066957771778107,-0.0016575135523453355,0.002869773656129837,0.00507004139944911,0.00589554850012064,-0.0005232543335296214,0.002564270980656147,0.0007897564209997654,-0.001430011703632772,-0.003077775239944458,0.0035685293842107058,6.825056334491819e-05,-0.0013000107137486339,0.0002827523276209831,6.500053132185712e-05,-0.0006207550759427249,-0.013074858114123344,0.029269739985466003,-0.003295527072623372,-0.0045370371080935,0.018333401530981064,0.0005622546304948628,0.010006831958889961,-0.0014787621330469847,0.00505704153329134,0.0067145549692213535,0.006399302277714014],[-0.00012025098840240389,-0.0007962565287016332,0.015193874016404152,-0.004693038295954466,0.004345285706222057,0.003900031791999936,-0.02277618832886219,0.026851721107959747,-0.00016575136396568269,-0.003438528161495924,0.0019207658478990197,-0.0002535020757932216,4.2250347178196535e-05,-0.0068478062748909,-0.0004290034994482994,-0.004621537867933512,0.002203518059104681,-0.012769355438649654,-0.011586345732212067,0.013130106963217258,-0.00252852076664567,-0.007917065173387527,-0.006669054739177227,-0.0036822801921516657,-0.0038057812489569187,-0.006500053219497204,-0.007169559132307768,0.04026133194565773,0.0028665235731750727,0.021667927503585815,-0.012580853886902332,-0.0004550037265289575],[3.900032243109308e-05,0.057284969836473465,-0.040024079382419586,-0.005541295278817415,-3.900032243109308e-05,0.007302810437977314,0.071474589407444,-0.001306510646827519,0.006126300431787968,0.017998648807406425,0.0031460258178412914,-0.00010725087486207485,-0.00037700310349464417,0.006126300431787968,-0.009688329882919788,0.002210017992183566,0.00010400085739092901,-0.014547118917107582,0.01911015808582306,0.0019272658973932266,0.0018070148071274161,-0.03526603803038597,-0.0011310093104839325,0.00965907983481884,-0.007315810304135084,-0.009340576827526093,-0.0171893909573555,-0.0018265149556100368,-0.0006110050017014146,0.0012415101518854499,-0.011726096272468567,0.01793689653277397],[0.0012480103177949786,-0.03295852243900299,-0.006574803963303566,-0.004468786530196667,0.026893971487879753,0.004231534898281097,-0.011199591681361198,0.03994932770729065,-0.0034320279955863953,-0.0007247559260576963,-0.0030290246941149235,0.008625570684671402,-0.011206092312932014,-0.016370385885238647,-0.01040983572602272,-0.01914265751838684,-0.013204858638346195,-0.0009880081051960588,-0.004962790757417679,-0.0035100288223475218,-0.0010530087165534496,0.008817322552204132,0.00029575242660939693,-0.0005135042010806501,-0.011735846288502216,-0.0006987557280808687,0.00883032288402319,-0.002765772631391883,-0.001316260895691812,-0.007094807922840118,0.0036237798631191254,0.0053527941927313805],[0.007725313305854797,0.01040983572602272,0.019958414137363434,-0.0020182665903121233,0.013617612421512604,0.011622095480561256,0.017829647287726402,-0.002609771443530917,-0.002577271079644561,0.00970457959920168,0.04762263968586922,0.027436725795269012,-0.0853879526257515,0.00016900138871278614,-0.008212816901504993,0.008947324007749557,-0.015112623572349548,-0.002882773755118251,-0.003922782372683287,0.007059057708829641,-0.009847580455243587,0.00579479755833745,0.01095259003341198,-0.006337551865726709,-0.024303698912262917,0.00013975115143693984,-0.017072390764951706,-0.0004387536027934402,-0.03876306861639023,0.34613433480262756,0.0008352568256668746,-0.0020540168043226004],[0.00022750186326447874,-0.0023530193138867617,0.0110403411090374,0.004290034994482994,-0.006175050511956215,0.011225592344999313,-0.00022100182832218707,0.009766330011188984,-0.0016282635042443871,-0.0018655153689906001,-0.009535578079521656,0.007150058634579182,-0.06474702805280685,-0.00035425293026492,-0.008544320240616798,-0.002037766622379422,0.00036075295065529644,0.004543537274003029,0.022997189313173294,-0.0007865064544603229,0.009545328095555305,-0.043014101684093475,0.00029575242660939693,-0.005879298318177462,0.031694259494543076,-0.009620078839361668,-0.09167350083589554,0.006360302213579416,0.006045049522072077,-0.22012430429458618,-0.007484811823815107,-0.008450069464743137],[0.39871978759765625,0.002934774151071906,-0.30459901690483093,-0.000809256627690047,1.3000107173866127e-05,0.003815531497821212,0.032113511115312576,-0.0011667595244944096,-0.005125292111188173,-0.005859797820448875,0.01596412993967533,0.006656054873019457,0.0038480316288769245,0.0003835031238850206,-0.006682054605334997,-0.026926470920443535,-0.10716637969017029,0.03034224733710289,0.10436160862445831,0.00022750186326447874,0.0016217633383348584,-0.2846730947494507,0.12615303695201874,-0.002782022813335061,-0.025158457458019257,0.1744094341993332,0.01985766366124153,0.010169333778321743,0.00027300225337967277,-0.003181776264682412,-0.006844555959105492,0.002564270980656147],[-0.024040447548031807,0.01182684674859047,-0.24678102135658264,0.0010790089145302773,0.6775980591773987,-0.13342659175395966,-0.0019695162773132324,-0.0024472700897604227,0.00923007633537054,-0.008804322220385075,0.0021027671173214912,-0.0010172583861276507,0.00867432076483965,0.001459261984564364,-0.0016315134707838297,0.005755797028541565,0.00891807395964861,0.04130784049630165,-0.0005037541268393397,-0.009204075671732426,-0.004797039553523064,-0.0007215059013105929,-0.003207776229828596,-0.006256301421672106,0.0006662554806098342,-0.02689722180366516,-0.0017647644272074103,0.00918782502412796,-0.017059391364455223,-0.0003835031238850206,0.00611330009996891,-0.0005622546304948628],[-0.008378569036722183,0.002593521261587739,-0.06324227154254913,-0.006441553123295307,0.026871221140027046,0.0009977581212297082,-0.07394785434007645,0.005963799078017473,-0.004894540179520845,0.0013390110107138753,0.0005915048532187939,-0.00034775285166688263,-0.004143784288316965,0.007387310266494751,-0.008268067613244057,-0.020832670852541924,0.007237809710204601,0.001092009013518691,-0.0011277592275291681,0.05336868762969971,0.09059450030326843,0.006630054209381342,0.06274176388978958,0.003363777883350849,-0.005408044438809156,0.004962790757417679,0.00013975115143693984,-0.0008417568751610816,0.0005557545810006559,-0.014771371148526669,-0.141808420419693,0.0016965139657258987],[-0.0011375093599781394,0.0370340533554554,0.17114640772342682,-0.0005817547789774835,0.15375226736068726,0.000390003202483058,-0.004946540575474501,-0.0003282527031842619,-0.004026783164590597,-0.003708280622959137,-0.0009165075607597828,0.005040791351348162,0.0017972649075090885,0.06270276755094528,-0.0030387749429792166,0.005778547376394272,0.0026195214595645666,-0.0029737744480371475,-7.47506128391251e-05,-0.025246208533644676,-0.0006500053568743169,0.007189059630036354,-0.0018362650880590081,0.34660884737968445,-0.002635771641507745,-0.002216518158093095,0.013841863721609116,-0.0012772604823112488,0.006097050383687019,-0.38277512788772583,0.009538828395307064,-0.04916965216398239],[0.009213825687766075,7.47506128391251e-05,-0.04012482985854149,-0.02153792791068554,0.0115895951166749,0.00014625120093114674,0.02145342528820038,0.0032825267408043146,-0.0006987557280808687,-9.75008060777327e-06,0.000968508014921099,-0.0005752547294832766,-0.000195001601241529,0.0009262575767934322,0.00044200365664437413,0.0007670062477700412,-0.0016347634373232722,-0.0008450069581158459,0.0010757588315755129,-0.05676496773958206,-0.010416335426270962,-0.0012805105652660131,-0.00040625332621857524,0.0009815080557018518,-0.0011570095084607601,-0.0019857664592564106,0.01405311468988657,-0.0023205189500004053,0.0014137616381049156,0.006451302673667669,-0.0004940040525980294,3.250026566092856e-05],[-0.0007930065039545298,-0.0005232543335296214,-0.008011315949261189,0.00021775178902316839,-0.0028567733243107796,-0.0003802531282417476,0.0019012655830010772,0.011794347316026688,0.002297768834978342,-0.00025675210054032505,-0.022275682538747787,-0.0003835031238850206,-0.0002925024018622935,-0.0004842540074605495,-0.006870556622743607,0.0011277592275291681,-0.0011830097064375877,-0.0015047623310238123,0.000968508014921099,-0.0006402552826330066,-0.004007282666862011,-0.00025675210054032505,-0.0006467553321272135,0.004283535294234753,0.41767069697380066,-0.0005005041020922363,-0.00170626409817487,-0.0012415101518854499,0.002411519642919302,0.0032337764278054237,0.0008580069988965988,0.00018200150225311518],[-0.0008970073540695012,-0.0008580069988965988,-0.001105008996091783,-0.0007280060090124607,0.003077775239944458,-0.0003510028764139861,0.007933314889669418,0.050645165145397186,8.775071910349652e-05,-0.0007507561822421849,-0.001771264593116939,-0.0004647538298740983,-1.3000107173866127e-05,0.00027625224902294576,-0.0003737530787475407,0.0002535020757932216,-0.0035067787393927574,-9.425077587366104e-05,0.0007345060585066676,-0.06077225133776665,-0.0016932639991864562,0.0007475061574950814,-0.0004192534543108195,-0.00012025098840240389,-0.0013780113076791167,0.0011147592449560761,-0.010669837705790997,0.0032175262458622456,0.018362650647759438,-0.00027300225337967277,-0.0015762628754600883,-0.00037050305400043726],[-0.0003802531282417476,0.004878289997577667,0.0019760162103921175,-0.0015372626949101686,-0.00021775178902316839,-0.0008905072463676333,-0.002080017002299428,-0.0002145017497241497,0.00012025098840240389,-0.000487503973999992,-0.0008255067514255643,-0.0011245092609897256,-0.0002860023523680866,0.0062238010577857494,0.0013422609772533178,0.0002145017497241497,0.003165526082739234,-0.00010075083264382556,0.00040300333057530224,0.0005525044980458915,0.000243751986999996,-2.2750187781639397e-05,-0.00044525362318381667,-0.0001722514134598896,-0.0010172583861276507,-0.004985541105270386,-0.00018525152700021863,-0.0007312560337595642,-0.001140759326517582,-0.0019207658478990197,-0.0005135042010806501,-0.00033150272793136537],[0.056394461542367935,0.0002827523276209831,-0.0008645071065984666,0.00011050091416109353,-0.0001430011761840433,-0.0037927809171378613,0.0021710179280489683,0.0,0.0004777539288625121,0.00015925129991956055,-0.002983524464070797,-0.0005167542258277535,-0.0015697629423812032,-0.0002242518385173753,-0.0009457577834837139,-0.8505807518959045,-3.575029404601082e-05,-0.0011960098054260015,-0.0002632521791383624,-0.003714780556038022,-0.0004647538298740983,-0.0004680038255173713,-0.0012740103993564844,-5.200042869546451e-05,-0.00010725087486207485,-0.0007507561822421849,-0.0006565054063685238,-0.00039325322723016143,0.0002827523276209831,0.02209693193435669,0.004166534170508385,-0.004108033608645201],[0.0002145017497241497,0.00025025205104611814,-0.0025675210636109114,9.425077587366104e-05,-0.00012350101314950734,-0.0009425077005289495,-0.0001430011761840433,-0.0003445028269197792,-0.0016705136513337493,-0.00016575136396568269,0.3546331524848938,6.175050657475367e-05,0.00032500267843715847,-0.0011082590790465474,-0.0020475168712437153,-0.000390003202483058,-0.00029575242660939693,-7.150058809202164e-05,0.0004647538298740983,0.010832338593900204,-0.0005525044980458915,-0.00030225247610360384,0.000321752653690055,0.0003445028269197792,-1.950016121554654e-05,-0.007413310930132866,-2.2750187781639397e-05,-0.002037766622379422,-0.00010725087486207485,-0.00013975115143693984,0.018700653687119484,-0.0006760055548511446],[0.00027300225337967277,-9.100075112655759e-05,-0.0007637562812305987,-0.0008190067019313574,-0.003607529681175947,-0.0005232543335296214,0.00015275125042535365,0.0004387536027934402,-0.0026877720374614,-0.00011375093163223937,-4.5500375563278794e-05,0.0001267510378966108,-0.0003152526041958481,-0.00012350101314950734,-0.00023400191275868565,-0.001618513255380094,0.011852847412228584,-0.00045175370178185403,-0.0002925024018622935,-0.0004290034994482994,-0.000679255579598248,0.0,0.000520004250574857,-0.0004647538298740983,0.0017647644272074103,-0.00039325322723016143,-0.0004582537803798914,0.0013032606802880764,-0.00021775178902316839,-0.00031200257944874465,0.004400536417961121,0.001595763023942709],[0.0004095033509656787,-0.0016445135697722435,-0.00023725195205770433,-3.900032243109308e-05,-0.00028925237711519003,-0.0013910114066675305,-0.0003802531282417476,-0.00015925129991956055,-0.0005817547789774835,0.0035685293842107058,0.0003380027774255723,0.00011050091416109353,-0.0005265043582767248,0.0011082590790465474,-0.0018882654840126634,0.01591538079082966,7.150058809202164e-05,-0.0001787514629540965,0.00017550143820699304,0.0003380027774255723,-4.875040031038225e-05,0.00035425293026492,0.0003282527031842619,-9.75008006207645e-05,-0.0028080230113118887,-0.0005037541268393397,-0.000243751986999996,-0.0003185025998391211,0.00023075190256349742,-0.0003867531777359545,-0.0016607637517154217,-0.00031200257944874465],[0.0001787514629540965,0.0006045049522072077,1.950016121554654e-05,0.00027300225337967277,-1.3000107173866127e-05,-0.0003835031238850206,3.2500267934665317e-06,8.450069435639307e-05,-5.5250457080546767e-05,-0.00022100182832218707,-0.0029900246299803257,-0.00023075190256349742,0.0015275125624611974,-4.2250347178196535e-05,-0.001163509557954967,0.00020800171478185803,0.0005102541763335466,2.9250239094835706e-05,0.00027950230287387967,-0.00020150166528765112,0.0002600021252874285,0.001192759838886559,-0.00047125385026447475,0.00024050197680480778,0.00017550143820699304,0.0006240051588974893,0.000614255026448518,-0.00011375093163223937,-0.00013000106264371425,0.0003380027774255723,-0.0003055025008507073,0.00024050197680480778],[0.00011700095637934282,0.0001787514629540965,-0.0004322535532992333,-0.00017550143820699304,-0.00015275125042535365,-0.00041600342956371605,-6.500053586933063e-06,-0.00012350101314950734,-0.00020475167548283935,-0.00013650112668983638,-0.0001722514134598896,-0.0010725087486207485,0.00012025098840240389,-0.0008775072055868804,0.0003802531282417476,0.00040300333057530224,-0.0005720047047361732,0.00033150272793136537,0.00033150272793136537,-0.00027625224902294576,0.0,0.0007507561822421849,-0.00024050197680480778,-0.0008970073540695012,0.00015275125042535365,0.00040300333057530224,-1.3000107173866127e-05,-0.00015600128972437233,-0.0004485036770347506,-0.00010400085739092901,8.125066960928962e-05,-0.0004582537803798914],[-0.0001787514629540965,0.0017517644446343184,-0.00034775285166688263,0.00028925237711519003,4.2250347178196535e-05,-0.0004680038255173713,0.00027300225337967277,-0.0001495012256782502,0.00021775178902316839,0.0007962565287016332,0.00035425293026492,-0.0008450069581158459,0.003207776229828596,6.500053132185712e-05,-0.002262018620967865,4.5500375563278794e-05,3.900032243109308e-05,0.0005590046057477593,9.75008060777327e-06,3.250026566092856e-05,-0.000243751986999996,0.00012350101314950734,-7.800064486218616e-05,8.775071910349652e-05,-0.00020150166528765112,-0.0003672530292533338,-3.2500267934665317e-06,-0.00022100182832218707,-0.00025025205104611814,-6.500053586933063e-06,-0.00036075295065529644,0.00023725195205770433],[0.007046057842671871,0.0002990024513565004,-0.000357502925908193,-0.00023725195205770433,-0.00011375093163223937,-0.0011245092609897256,-0.00025025205104611814,-0.0001267510378966108,-7.150058809202164e-05,-0.00036075295065529644,-8.775071910349652e-05,-0.0002827523276209831,-3.575029404601082e-05,-0.00018525152700021863,0.0007312560337595642,-0.0001430011761840433,-0.0001917515619425103,-0.00011375093163223937,-0.00016250133921857923,-0.00017550143820699304,9.75008006207645e-05,0.0004680038255173713,-0.00020150166528765112,-0.0002665021747816354,0.0001917515619425103,-1.3000107173866127e-05,-0.00011050091416109353,-0.0004972540773451328,0.0003510028764139861,-1.625013283046428e-05,-1.625013283046428e-05,-0.00046150380512699485],[-0.0001332510873908177,-0.0001787514629540965,-0.00012350101314950734,0.0001722514134598896,-0.0001430011761840433,-0.00023400191275868565,-0.00015600128972437233,-0.0006175050511956215,0.004709288477897644,-4.2250347178196535e-05,-1.625013283046428e-05,-0.0011310093104839325,-5.850047818967141e-05,-0.003337777452543378,0.09147850424051285,0.00013650112668983638,-0.00015925129991956055,-0.00041275337571278214,-0.0006630054558627307,-0.000614255026448518,-0.0008970073540695012,0.0017485142452642322,-0.00030225247610360384,-0.0005915048532187939,0.00022750186326447874,-0.00037050305400043726,-0.00030875252559781075,-0.002632521791383624,-0.00036400300450623035,0.00013975115143693984,-4.2250347178196535e-05,-0.00022750186326447874],[1.3000107173866127e-05,0.00021775178902316839,-8.775071910349652e-05,-0.00022750186326447874,-0.00013975115143693984,0.0002535020757932216,3.2500267934665317e-06,-0.0008970073540695012,5.5250457080546767e-05,-0.00013000106264371425,-0.00017550143820699304,-2.2750187781639397e-05,-3.2500267934665317e-06,-0.0009360076510347426,1.950016121554654e-05,-0.0004907540278509259,6.825056334491819e-05,-0.00016250133921857923,-0.00023725195205770433,0.001235010102391243,-0.0001722514134598896,8.775071910349652e-05,0.0,6.825056334491819e-05,-1.625013283046428e-05,0.0001332510873908177,-0.0003445028269197792,0.00042250347905792296,-0.0001787514629540965,0.00012025098840240389,1.950016121554654e-05,7.150058809202164e-05],[-0.0007117558852769434,-0.00016575136396568269,7.150058809202164e-05,0.0001332510873908177,-0.0003737530787475407,0.00011700095637934282,7.47506128391251e-05,-0.000357502925908193,0.0001787514629540965,-0.0002470020262990147,-0.00041600342956371605,-0.0001722514134598896,-0.0007540062069892883,-0.00236926949582994,0.00011700095637934282,-5.5250457080546767e-05,-0.0006110050017014146,-3.250026566092856e-05,-1.3000107173866127e-05,-0.0004680038255173713,0.00014625120093114674,0.0,0.00011050091416109353,1.3000107173866127e-05,-6.500053132185712e-05,-0.0002827523276209831,3.2500267934665317e-06,0.0025837712455540895,-0.0007832564879208803,5.850047818967141e-05,-0.0006597554311156273,-0.0001787514629540965],[-1.950016121554654e-05,-0.00010725087486207485,2.2750187781639397e-05,0.0004940040525980294,-0.0002632521791383624,-0.0012610104167833924,-1.3000107173866127e-05,-0.00027950230287387967,-5.850047818967141e-05,4.875040031038225e-05,0.00011700095637934282,-0.00020150166528765112,-0.0001787514629540965,-5.5250457080546767e-05,1.950016121554654e-05,-2.9250239094835706e-05,-0.00025025205104611814,-6.825056334491819e-05,-0.0005330043495632708,7.150058809202164e-05,0.00010075083264382556,-9.100075112655759e-05,-0.0008807572303339839,0.00027300225337967277,0.0001495012256782502,-0.00019825162598863244,-5.850047818967141e-05,-1.950016121554654e-05,-6.500053132185712e-05,0.00015600128972437233,-0.010013332590460777,3.2500267934665317e-06],[3.2500267934665317e-06,3.2500267934665317e-06,-0.00010400085739092901,-5.850047818967141e-05,-2.9250239094835706e-05,-4.875040031038225e-05,-5.200042869546451e-05,-2.2750187781639397e-05,-1.625013283046428e-05,-0.0003737530787475407,-1.625013283046428e-05,-9.100075112655759e-05,-0.00018200150225311518,5.200042869546451e-05,-9.75008060777327e-06,0.00015600128972437233,0.0,-0.0009717579814605415,-3.2500267934665317e-06,-0.00018525152700021863,-5.850047818967141e-05,9.425077587366104e-05,-0.00013650112668983638,-0.00010075083264382556,0.0,-0.00018200150225311518,-0.00015275125042535365,-1.3000107173866127e-05,-0.00035425293026492,0.00013000106264371425,-5.850047818967141e-05,-0.0008970073540695012]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff variation: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff variation\"},\"ticksuffix\":\"%\"},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Patching output of attention heads (corrupted -\\u003e clean)\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e3f68124-9f8a-4199-a4d4-2c26376d4794');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_p(\n",
    "    results['k'] * 100,\n",
    "    title=\"Patching output of attention heads (corrupted -> clean)\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff variation\"},\n",
    "    coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d91eef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d685db4a4cd44d059856df6f17425787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results['z'].shape = (layer=32, head=32)\n"
     ]
    }
   ],
   "source": [
    "results = path_patch(\n",
    "    model,\n",
    "    orig_input=clean_tokens,\n",
    "    new_input=corrupted_tokens,\n",
    "    sender_nodes=IterNode('z', seq_pos=full_stop_patch_pos),\n",
    "    receiver_nodes=[Node(\"v\", layer, head=head) for layer, head in MID_IAM_CM_ATTN_HEADS],\n",
    "    patching_metric=logit_diff_noising,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59174694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff variation: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           -0.002551270881667733,
           0.0003737530787475407,
           0.01291560661047697,
           0.0013130108127370477,
           0.0004095033509656787,
           -0.003919532056897879,
           -0.0019240158144384623,
           0.0005297543248161674,
           -0.0002990024513565004,
           0.0004322535532992333,
           -0.0003380027774255723,
           -0.0005460045067593455,
           0.0035652793012559414,
           0.0009945081546902657,
           0.00001950016121554654,
           0.00001625013283046428,
           0.002609771443530917,
           0.004862040281295776,
           -0.012125849723815918,
           0.0010985089465975761,
           0.0007345060585066676,
           -0.010923339985311031,
           -0.012610103003680706,
           -0.0024960206355899572,
           -0.0003445028269197792,
           0.002616271609440446,
           -0.00010400085739092901,
           0.000487503973999992,
           0.000744256132747978,
           0.0004192534543108195,
           -0.005554295610636473,
           -0.0009295076597481966
          ],
          [
           0.001335760927759111,
           -0.007416561245918274,
           0.0033117772545665503,
           0.0010140083031728864,
           -0.00226851855404675,
           -0.003298776922747493,
           0.004556537605822086,
           0.0010985089465975761,
           -0.0035587793681770563,
           -0.0007020057528279722,
           0.005226042587310076,
           0.0011797597398981452,
           -0.000744256132747978,
           -0.004010532982647419,
           0.002717022318392992,
           -0.00013000106264371425,
           0.002372519578784704,
           -0.004748289007693529,
           -0.03278626874089241,
           0.0014430118026211858,
           0.00007150058809202164,
           -0.007315810304135084,
           -0.015941381454467773,
           0.002232768340036273,
           0.012756354175508022,
           0.0026487717404961586,
           0.0008970073540695012,
           0.0015632627764716744,
           -0.0016380134038627148,
           0.0029217738192528486,
           -0.0037960312329232693,
           -0.0012545103672891855
          ],
          [
           -0.0036530299112200737,
           -0.0014462618855759501,
           0.0007605062564834952,
           -0.0003672530292533338,
           -0.00007800064486218616,
           0.00342552806250751,
           0.0008417568751610816,
           0.002538271015509963,
           0.0011797597398981452,
           -0.00039325322723016143,
           0.007358060218393803,
           -0.0012415101518854499,
           -0.0045142872259020805,
           -0.0008905072463676333,
           0.005765547510236502,
           -0.003295527072623372,
           0.000520004250574857,
           0.0026617718394845724,
           -0.0016152632888406515,
           0.0011310093104839325,
           0.0009815080557018518,
           0.00041600342956371605,
           -0.0018980156164616346,
           0.0007702563307248056,
           -0.010877839289605618,
           -0.0002827523276209831,
           0.002080017002299428,
           0.0006402552826330066,
           0.012223349884152412,
           -0.0013715112581849098,
           0.007579062134027481,
           -0.00011050091416109353
          ],
          [
           -0.0007052557775750756,
           -0.019383160397410393,
           -0.0018232649890705943,
           0.006019049324095249,
           0.01610713265836239,
           -0.0010010082041844726,
           0.0023237690329551697,
           -0.000006500053586933063,
           0.0017030138988047838,
           0.01614288240671158,
           -0.00025675210054032505,
           0.0037440306041389704,
           0.0004972540773451328,
           0.0013130108127370477,
           0.002310768933966756,
           -0.00039325322723016143,
           -0.006275801453739405,
           0.00044525362318381667,
           -0.002239268273115158,
           0.00023725195205770433,
           0.005983299110084772,
           0.0012252600863575935,
           -0.00027950230287387967,
           -0.0011732595739886165,
           -0.0005525044980458915,
           0.00014625120093114674,
           -0.01873965375125408,
           -0.0010952589800581336,
           -0.002632521791383624,
           -0.00831356830894947,
           -0.016584886237978935,
           -0.0012967606307938695
          ],
          [
           -0.0014690121170133352,
           0.003919532056897879,
           -0.006321301683783531,
           0.0014852621825411916,
           0.01849590241909027,
           0.0011440094094723463,
           -0.02003641426563263,
           0.013130106963217258,
           0.0052682929672300816,
           -0.0007117558852769434,
           0.02241218462586403,
           -0.00001950016121554654,
           0.0007897564209997654,
           0.0004907540278509259,
           -0.002336769364774227,
           0.00064350530738011,
           0.0001430011761840433,
           0.0033897776156663895,
           -0.00010400085739092901,
           -0.0006175050511956215,
           -0.0015860130079090595,
           -0.0006662554806098342,
           0.016126632690429688,
           0.002876273589208722,
           -0.024238698184490204,
           -0.04533137381076813,
           0.002635771641507745,
           0.020562918856739998,
           0.002216518158093095,
           -0.003454778576269746,
           0.00007150058809202164,
           -0.014768121764063835
          ],
          [
           -0.005183792673051357,
           0.0031037754379212856,
           -0.009129324927926064,
           0.016831889748573303,
           0.00821931753307581,
           -0.02262018620967865,
           -0.001105008996091783,
           -0.006899806670844555,
           -0.006604054011404514,
           0.0003445028269197792,
           -0.025593960657715797,
           0.011943847872316837,
           -0.07600187510251999,
           -0.00030875252559781075,
           -0.026191966608166695,
           -0.013672861270606518,
           -0.011550595052540302,
           0.004813289735466242,
           -0.025935212150216103,
           0.038350313901901245,
           0.08257993310689926,
           -0.006487053353339434,
           -0.0003185025998391211,
           0.007419811096042395,
           -0.006058049388229847,
           -0.002411519642919302,
           -0.2550230920314789,
           -0.013289359398186207,
           -0.011514844372868538,
           0.002645521890372038,
           -0.003006274811923504,
           0.0033215275034308434
          ],
          [
           0.008430569432675838,
           0.0053755440749228,
           0.027235224843025208,
           0.0064708031713962555,
           0.002457020105794072,
           -0.00030875252559781075,
           0.02080342173576355,
           0.002249018521979451,
           0.010091332718729973,
           0.0016315134707838297,
           0.0191459059715271,
           0.03344602510333061,
           0.0004290034994482994,
           -0.012070599012076855,
           -0.07816964387893677,
           -0.008248567581176758,
           -0.002249018521979451,
           0.0010432585841044784,
           0.0005980049027130008,
           -0.00473853899165988,
           0.003779781050980091,
           0.0034157780464738607,
           0.006526053883135319,
           -0.060723502188920975,
           -0.00043550357804633677,
           0.0008970073540695012,
           0.0041145337745547295,
           -0.011043591424822807,
           -0.02125517465174198,
           0.00001950016121554654,
           0.0250154547393322,
           0.0007637562812305987
          ],
          [
           0.009607079438865185,
           -0.00915207527577877,
           -0.0024180198088288307,
           0.02583121322095394,
           -0.003666030243039131,
           -0.011768346652388573,
           -0.014407368376851082,
           -0.001482012216001749,
           0.0006370051996782422,
           0.015473376959562302,
           -0.03193476423621178,
           0.00044525362318381667,
           0.0009522577747702599,
           -0.002060516970232129,
           -0.0019012655830010772,
           0.029263241216540337,
           -0.002249018521979451,
           0.029269739985466003,
           -0.09151099622249603,
           0.008232317864894867,
           -0.048812150955200195,
           0.006412303075194359,
           -0.029237238690257072,
           -0.0005655046552419662,
           0.008463069796562195,
           0.009122825227677822,
           -0.035818543285131454,
           -0.020111165940761566,
           0.02179792895913124,
           0.0018037648405879736,
           -0.014170116744935513,
           -0.009145575575530529
          ],
          [
           -0.0018525151535868645,
           0.00030875252559781075,
           0.0030745253898203373,
           0.039660073816776276,
           0.054294947534799576,
           0.00012025098840240389,
           0.00803406536579132,
           -0.013500610366463661,
           0.0016705136513337493,
           -0.032552268356084824,
           0.02049141749739647,
           0.04238034784793854,
           0.019678911194205284,
           -0.011625345796346664,
           -0.020114416256546974,
           -0.026906970888376236,
           0.056391216814517975,
           -0.008784822188317776,
           -0.0015405126614496112,
           0.009126074612140656,
           0.006175050511956215,
           0.001719264080747962,
           0.012792105786502361,
           -0.00969482958316803,
           0.0017452642787247896,
           -0.04347235709428787,
           0.022012431174516678,
           0.0041535343043506145,
           0.002882773755118251,
           -0.022434933111071587,
           0.02513570711016655,
           -0.007029807660728693
          ],
          [
           -0.02270793542265892,
           0.02417369745671749,
           0.009051323868334293,
           -0.01933440938591957,
           -0.03165201097726822,
           0.0036952802911400795,
           0.00283402344211936,
           -0.0068315560929477215,
           -0.00696155708283186,
           0.024222448468208313,
           0.013910114765167236,
           0.04195459559559822,
           -0.0014202616875991225,
           0.015827631577849388,
           0.005833798088133335,
           -0.00820956751704216,
           0.0005785047542303801,
           0.00908382423222065,
           0.028931736946105957,
           -0.051818426698446274,
           -0.0048717898316681385,
           0.0019402659963816404,
           -0.006942057516425848,
           -0.01095259003341198,
           0.004296535160392523,
           0.0006857556290924549,
           -0.0051382919773459435,
           0.006558554247021675,
           0.03460303321480751,
           0.033147022128105164,
           -0.14559145271778107,
           -0.0275114756077528
          ],
          [
           -0.003604279365390539,
           0.02167442813515663,
           0.03650429844856262,
           -0.014088866300880909,
           0.0027495224494487047,
           0.009421827271580696,
           -0.014153866097331047,
           -0.017351893708109856,
           -0.021599676460027695,
           -0.03981932997703552,
           -0.003360527567565441,
           0.0142448665574193,
           0.002453770022839308,
           -0.011755346320569515,
           0.08478344231843948,
           0.0005330043495632708,
           -0.18318450450897217,
           0.0014040115056559443,
           -0.02646496891975403,
           0.016188383102416992,
           -0.1308298259973526,
           0.19544686377048492,
           0.10488486289978027,
           0.006906306836754084,
           0.004777539521455765,
           0.011947098188102245,
           -6.032134056091309,
           0.01633463427424431,
           0.0023497692309319973,
           0.0855114534497261,
           -0.07869614660739899,
           0.11586995422840118
          ],
          [
           0.019067907705903053,
           -0.025860462337732315,
           0.13927990198135376,
           -0.06761355698108673,
           0.004023532848805189,
           0.050820667296648026,
           0.004338785540312529,
           -0.007748064119368792,
           -0.043417107313871384,
           0.005300793331116438,
           -0.004338785540312529,
           0.01084858924150467,
           -0.060567498207092285,
           -0.00789431482553482,
           0.009932081215083599,
           -0.07049957662820816,
           -1.7499314546585083,
           -1.1661258935928345,
           0.17853371798992157,
           0.007413310930132866,
           -0.0006110050017014146,
           0.04435311257839203,
           -0.012512602843344212,
           0.004670288413763046,
           -0.3149438500404358,
           -0.2831488251686096,
           0.03889956697821617,
           0.03262701630592346,
           0.005775297526270151,
           0.034648533910512924,
           -0.007036307826638222,
           0.019227158278226852
          ],
          [
           0.023952696472406387,
           0.04959540814161301,
           -0.011228841729462147,
           0.0018687653355300426,
           -0.798027753829956,
           -0.012363102287054062,
           0.003666030243039131,
           -0.0017290142131969333,
           -0.0028957738541066647,
           -0.000585004803724587,
           0.004134033806622028,
           -0.0004842540074605495,
           -0.0016932639991864562,
           -0.0022230183240026236,
           -0.011033840477466583,
           -0.0011245092609897256,
           -0.02012091502547264,
           -0.00908382423222065,
           -0.006269301287829876,
           -0.012935105711221695,
           0.037914812564849854,
           0.002431020140647888,
           -0.02158992737531662,
           0.005833798088133335,
           0.0014625120675191283,
           -0.01008808333426714,
           -0.0029900246299803257,
           -0.006145800463855267,
           -0.015749629586935043,
           -0.0008970073540695012,
           -0.004543537274003029,
           0.005950798746198416
          ],
          [
           0.004439536482095718,
           -0.013942615129053593,
           0.01094284001737833,
           0.028499484062194824,
           0.014186366461217403,
           0.0038902820087969303,
           -0.011436844244599342,
           0.0020572668872773647,
           0.029945746064186096,
           0.009418576955795288,
           0.007286560256034136,
           0.00452403724193573,
           -0.029194990172982216,
           0.02246418409049511,
           -0.006152300164103508,
           0.006945306900888681,
           0.022022180259227753,
           -0.0729338526725769,
           0.0015242625959217548,
           -0.0012805105652660131,
           0.09138750284910202,
           0.0006727555301040411,
           0.037144556641578674,
           0.0021190172992646694,
           0.004553287290036678,
           0.005053791217505932,
           0.00010725087486207485,
           -0.003068024991080165,
           0.0008287567761726677,
           0.04121033847332001,
           -0.12340676039457321,
           0.022854186594486237
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "ticksuffix": "%",
          "title": {
           "text": "Logit diff variation"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Direct effect on Intermediate AE Heads' values by comma token patching"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"d8a27450-0d82-458a-9e81-5483b1dad0aa\" class=\"plotly-graph-div\" style=\"height:525px; width:700px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d8a27450-0d82-458a-9e81-5483b1dad0aa\")) {                    Plotly.newPlot(                        \"d8a27450-0d82-458a-9e81-5483b1dad0aa\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.002551270881667733,0.0003737530787475407,0.01291560661047697,0.0013130108127370477,0.0004095033509656787,-0.003919532056897879,-0.0019240158144384623,0.0005297543248161674,-0.0002990024513565004,0.0004322535532992333,-0.0003380027774255723,-0.0005460045067593455,0.0035652793012559414,0.0009945081546902657,1.950016121554654e-05,1.625013283046428e-05,0.002609771443530917,0.004862040281295776,-0.012125849723815918,0.0010985089465975761,0.0007345060585066676,-0.010923339985311031,-0.012610103003680706,-0.0024960206355899572,-0.0003445028269197792,0.002616271609440446,-0.00010400085739092901,0.000487503973999992,0.000744256132747978,0.0004192534543108195,-0.005554295610636473,-0.0009295076597481966],[0.001335760927759111,-0.007416561245918274,0.0033117772545665503,0.0010140083031728864,-0.00226851855404675,-0.003298776922747493,0.004556537605822086,0.0010985089465975761,-0.0035587793681770563,-0.0007020057528279722,0.005226042587310076,0.0011797597398981452,-0.000744256132747978,-0.004010532982647419,0.002717022318392992,-0.00013000106264371425,0.002372519578784704,-0.004748289007693529,-0.03278626874089241,0.0014430118026211858,7.150058809202164e-05,-0.007315810304135084,-0.015941381454467773,0.002232768340036273,0.012756354175508022,0.0026487717404961586,0.0008970073540695012,0.0015632627764716744,-0.0016380134038627148,0.0029217738192528486,-0.0037960312329232693,-0.0012545103672891855],[-0.0036530299112200737,-0.0014462618855759501,0.0007605062564834952,-0.0003672530292533338,-7.800064486218616e-05,0.00342552806250751,0.0008417568751610816,0.002538271015509963,0.0011797597398981452,-0.00039325322723016143,0.007358060218393803,-0.0012415101518854499,-0.0045142872259020805,-0.0008905072463676333,0.005765547510236502,-0.003295527072623372,0.000520004250574857,0.0026617718394845724,-0.0016152632888406515,0.0011310093104839325,0.0009815080557018518,0.00041600342956371605,-0.0018980156164616346,0.0007702563307248056,-0.010877839289605618,-0.0002827523276209831,0.002080017002299428,0.0006402552826330066,0.012223349884152412,-0.0013715112581849098,0.007579062134027481,-0.00011050091416109353],[-0.0007052557775750756,-0.019383160397410393,-0.0018232649890705943,0.006019049324095249,0.01610713265836239,-0.0010010082041844726,0.0023237690329551697,-6.500053586933063e-06,0.0017030138988047838,0.01614288240671158,-0.00025675210054032505,0.0037440306041389704,0.0004972540773451328,0.0013130108127370477,0.002310768933966756,-0.00039325322723016143,-0.006275801453739405,0.00044525362318381667,-0.002239268273115158,0.00023725195205770433,0.005983299110084772,0.0012252600863575935,-0.00027950230287387967,-0.0011732595739886165,-0.0005525044980458915,0.00014625120093114674,-0.01873965375125408,-0.0010952589800581336,-0.002632521791383624,-0.00831356830894947,-0.016584886237978935,-0.0012967606307938695],[-0.0014690121170133352,0.003919532056897879,-0.006321301683783531,0.0014852621825411916,0.01849590241909027,0.0011440094094723463,-0.02003641426563263,0.013130106963217258,0.0052682929672300816,-0.0007117558852769434,0.02241218462586403,-1.950016121554654e-05,0.0007897564209997654,0.0004907540278509259,-0.002336769364774227,0.00064350530738011,0.0001430011761840433,0.0033897776156663895,-0.00010400085739092901,-0.0006175050511956215,-0.0015860130079090595,-0.0006662554806098342,0.016126632690429688,0.002876273589208722,-0.024238698184490204,-0.04533137381076813,0.002635771641507745,0.020562918856739998,0.002216518158093095,-0.003454778576269746,7.150058809202164e-05,-0.014768121764063835],[-0.005183792673051357,0.0031037754379212856,-0.009129324927926064,0.016831889748573303,0.00821931753307581,-0.02262018620967865,-0.001105008996091783,-0.006899806670844555,-0.006604054011404514,0.0003445028269197792,-0.025593960657715797,0.011943847872316837,-0.07600187510251999,-0.00030875252559781075,-0.026191966608166695,-0.013672861270606518,-0.011550595052540302,0.004813289735466242,-0.025935212150216103,0.038350313901901245,0.08257993310689926,-0.006487053353339434,-0.0003185025998391211,0.007419811096042395,-0.006058049388229847,-0.002411519642919302,-0.2550230920314789,-0.013289359398186207,-0.011514844372868538,0.002645521890372038,-0.003006274811923504,0.0033215275034308434],[0.008430569432675838,0.0053755440749228,0.027235224843025208,0.0064708031713962555,0.002457020105794072,-0.00030875252559781075,0.02080342173576355,0.002249018521979451,0.010091332718729973,0.0016315134707838297,0.0191459059715271,0.03344602510333061,0.0004290034994482994,-0.012070599012076855,-0.07816964387893677,-0.008248567581176758,-0.002249018521979451,0.0010432585841044784,0.0005980049027130008,-0.00473853899165988,0.003779781050980091,0.0034157780464738607,0.006526053883135319,-0.060723502188920975,-0.00043550357804633677,0.0008970073540695012,0.0041145337745547295,-0.011043591424822807,-0.02125517465174198,1.950016121554654e-05,0.0250154547393322,0.0007637562812305987],[0.009607079438865185,-0.00915207527577877,-0.0024180198088288307,0.02583121322095394,-0.003666030243039131,-0.011768346652388573,-0.014407368376851082,-0.001482012216001749,0.0006370051996782422,0.015473376959562302,-0.03193476423621178,0.00044525362318381667,0.0009522577747702599,-0.002060516970232129,-0.0019012655830010772,0.029263241216540337,-0.002249018521979451,0.029269739985466003,-0.09151099622249603,0.008232317864894867,-0.048812150955200195,0.006412303075194359,-0.029237238690257072,-0.0005655046552419662,0.008463069796562195,0.009122825227677822,-0.035818543285131454,-0.020111165940761566,0.02179792895913124,0.0018037648405879736,-0.014170116744935513,-0.009145575575530529],[-0.0018525151535868645,0.00030875252559781075,0.0030745253898203373,0.039660073816776276,0.054294947534799576,0.00012025098840240389,0.00803406536579132,-0.013500610366463661,0.0016705136513337493,-0.032552268356084824,0.02049141749739647,0.04238034784793854,0.019678911194205284,-0.011625345796346664,-0.020114416256546974,-0.026906970888376236,0.056391216814517975,-0.008784822188317776,-0.0015405126614496112,0.009126074612140656,0.006175050511956215,0.001719264080747962,0.012792105786502361,-0.00969482958316803,0.0017452642787247896,-0.04347235709428787,0.022012431174516678,0.0041535343043506145,0.002882773755118251,-0.022434933111071587,0.02513570711016655,-0.007029807660728693],[-0.02270793542265892,0.02417369745671749,0.009051323868334293,-0.01933440938591957,-0.03165201097726822,0.0036952802911400795,0.00283402344211936,-0.0068315560929477215,-0.00696155708283186,0.024222448468208313,0.013910114765167236,0.04195459559559822,-0.0014202616875991225,0.015827631577849388,0.005833798088133335,-0.00820956751704216,0.0005785047542303801,0.00908382423222065,0.028931736946105957,-0.051818426698446274,-0.0048717898316681385,0.0019402659963816404,-0.006942057516425848,-0.01095259003341198,0.004296535160392523,0.0006857556290924549,-0.0051382919773459435,0.006558554247021675,0.03460303321480751,0.033147022128105164,-0.14559145271778107,-0.0275114756077528],[-0.003604279365390539,0.02167442813515663,0.03650429844856262,-0.014088866300880909,0.0027495224494487047,0.009421827271580696,-0.014153866097331047,-0.017351893708109856,-0.021599676460027695,-0.03981932997703552,-0.003360527567565441,0.0142448665574193,0.002453770022839308,-0.011755346320569515,0.08478344231843948,0.0005330043495632708,-0.18318450450897217,0.0014040115056559443,-0.02646496891975403,0.016188383102416992,-0.1308298259973526,0.19544686377048492,0.10488486289978027,0.006906306836754084,0.004777539521455765,0.011947098188102245,-6.032134056091309,0.01633463427424431,0.0023497692309319973,0.0855114534497261,-0.07869614660739899,0.11586995422840118],[0.019067907705903053,-0.025860462337732315,0.13927990198135376,-0.06761355698108673,0.004023532848805189,0.050820667296648026,0.004338785540312529,-0.007748064119368792,-0.043417107313871384,0.005300793331116438,-0.004338785540312529,0.01084858924150467,-0.060567498207092285,-0.00789431482553482,0.009932081215083599,-0.07049957662820816,-1.7499314546585083,-1.1661258935928345,0.17853371798992157,0.007413310930132866,-0.0006110050017014146,0.04435311257839203,-0.012512602843344212,0.004670288413763046,-0.3149438500404358,-0.2831488251686096,0.03889956697821617,0.03262701630592346,0.005775297526270151,0.034648533910512924,-0.007036307826638222,0.019227158278226852],[0.023952696472406387,0.04959540814161301,-0.011228841729462147,0.0018687653355300426,-0.798027753829956,-0.012363102287054062,0.003666030243039131,-0.0017290142131969333,-0.0028957738541066647,-0.000585004803724587,0.004134033806622028,-0.0004842540074605495,-0.0016932639991864562,-0.0022230183240026236,-0.011033840477466583,-0.0011245092609897256,-0.02012091502547264,-0.00908382423222065,-0.006269301287829876,-0.012935105711221695,0.037914812564849854,0.002431020140647888,-0.02158992737531662,0.005833798088133335,0.0014625120675191283,-0.01008808333426714,-0.0029900246299803257,-0.006145800463855267,-0.015749629586935043,-0.0008970073540695012,-0.004543537274003029,0.005950798746198416],[0.004439536482095718,-0.013942615129053593,0.01094284001737833,0.028499484062194824,0.014186366461217403,0.0038902820087969303,-0.011436844244599342,0.0020572668872773647,0.029945746064186096,0.009418576955795288,0.007286560256034136,0.00452403724193573,-0.029194990172982216,0.02246418409049511,-0.006152300164103508,0.006945306900888681,0.022022180259227753,-0.0729338526725769,0.0015242625959217548,-0.0012805105652660131,0.09138750284910202,0.0006727555301040411,0.037144556641578674,0.0021190172992646694,0.004553287290036678,0.005053791217505932,0.00010725087486207485,-0.003068024991080165,0.0008287567761726677,0.04121033847332001,-0.12340676039457321,0.022854186594486237],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff variation: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff variation\"},\"ticksuffix\":\"%\"},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Direct effect on Intermediate AE Heads' values by comma token patching\"},\"width\":700,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d8a27450-0d82-458a-9e81-5483b1dad0aa');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_p(\n",
    "        results[\"z\"][:22] * 100,\n",
    "        title=f\"Direct effect on Intermediate AE Heads' values by comma token patching\",\n",
    "        labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff variation\"},\n",
    "        coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "        border=True,\n",
    "        width=700,\n",
    "        margin={\"r\": 100, \"l\": 100}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708109ac",
   "metadata": {},
   "source": [
    "#### Ablating Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d57bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ablation import (\n",
    "    resample_cache_component,\n",
    "    mean_over_cache_component,\n",
    "    zero_cache_component,\n",
    "    freeze_attn_pattern_hook,\n",
    "    freeze_attn_head_pos_hook,\n",
    "    freeze_mlp_pos_hook,\n",
    "    freeze_layer_pos_hook,\n",
    "    ablate_attn_head_pos_hook,\n",
    "    ablate_mlp_pos_hook,\n",
    "    ablate_layer_pos_hook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc39f2",
   "metadata": {},
   "source": [
    "##### Swapping Attention Head Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46181d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (2, 21), (2, 22), (2, 23), (2, 24), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (3, 24), (3, 25), (3, 26), (3, 27), (3, 28), (3, 29), (3, 30), (3, 31), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24), (4, 25), (4, 26), (4, 27), (4, 28), (4, 29), (4, 30), (4, 31), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25), (5, 26), (5, 27), (5, 28), (5, 29), (5, 30), (5, 31), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (6, 20), (6, 21), (6, 22), (6, 23), (6, 24), (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30), (6, 31), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (7, 20), (7, 21), (7, 22), (7, 23), (7, 24), (7, 25), (7, 26), (7, 27), (7, 28), (7, 29), (7, 30), (7, 31), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (8, 20), (8, 21), (8, 22), (8, 23), (8, 24), (8, 25), (8, 26), (8, 27), (8, 28), (8, 29), (8, 30), (8, 31), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15), (15, 16), (15, 17), (15, 18), (15, 19), (15, 20), (15, 21), (15, 22), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (15, 30), (15, 31), (16, 0), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 7), (16, 8), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 15), (16, 16), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (16, 30), (16, 31), (17, 0), (17, 1), (17, 2), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (17, 10), (17, 11), (17, 12), (17, 13), (17, 14), (17, 15), (17, 16), (17, 17), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (17, 30), (17, 31), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (18, 30), (18, 31), (19, 0), (19, 1), (19, 2), (19, 3), (19, 4), (19, 5), (19, 6), (19, 7), (19, 8), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (19, 14), (19, 15), (19, 16), (19, 17), (19, 18), (19, 19), (19, 20), (19, 21), (19, 22), (19, 23), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (19, 30), (19, 31), (20, 0), (20, 1), (20, 2), (20, 3), (20, 4), (20, 5), (20, 6), (20, 7), (20, 8), (20, 9), (20, 10), (20, 11), (20, 12), (20, 13), (20, 14), (20, 15), (20, 16), (20, 17), (20, 18), (20, 19), (20, 20), (20, 21), (20, 22), (20, 23), (20, 24), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (20, 30), (20, 31), (21, 0), (21, 1), (21, 2), (21, 3), (21, 4), (21, 5), (21, 6), (21, 7), (21, 8), (21, 9), (21, 10), (21, 11), (21, 12), (21, 13), (21, 14), (21, 15), (21, 16), (21, 17), (21, 18), (21, 19), (21, 20), (21, 21), (21, 22), (21, 23), (21, 24), (21, 25), (21, 26), (21, 27), (21, 28), (21, 29), (21, 30), (21, 31), (22, 0), (22, 1), (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8), (22, 9), (22, 10), (22, 11), (22, 12), (22, 13), (22, 14), (22, 15), (22, 16), (22, 17), (22, 18), (22, 19), (22, 20), (22, 21), (22, 22), (22, 23), (22, 24), (22, 25), (22, 26), (22, 27), (22, 28), (22, 29), (22, 30), (22, 31), (23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n",
      "Original logit diff: 0.9170\n",
      "Post ablation logit diff: 0.5737234950065613\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")\n",
    "\n",
    "def freeze_attn_hook(pattern, hook, layer=0, head_idx=0):\n",
    "    pattern[:, head_idx, :, :] = clean_cache[f\"blocks.{layer}.attn.hook_pattern\"][:, head_idx, :, :] \n",
    "    pattern[:, head_idx, :, :] = clean_cache[f\"blocks.{layer}.attn.hook_pattern\"][:, head_idx, :, :]\n",
    "    return pattern\n",
    "\n",
    "for layer, head in heads_to_freeze:\n",
    "    freeze_attn = partial(freeze_attn_hook, layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_pattern.add_hook(freeze_attn)\n",
    "\n",
    "def swap_commas_hook(z, hook, layer=0, head_idx=0):\n",
    "    #z[:, 4, head_idx, :] = resample_cache_component(corrupted_cache[f\"blocks.{layer}.attn.hook_v\"][:, 4, head_idx, :])\n",
    "    #z[:, 14, head_idx, :] = resample_cache_component(corrupted_cache[f\"blocks.{layer}.attn.hook_v\"][:, 14, head_idx, :])\n",
    "\n",
    "    z[:, 4, head_idx, :] = corrupted_cache[f\"blocks.{layer}.attn.hook_v\"][:, 4, head_idx, :]\n",
    "    z[:, 14, head_idx, :] = corrupted_cache[f\"blocks.{layer}.attn.hook_v\"][:, 14, head_idx, :]\n",
    "    #z[:, 10, head_idx, :] = 0 #temp_z[:, 20, head_idx, :]\n",
    "    #z[:, 20, head_idx, :] = 0 #temp_z[:, 10, head_idx, :]\n",
    "    return z\n",
    "\n",
    "for layer, head in heads_to_ablate:\n",
    "    swap_head_hook = partial(swap_commas_hook, layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_v.add_hook(swap_head_hook)\n",
    "\n",
    "\n",
    "\n",
    "ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {get_logit_diff(ablated_logits, answer_tokens).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f1fd618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n",
      "Original logit diff: 0.9170\n",
      "Post ablation logit diff: 0.9068796038627625\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(23, 32) for head in range(model.cfg.n_heads)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")\n",
    "def ablate_precommas_hook(z, hook, head_idx=0):\n",
    "    temp_z = z.clone()\n",
    "    #z[:, 1:4, head_idx, :] = temp_z[:, 11:14, head_idx, :]\n",
    "    #z[:, 11:14, head_idx, :] = temp_z[:, 1:4, head_idx, :]\n",
    "    z[:, 1:4, head_idx, :] = corrupted_cache[f\"blocks.{layer}.attn.hook_z\"][:, 11:14, head_idx, :] \n",
    "    z[:, 11:14, head_idx, :] = corrupted_cache[f\"blocks.{layer}.attn.hook_z\"][:, 1:4, head_idx, :] \n",
    "    #z[:, 10, head_idx, :] = 0 #temp_z[:, 20, head_idx, :]\n",
    "    #z[:, 20, head_idx, :] = 0 #temp_z[:, 10, head_idx, :]\n",
    "    return z\n",
    "\n",
    "for layer, head in heads_to_ablate:\n",
    "    ablate_precommas = partial(ablate_precommas_hook, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_z.add_hook(ablate_precommas)\n",
    "\n",
    "ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {get_logit_diff(ablated_logits, answer_tokens).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbebc3",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Non-Commas with Frozen Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99d0c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "precomma_pos = torch.tensor([1, 2, 3, 11, 12, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45daba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(9, 15) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "layers_to_freeze = [layer for layer in range(0, 32)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a941f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_precomma_ablation_experiment(heads_to_ablate, heads_to_freeze):\n",
    "    comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "    precomma_pos = torch.tensor([1, 2, 3, 11, 12, 13])\n",
    "\n",
    "    # freeze attention patterns\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_attn = partial(freeze_attn_pattern_hook, cache=clean_cache, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_pattern.add_hook(freeze_attn)\n",
    "\n",
    "    # freeze comma attn values\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_commas = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_v\", pos=comma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(freeze_commas)\n",
    "\n",
    "    # freeze comma mlp_out positions\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_mlps = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_mlp_out\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_mlp_out.add_hook(freeze_comma_mlps)\n",
    "\n",
    "    # freeze comma resid_post\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_resid = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_resid_post\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_resid_post.add_hook(freeze_comma_resid)\n",
    "\n",
    "    # ablate pre-comma values\n",
    "    for layer, head in heads_to_ablate:\n",
    "        ablate_precommas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_v\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(ablate_precommas)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return get_logit_diff(ablated_logits, answer_tokens).item(), get_logit_diff(ablated_logits, answer_tokens, per_prompt=True), get_logit_diff(clean_logits, answer_tokens, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "feb229ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.9170\n",
      "Post ablation logit diff: 0.5683\n",
      "Logit diff % change: -38.02%\n"
     ]
    }
   ],
   "source": [
    "ablated_logit_diff, ablated_ld_list, clean_ld_list = run_precomma_ablation_experiment(heads_to_ablate, heads_to_freeze)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {ablated_logit_diff:.4f}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40861841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1c172",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Non-Commas with Frozen Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b23a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (2, 21), (2, 22), (2, 23), (2, 24), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (3, 24), (3, 25), (3, 26), (3, 27), (3, 28), (3, 29), (3, 30), (3, 31), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24), (4, 25), (4, 26), (4, 27), (4, 28), (4, 29), (4, 30), (4, 31), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25), (5, 26), (5, 27), (5, 28), (5, 29), (5, 30), (5, 31), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (6, 20), (6, 21), (6, 22), (6, 23), (6, 24), (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30), (6, 31), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (7, 20), (7, 21), (7, 22), (7, 23), (7, 24), (7, 25), (7, 26), (7, 27), (7, 28), (7, 29), (7, 30), (7, 31), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (8, 20), (8, 21), (8, 22), (8, 23), (8, 24), (8, 25), (8, 26), (8, 27), (8, 28), (8, 29), (8, 30), (8, 31), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15), (15, 16), (15, 17), (15, 18), (15, 19), (15, 20), (15, 21), (15, 22), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (15, 30), (15, 31), (16, 0), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 7), (16, 8), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 15), (16, 16), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (16, 30), (16, 31), (17, 0), (17, 1), (17, 2), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (17, 10), (17, 11), (17, 12), (17, 13), (17, 14), (17, 15), (17, 16), (17, 17), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (17, 30), (17, 31), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (18, 30), (18, 31), (19, 0), (19, 1), (19, 2), (19, 3), (19, 4), (19, 5), (19, 6), (19, 7), (19, 8), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (19, 14), (19, 15), (19, 16), (19, 17), (19, 18), (19, 19), (19, 20), (19, 21), (19, 22), (19, 23), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (19, 30), (19, 31), (20, 0), (20, 1), (20, 2), (20, 3), (20, 4), (20, 5), (20, 6), (20, 7), (20, 8), (20, 9), (20, 10), (20, 11), (20, 12), (20, 13), (20, 14), (20, 15), (20, 16), (20, 17), (20, 18), (20, 19), (20, 20), (20, 21), (20, 22), (20, 23), (20, 24), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (20, 30), (20, 31), (21, 0), (21, 1), (21, 2), (21, 3), (21, 4), (21, 5), (21, 6), (21, 7), (21, 8), (21, 9), (21, 10), (21, 11), (21, 12), (21, 13), (21, 14), (21, 15), (21, 16), (21, 17), (21, 18), (21, 19), (21, 20), (21, 21), (21, 22), (21, 23), (21, 24), (21, 25), (21, 26), (21, 27), (21, 28), (21, 29), (21, 30), (21, 31), (22, 0), (22, 1), (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8), (22, 9), (22, 10), (22, 11), (22, 12), (22, 13), (22, 14), (22, 15), (22, 16), (22, 17), (22, 18), (22, 19), (22, 20), (22, 21), (22, 22), (22, 23), (22, 24), (22, 25), (22, 26), (22, 27), (22, 28), (22, 29), (22, 30), (22, 31), (23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "layers_to_freeze = [layer for layer in range(0, 32)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77e269a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "precomma_pos = torch.tensor([[1, 2, 3], [11, 12, 13]])\n",
    "\n",
    "# freeze values\n",
    "for layer, head in heads_to_freeze:\n",
    "    freeze_values = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_v\", pos=\"each\", layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_v.add_hook(freeze_values)\n",
    "\n",
    "# freeze comma attn values\n",
    "for layer, head in heads_to_freeze:\n",
    "    freeze_commas = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_z\", pos=comma_pos, layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_z.add_hook(freeze_commas)\n",
    "\n",
    "# freeze comma mlp_out positions\n",
    "# for layer in layers_to_freeze:\n",
    "#     freeze_comma_mlps = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_mlp_out\", pos=comma_pos, layer=layer)\n",
    "#     model.blocks[layer].hook_mlp_out.add_hook(freeze_comma_mlps)\n",
    "\n",
    "# freeze comma resid_post\n",
    "# for layer in layers_to_freeze:\n",
    "#     freeze_comma_resid = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_resid_post\", pos=comma_pos, layer=layer)\n",
    "#     model.blocks[layer].hook_resid_post.add_hook(freeze_comma_mlps)\n",
    "\n",
    "# ablate pre-comma values\n",
    "for layer, head in heads_to_ablate:\n",
    "    ablate_precommas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_q\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_q.add_hook(ablate_precommas) \n",
    "\n",
    "# ablate pre-comma values\n",
    "for layer, head in heads_to_ablate:\n",
    "    ablate_precommas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_k\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "    model.blocks[layer].attn.hook_k.add_hook(ablate_precommas) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcf9f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.9170\n",
      "Post ablation logit diff: 0.9169943928718567\n",
      "Logit diff % change: -38.02%\n"
     ]
    }
   ],
   "source": [
    "ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {get_logit_diff(ablated_logits, answer_tokens).item()}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "504eb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95899b3b",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "351e13d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(9, 15) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(0, 32) for head in range(model.cfg.n_heads)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "629299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "precomma_pos = torch.tensor([[1, 2, 3, 11, 12, 13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bcc904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comma_ablation_experiment(heads_to_ablate, heads_to_freeze):\n",
    "    comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "    precomma_pos = torch.tensor([[1, 2, 3, 11, 12, 13]])\n",
    "\n",
    "    # freeze attention patterns\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_attn = partial(freeze_attn_pattern_hook, cache=clean_cache, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_pattern.add_hook(freeze_attn)\n",
    "\n",
    "    # freeze precomma attn values\n",
    "    # for layer, head in heads_to_freeze:\n",
    "    #     freeze_precommas = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_v\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "    #     model.blocks[layer].attn.hook_v.add_hook(freeze_precommas)\n",
    "\n",
    "    # freeze comma mlp_out positions\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_mlps = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_mlp_out\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_mlp_out.add_hook(freeze_comma_mlps)\n",
    "\n",
    "    # ablate comma attn values\n",
    "    for layer, head in heads_to_ablate:\n",
    "        ablate_commas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_v\", pos=comma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(ablate_commas)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return get_logit_diff(ablated_logits, answer_tokens).item(), get_logit_diff(ablated_logits, answer_tokens, per_prompt=True), get_logit_diff(clean_logits, answer_tokens, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd420346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.9170\n",
      "Post ablation logit diff: 0.5873\n",
      "Logit diff % change: -35.96%\n"
     ]
    }
   ],
   "source": [
    "ablated_logit_diff, ablated_ld_list, clean_ld_list = run_comma_ablation_experiment(heads_to_ablate, heads_to_freeze)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {ablated_logit_diff:.4f}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea6b5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
