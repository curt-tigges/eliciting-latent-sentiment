{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df58c6a",
   "metadata": {},
   "source": [
    "# Mood Inference Circuit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04addc8d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bed7069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attribution_Patching_Demo.ipynb\n",
      " CircuitsVis\n",
      " Dockerfile\n",
      " LICENSE\n",
      " README.md\n",
      " __pycache__\n",
      " adjective_token_lengths.txt\n",
      " ccs.py\n",
      " ccs_act_patching.py\n",
      " ccs_circuit_analysis.py\n",
      " ccs_circuit_attribution.py\n",
      " ccs_circuit_path_patching.py\n",
      " circuit.md\n",
      " circuit_analysis_classification_prompt_experimentation_pythia2_8b.ipynb\n",
      " circuit_analysis_contrastive_sentiment_gpt2_small.py\n",
      " circuit_analysis_restaurant_review_classification_pythia1_4b.ipynb\n",
      "'circuit_analysis_sentiment continuation_pythia1_4b.py'\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.py\n",
      " circuit_analysis_sentiment_continuation_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_contradiction_pythia1_4b.ipynb\n",
      "'circuit_analysis_simple single sentiment_gpt2_small.py'\n",
      " circuit_analysis_simple_sentiment_gpt2_small.ipynb\n",
      " circuit_analysis_simple_sentiment_gpt2_small.py\n",
      "'circuit_analysis_task comparison_pythia1_4b.ipynb'\n",
      " circuit_for_mood_binding_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_characteristic.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas_alt_prompt.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_situation.ipynb\n",
      " circuit_for_sentiment_classification_pythia1_4b.ipynb\n",
      " circuits\n",
      " classification.py\n",
      " classifier_accuracy.py\n",
      " comma_ablation_experiments.ipynb\n",
      " compare_lines.py\n",
      " data\n",
      " dataset_stats.py\n",
      " derivative_log_prob.py\n",
      " direct_linear_attribution.py\n",
      " direction_patching_results.py\n",
      " direction_patching_suite.py\n",
      " dlk\n",
      " environment.yml\n",
      " finetuning-imdb.ipynb\n",
      " fit_directions.py\n",
      " gpt2-simple_sentiment-act_patching_at_resid_stream_and_layer_outputs.pdf\n",
      " gpt2_imdb_classifier\n",
      " gpt2_imdb_test\n",
      " head_cosine_sim.py\n",
      " hp.txt\n",
      " imdb_balanced_subset\n",
      " imdb_neg_subset\n",
      " imdb_pos_subset\n",
      " imdb_zero_shot\n",
      " initial_exploration.py\n",
      " leace.py\n",
      " localising_by_direction.py\n",
      " model_cache\n",
      " mood_inference_names.txt\n",
      " movie_review_finetuning.ipynb\n",
      " negation_experiment.py\n",
      " neuron_directions.py\n",
      " openai_api_labels.py\n",
      " openai_api_labels_steering.py\n",
      " openwebtext_performance.py\n",
      " ov_unembed.py\n",
      " patching_at_resid_stream_and_layer_outputs.pdf\n",
      " path_patching.py\n",
      " plot_gpu_memory.py\n",
      " projection_neuroscope.py\n",
      " projection_neuroscope2.ipynb\n",
      " prompt_utils.py\n",
      " prompts.yaml\n",
      " pythia_classification_experiments.ipynb\n",
      " random_directions.py\n",
      " record_gpu_memory.sh\n",
      " resample_ablation.py\n",
      " resample_ablation_mood_inference.py\n",
      " sentiment_ablated_act_patching.py\n",
      " sst2\n",
      " sst_balanced_subset\n",
      " sst_neg_subset\n",
      " sst_pos_subset\n",
      " sst_zero_shot\n",
      " sst_zero_shot_EleutherAI_pythia-1.4b\n",
      " sst_zero_shot_EleutherAI_pythia-2.8b\n",
      " sst_zero_shot_EleutherAI_pythia-6.9b\n",
      " sst_zero_shot_gpt2-xl\n",
      " stanfordSentimentTreebank\n",
      " test_prompt.py\n",
      " tests\n",
      " treebank_data_gen.py\n",
      " utils\n",
      " wandb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "745a76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'eliciting-latent-sentiment'\n",
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af695c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting circuitsvis\n",
      "  Downloading circuitsvis-1.43.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (6.0.0)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.24\n",
      "  Downloading numpy-1.26.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.12.1+cu116)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.1.0->circuitsvis) (3.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->circuitsvis) (4.4.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, circuitsvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.9.2 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed circuitsvis-1.43.2 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformer_lens\n",
      "  Downloading transformer_lens-1.10.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-curand-cu12>=10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.26.2)\n",
      "Collecting datasets>=2.7.1\n",
      "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (11.4.5.107)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.12.1+cu116)\n",
      "Collecting einops>=0.6.0\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.105)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12>=8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.105)\n",
      "Collecting fancy-einsum>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting wandb>=0.13.5\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.3.1)\n",
      "Collecting jaxtyping>=0.2.11\n",
      "  Downloading jaxtyping-0.2.23-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: triton>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (12.1.0.106)\n",
      "Collecting accelerate>=0.23.0\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.25.1\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.23.0->transformer_lens) (23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.12.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Collecting typeguard<3,>=2.13.3\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12>=11.4.5.107->transformer_lens) (12.3.101)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (3.9.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (66.1.1)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.19.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.19.2-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Installing collected packages: appdirs, typeguard, safetensors, pyarrow-hotfix, fancy-einsum, einops, beartype, jaxtyping, huggingface-hub, wandb, tokenizers, accelerate, transformers, datasets, transformer_lens\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "Successfully installed accelerate-0.24.1 appdirs-1.4.4 beartype-0.14.1 datasets-2.14.7 einops-0.7.0 fancy-einsum-0.0.3 huggingface-hub-0.17.3 jaxtyping-0.2.23 pyarrow-hotfix-0.5 safetensors-0.4.0 tokenizers-0.15.0 transformer_lens-1.10.0 transformers-4.35.2 typeguard-2.13.3 wandb-0.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jaxtyping==0.2.13\n",
      "  Downloading jaxtyping-0.2.13-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (2.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.26.2)\n",
      "Installing collected packages: jaxtyping\n",
      "  Attempting uninstall: jaxtyping\n",
      "    Found existing installation: jaxtyping 0.2.23\n",
      "    Uninstalling jaxtyping-0.2.23:\n",
      "      Successfully uninstalled jaxtyping-0.2.23\n",
      "Successfully installed jaxtyping-0.2.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.18.0 tenacity-8.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtyping\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (2.13.3)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.4.0)\n",
      "Installing collected packages: torchtyping\n",
      "Successfully installed torchtyping-0.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting imgkit\n",
      "  Downloading imgkit-1.2.3-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from imgkit) (1.14.0)\n",
      "Installing collected packages: imgkit\n",
      "Successfully installed imgkit-1.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-3ltu0ztd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-3ltu0ztd\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.7.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.26.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.18.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "Building wheels for collected packages: neel-plotly\n",
      "  Building wheel for neel-plotly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neel-plotly: filename=neel_plotly-0.0.0-py3-none-any.whl size=10186 sha256=91734369ba5702c15ee643634bfccd4253b9f6d4ebf5df98e875948bbf3ab332\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-73s24mn_/wheels/e1/3c/c0/b5897c402b85e7fc329feb205ad5948b518f0423d891a79f7f\n",
      "Successfully built neel-plotly\n",
      "Installing collected packages: neel-plotly\n",
      "Successfully installed neel-plotly-0.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting kaleido\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install circuitsvis\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install imgkit\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "#!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "!pip install kaleido\n",
    "#%pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "#%pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67acf14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6327938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:155: UserWarning:\n",
      "\n",
      "A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set\n",
    "from rich import print as rprint\n",
    "\n",
    "from typing import List, Union\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching import Node, IterNode, path_patch, act_patch\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "#from utils.visualization import get_attn_head_patterns, imshow_p, plot_attention_heads, scatter_attention_and_contribution_simple\n",
    "#from utils.visualization import get_attn_pattern, plot_attention\n",
    "\n",
    "from utils.prompts import get_dataset\n",
    "from utils.circuit_analysis import get_logit_diff, logit_diff_denoising, logit_diff_noising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "504aeb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92f89a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly\n",
    "#plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fbd9fe8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def two_lines(tensor1, tensor2, renderer=None, **kwargs):\n",
    "    px.line(y=[utils.to_numpy(tensor1), utils.to_numpy(tensor2)], **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4117f8",
   "metadata": {},
   "source": [
    "## Circuit Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41247d7a",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f1fe429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_name):\n",
    "    if model_name == \"EleutherAI/pythia-6.9b\" or model_name == \"StabilityAI/stablelm-tuned-alpha-7b\":\n",
    "        source_model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"model_cache\").to('cpu').bfloat16()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            model_name,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False,\n",
    "            tokenizer=tokenizer,\n",
    "            hf_model=source_model,\n",
    "        )\n",
    "    else:\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            model_name,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "438c880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d1eca7cdd34ea88e000470c7b00ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6512c0db5e4f86b0dc5955a91285e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae5afc008964515819c48a694eccf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824d5ba4613e468a8abe1cd75e08e681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1bd22b9b3f45cda1207ac5f8dc498b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456c008af1c9452fab8bf1ec43fe833c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77396010028040a699330b21a690658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297d48caac914534af58e4b03774238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da9d1e974cb468db2d6c00fc465c30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c1fce66e144190a0402a3cea93f2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model stabilityai/stablelm-base-alpha-3b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"stabilityai/stablelm-base-alpha-3b\")\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f53a6c5",
   "metadata": {},
   "source": [
    "### Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "158ccfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompts import CircularList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0d99afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \" John, Anne, Mark, Mary, Peter, Paul, James, Sarah, Mike, Tom, Carl, Sam, Sarah, Carl, Jack\"\n",
    "names = [n[1:] for n in model.to_str_tokens(names)[1::2]]\n",
    "names = CircularList(names)\n",
    "# for n in names:\n",
    "#     print(n)\n",
    "#     print(model.to_str_tokens(n))\n",
    "#     print(model.to_str_tokens(\" \" + n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b42cf330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' John', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' Anne', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' Anne', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' John', ' feels', ' very'] 36\n",
      "['<|endoftext|>', 'John', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' Anne', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' quiet', ' private', ' library', '.', ' John', ' feels', ' very'] 36\n"
     ]
    }
   ],
   "source": [
    "orig_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. {names[i]} feels very\" for i in range(len(names))]\n",
    "name_flip_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. {names[i+1]} feels very\" for i in range(len(names))]\n",
    "\n",
    "ch_flip_prompts_a = [f\"{names[i]} hates parties, and avoids them whenever possible. {names[i+1]} loves parties, and joins them whenever possible. One day, they were invited to a grand gala. {names[i]} feels very\" for i in range(len(names))]\n",
    "ch_flip_prompts_b = [f\"{names[i]} hates parties, and avoids them whenever possible. {names[i+1]} loves parties, and joins them whenever possible. One day, they were invited to a grand gala. {names[i+1]} feels very\" for i in range(len(names))]\n",
    "\n",
    "sit_flip_prompts = [f\"{names[i]} loves parties, and joins them whenever possible. {names[i+1]} hates parties, and avoids them whenever possible. One day, they were invited to a quiet private library. {names[i]} feels very\" for i in range(len(names))]\n",
    "\n",
    "print(model.to_str_tokens(orig_prompts[0]), len(model.to_str_tokens(orig_prompts[0])))\n",
    "print(model.to_str_tokens(name_flip_prompts[0]), len(model.to_str_tokens(name_flip_prompts[0])))\n",
    "print(model.to_str_tokens(ch_flip_prompts_a[0]), len(model.to_str_tokens(ch_flip_prompts_a[0])))\n",
    "print(model.to_str_tokens(sit_flip_prompts[0]), len(model.to_str_tokens(sit_flip_prompts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3fd51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_answers = [\" excited\", \" honored\"] #, \" amazing\", \" good\"]\n",
    "neg_answers = [\" nervous\", \" uneasy\"] #, \" terrible\", \" bad\"]\n",
    "batch_size = 4 * len(orig_prompts)\n",
    "n_pairs = 1\n",
    "\n",
    "def create_dataset_4way(orig_prompts_1, orig_prompts_2, flip_prompts_1, flip_prompts_2):\n",
    "    clean_prompts = []\n",
    "    corrupt_prompts = []\n",
    "    answer_tokens = torch.empty(\n",
    "            (batch_size, n_pairs, 2), \n",
    "            device=device, \n",
    "            dtype=torch.long\n",
    "        )\n",
    "    for i in range(len(orig_prompts)):\n",
    "        clean_prompts.append(orig_prompts_1[i])\n",
    "        clean_prompts.append(flip_prompts_1[i])\n",
    "        clean_prompts.append(orig_prompts_2[i])\n",
    "        clean_prompts.append(flip_prompts_2[i])\n",
    "\n",
    "        corrupt_prompts.append(flip_prompts_1[i])\n",
    "        corrupt_prompts.append(orig_prompts_1[i])\n",
    "        corrupt_prompts.append(flip_prompts_2[i])\n",
    "        corrupt_prompts.append(orig_prompts_2[i])\n",
    "\n",
    "        for pair_idx in range(n_pairs):\n",
    "                pos_token = model.to_single_token(pos_answers[pair_idx])\n",
    "                neg_token = model.to_single_token(neg_answers[pair_idx])\n",
    "                tokens_dict = {\n",
    "                    'positive': pos_token, \n",
    "                    'negative': neg_token, \n",
    "                }\n",
    "                answer_tokens[i * 4, pair_idx, 0] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4, pair_idx, 1] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 1, pair_idx, 0] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 1, pair_idx, 1] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 2, pair_idx, 0] = tokens_dict['negative']\n",
    "                answer_tokens[i * 4 + 2, pair_idx, 1] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 3, pair_idx, 0] = tokens_dict['positive']\n",
    "                answer_tokens[i * 4 + 3, pair_idx, 1] = tokens_dict['negative']\n",
    "\n",
    "    prompts_tokens: Float[Tensor, \"batch pos\"] = model.to_tokens(\n",
    "            clean_prompts, prepend_bos=True\n",
    "        )\n",
    "    clean_tokens = prompts_tokens.to(device)\n",
    "    corrupted_tokens = model.to_tokens(\n",
    "        corrupt_prompts, prepend_bos=True\n",
    "    ).to(device)\n",
    "\n",
    "    return clean_prompts, clean_tokens, corrupted_tokens, answer_tokens\n",
    "\n",
    "all_prompts, clean_tokens, corrupted_tokens, answer_tokens = create_dataset_4way(orig_prompts, name_flip_prompts, ch_flip_prompts_a, ch_flip_prompts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e1f61f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'John', ' hates', ' parties', ',', ' and', ' avoids', ' them', ' whenever', ' possible', '.', ' Anne', ' loves', ' parties', ',', ' and', ' joins', ' them', ' whenever', ' possible', '.', ' One', ' day', ',', ' they', ' were', ' invited', ' to', ' a', ' grand', ' gal', 'a', '.', ' Anne', ' feels', ' very']\n",
      "Tokenized answer: [' excited']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span><span style=\"font-weight: bold\">      Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.82</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | excited|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m170\u001b[0m\u001b[1m      Logit: \u001b[0m\u001b[1;36m17.82\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | excited|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.01 Prob: 39.35% Token: | unhappy|\n",
      "Top 1th token. Logit: 30.76 Prob: 30.59% Token: | uncomfortable|\n",
      "Top 2th token. Logit: 29.62 Prob:  9.82% Token: | shy|\n",
      "Top 3th token. Logit: 28.88 Prob:  4.67% Token: | tired|\n",
      "Top 4th token. Logit: 28.87 Prob:  4.64% Token: | embarrassed|\n",
      "Top 5th token. Logit: 28.85 Prob:  4.53% Token: | nervous|\n",
      "Top 6th token. Logit: 27.26 Prob:  0.93% Token: | depressed|\n",
      "Top 7th token. Logit: 27.19 Prob:  0.86% Token: | stressed|\n",
      "Top 8th token. Logit: 27.08 Prob:  0.78% Token: | miserable|\n",
      "Top 9th token. Logit: 26.91 Prob:  0.66% Token: | isolated|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' excited'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' excited'\u001b[0m, \u001b[1;36m170\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "test_prompt(all_prompts[3], \" excited\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd5ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Peter hates parties, and avoids them whenever possible. Paul loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Peter loves parties, and joins them whenever possible. Paul hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Paul feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Paul hates parties, and avoids them whenever possible. James loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>Paul loves parties, and joins them whenever possible. James hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. James feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>James hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>James loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Mike loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Mike hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mike feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Mike hates parties, and avoids them whenever possible. Tom loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Mike loves parties, and joins them whenever possible. Tom hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Tom feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Tom hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Tom loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Sam loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Sam hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sam feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sam hates parties, and avoids them whenever possible. Sarah loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sam loves parties, and joins them whenever possible. Sarah hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Sarah feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Sarah hates parties, and avoids them whenever possible. Carl loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Sarah loves parties, and joins them whenever possible. Carl hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Carl feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Carl hates parties, and avoids them whenever possible. Jack loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Carl loves parties, and joins them whenever possible. Jack hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Jack feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "\n",
      "\n",
      "<|endoftext|>Jack hates parties, and avoids them whenever possible. John loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "<|endoftext|>Jack loves parties, and joins them whenever possible. John hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(clean_tokens.shape[0]):\n",
    "    print(model.to_string(clean_tokens[i]))\n",
    "    print(model.to_string(corrupted_tokens[i]))\n",
    "    print(model.to_str_tokens(answer_tokens[i][0]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dcd3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, torch.Size([60, 1, 2]), torch.Size([60, 36]), torch.Size([60, 36]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prompts), answer_tokens.shape, clean_tokens.shape, corrupted_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afef9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.6354, device='cuda:0') \n",
      "\n",
      "John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. John feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.5870, device='cuda:0') \n",
      "\n",
      "John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(2.2780, device='cuda:0') \n",
      "\n",
      "John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.0796, device='cuda:0') \n",
      "\n",
      "Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.2941, device='cuda:0') \n",
      "\n",
      "Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.7551, device='cuda:0') \n",
      "\n",
      "Anne loves parties, and joins them whenever possible. Mark hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(2.1690, device='cuda:0') \n",
      "\n",
      "Anne hates parties, and avoids them whenever possible. Mark loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.1008, device='cuda:0') \n",
      "\n",
      "Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.6387, device='cuda:0') \n",
      "\n",
      "Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mark feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.6638, device='cuda:0') \n",
      "\n",
      "Mark loves parties, and joins them whenever possible. Mary hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(2.1460, device='cuda:0') \n",
      "\n",
      "Mark hates parties, and avoids them whenever possible. Mary loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.0839, device='cuda:0') \n",
      "\n",
      "Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(-0.1617, device='cuda:0') \n",
      "\n",
      "Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Mary feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(1.5977, device='cuda:0') \n",
      "\n",
      "Mary loves parties, and joins them whenever possible. Peter hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' nervous', ' excited']\n",
      "tensor(2.3051, device='cuda:0') \n",
      "\n",
      "Mary hates parties, and avoids them whenever possible. Peter loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Peter feels very\n",
      "[' excited', ' nervous']\n",
      "tensor(0.1179, device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 16, 1):\n",
    "    logits, _ = model.run_with_cache(all_prompts[i])\n",
    "    log_diff = get_logit_diff(logits, answer_tokens[i].unsqueeze(0))\n",
    "    #if log_diff < 0.1:\n",
    "    print(all_prompts[i])\n",
    "    print(model.to_str_tokens(answer_tokens[i][0]))\n",
    "    print(log_diff, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4765b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = all_prompts[:16]\n",
    "answer_tokens = answer_tokens[:16]\n",
    "clean_tokens = clean_tokens[:16]\n",
    "corrupted_tokens = corrupted_tokens[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e609123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>John hates parties, and avoids them whenever possible. Anne loves parties, and joins them whenever possible. One day, they were invited to a grand gala. Anne feels very',\n",
       " '<|endoftext|>John loves parties, and joins them whenever possible. Anne hates parties, and avoids them whenever possible. One day, they were invited to a grand gala. Anne feels very')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(clean_tokens[3]), model.to_string(corrupted_tokens[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9e84c0",
   "metadata": {},
   "source": [
    "#### Logit Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c19669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indices = [0, 3, 4, 7, 8, 11, 12, 15]\n",
    "neg_indices = [1, 2, 5, 6, 9, 10, 13, 14]\n",
    "name_1_indices = [0, 1, 4, 5, 8, 9, 12, 13]\n",
    "name_2_indices = [2, 3, 6, 7, 10, 11, 14, 15]\n",
    "pos_first_indices = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "neg_first_indices = [1, 3, 5, 7, 9, 11, 13, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8adb4951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2136, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logits, pos_cache = model.run_with_cache(clean_tokens[pos_indices,:])\n",
    "pos_logit_diff = get_logit_diff(pos_logits, answer_tokens[pos_indices,:])\n",
    "pos_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82c41dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9377, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_logits, neg_cache = model.run_with_cache(clean_tokens[neg_indices,:])\n",
    "neg_logit_diff = get_logit_diff(neg_logits, answer_tokens[neg_indices,:])\n",
    "neg_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d422344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6092, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_1_logits, name_1_cache = model.run_with_cache(clean_tokens[name_1_indices, :])\n",
    "name_1_logit_diff = get_logit_diff(name_1_logits, answer_tokens[name_1_indices, :])\n",
    "name_1_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fda79dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1149, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_2_logits, name_2_cache = model.run_with_cache(clean_tokens[name_2_indices, :])\n",
    "name_2_logit_diff = get_logit_diff(name_2_logits, answer_tokens[name_2_indices, :])\n",
    "name_2_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c93a5dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8960, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_first_logits, pos_first_cache = model.run_with_cache(clean_tokens[pos_first_indices,:])\n",
    "pos_first_logit_diff = get_logit_diff(pos_first_logits, answer_tokens[pos_first_indices,:])\n",
    "pos_first_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b43a89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8281, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_first_logits, neg_first_cache = model.run_with_cache(clean_tokens[neg_first_indices,:])\n",
    "neg_first_logit_diff = get_logit_diff(neg_first_logits, answer_tokens[neg_first_indices,:])\n",
    "neg_first_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ce996af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8621, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_tokens, per_prompt=False)\n",
    "clean_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31adef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8621, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_tokens, per_prompt=False)\n",
    "corrupted_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89f27f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_denoising(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[Tensor, \"batch n_pairs 2\"] = answer_tokens,\n",
    "    flipped_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    return_tensor: bool = False,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is\n",
    "    same as on flipped input, and 1 when performance is same as on clean input.\n",
    "    '''\n",
    "    patched_logit_diff = get_logit_diff(logits, answer_tokens)\n",
    "    ld = ((patched_logit_diff - flipped_logit_diff) / (clean_logit_diff  - flipped_logit_diff))\n",
    "    if return_tensor:\n",
    "        return ld\n",
    "    else:\n",
    "        return ld.item()\n",
    "\n",
    "\n",
    "def logit_diff_noising(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        clean_logit_diff: float = clean_logit_diff,\n",
    "        corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "        answer_tokens: Float[Tensor, \"batch n_pairs 2\"] = answer_tokens,\n",
    "        return_tensor: bool = False,\n",
    "    ) -> float:\n",
    "        '''\n",
    "        We calibrate this so that the value is 0 when performance isn't harmed (i.e. same as IOI dataset),\n",
    "        and -1 when performance has been destroyed (i.e. is same as ABC dataset).\n",
    "        '''\n",
    "        patched_logit_diff = get_logit_diff(logits, answer_tokens)\n",
    "        ld = ((patched_logit_diff - clean_logit_diff) / (clean_logit_diff - corrupted_logit_diff))\n",
    "\n",
    "        if return_tensor:\n",
    "            return ld\n",
    "        else:\n",
    "            return ld.item()\n",
    "\n",
    "logit_diff_denoising_tensor = partial(logit_diff_denoising, return_tensor=True)\n",
    "logit_diff_noising_tensor = partial(logit_diff_noising, return_tensor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b093f49b",
   "metadata": {},
   "source": [
    "### Ablation & Swap Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "708109ac",
   "metadata": {},
   "source": [
    "#### Ablating Commas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fedbebc3",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Entire Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99d0c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_pos = torch.tensor([1, 2, 3, 4, 10, 11, 12, 13, 14, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45daba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (2, 21), (2, 22), (2, 23), (2, 24), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (3, 24), (3, 25), (3, 26), (3, 27), (3, 28), (3, 29), (3, 30), (3, 31), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24), (4, 25), (4, 26), (4, 27), (4, 28), (4, 29), (4, 30), (4, 31), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25), (5, 26), (5, 27), (5, 28), (5, 29), (5, 30), (5, 31), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (6, 20), (6, 21), (6, 22), (6, 23), (6, 24), (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30), (6, 31), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (7, 20), (7, 21), (7, 22), (7, 23), (7, 24), (7, 25), (7, 26), (7, 27), (7, 28), (7, 29), (7, 30), (7, 31), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (8, 20), (8, 21), (8, 22), (8, 23), (8, 24), (8, 25), (8, 26), (8, 27), (8, 28), (8, 29), (8, 30), (8, 31), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15), (15, 16), (15, 17), (15, 18), (15, 19), (15, 20), (15, 21), (15, 22), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (15, 30), (15, 31), (16, 0), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 7), (16, 8), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 15), (16, 16), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (16, 30), (16, 31), (17, 0), (17, 1), (17, 2), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (17, 10), (17, 11), (17, 12), (17, 13), (17, 14), (17, 15), (17, 16), (17, 17), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (17, 30), (17, 31), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (18, 30), (18, 31), (19, 0), (19, 1), (19, 2), (19, 3), (19, 4), (19, 5), (19, 6), (19, 7), (19, 8), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (19, 14), (19, 15), (19, 16), (19, 17), (19, 18), (19, 19), (19, 20), (19, 21), (19, 22), (19, 23), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (19, 30), (19, 31), (20, 0), (20, 1), (20, 2), (20, 3), (20, 4), (20, 5), (20, 6), (20, 7), (20, 8), (20, 9), (20, 10), (20, 11), (20, 12), (20, 13), (20, 14), (20, 15), (20, 16), (20, 17), (20, 18), (20, 19), (20, 20), (20, 21), (20, 22), (20, 23), (20, 24), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (20, 30), (20, 31), (21, 0), (21, 1), (21, 2), (21, 3), (21, 4), (21, 5), (21, 6), (21, 7), (21, 8), (21, 9), (21, 10), (21, 11), (21, 12), (21, 13), (21, 14), (21, 15), (21, 16), (21, 17), (21, 18), (21, 19), (21, 20), (21, 21), (21, 22), (21, 23), (21, 24), (21, 25), (21, 26), (21, 27), (21, 28), (21, 29), (21, 30), (21, 31), (22, 0), (22, 1), (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8), (22, 9), (22, 10), (22, 11), (22, 12), (22, 13), (22, 14), (22, 15), (22, 16), (22, 17), (22, 18), (22, 19), (22, 20), (22, 21), (22, 22), (22, 23), (22, 24), (22, 25), (22, 26), (22, 27), (22, 28), (22, 29), (22, 30), (22, 31), (23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "layers_to_freeze = [layer for layer in range(model.cfg.n_layers)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a941f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phrase_ablation_experiment(heads_to_ablate, heads_to_freeze):\n",
    "    phrase_pos = torch.tensor([1, 2, 3, 4, 10, 11, 12, 13, 14, 20])\n",
    "\n",
    "    # ablate pre-comma values\n",
    "    for layer, head in heads_to_ablate:\n",
    "        ablate_precommas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_v\", pos=phrase_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(ablate_precommas)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return get_logit_diff(ablated_logits, answer_tokens).item(), get_logit_diff(ablated_logits, answer_tokens, per_prompt=True), get_logit_diff(clean_logits, answer_tokens, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb229ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.8621\n",
      "Post ablation logit diff: -0.4455\n",
      "Logit diff % change: -151.67%\n"
     ]
    }
   ],
   "source": [
    "ablated_logit_diff, ablated_ld_list, clean_ld_list = run_phrase_ablation_experiment(heads_to_ablate, heads_to_freeze)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {ablated_logit_diff:.4f}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40861841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c927d55",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Non-Commas with Frozen Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1435cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "precomma_pos = torch.tensor([1, 2, 3, 11, 12, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc1e3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (2, 21), (2, 22), (2, 23), (2, 24), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (3, 24), (3, 25), (3, 26), (3, 27), (3, 28), (3, 29), (3, 30), (3, 31), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24), (4, 25), (4, 26), (4, 27), (4, 28), (4, 29), (4, 30), (4, 31), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25), (5, 26), (5, 27), (5, 28), (5, 29), (5, 30), (5, 31), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (6, 20), (6, 21), (6, 22), (6, 23), (6, 24), (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30), (6, 31), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (7, 20), (7, 21), (7, 22), (7, 23), (7, 24), (7, 25), (7, 26), (7, 27), (7, 28), (7, 29), (7, 30), (7, 31), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (8, 20), (8, 21), (8, 22), (8, 23), (8, 24), (8, 25), (8, 26), (8, 27), (8, 28), (8, 29), (8, 30), (8, 31), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15), (15, 16), (15, 17), (15, 18), (15, 19), (15, 20), (15, 21), (15, 22), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (15, 30), (15, 31), (16, 0), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 7), (16, 8), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 15), (16, 16), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (16, 30), (16, 31), (17, 0), (17, 1), (17, 2), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (17, 10), (17, 11), (17, 12), (17, 13), (17, 14), (17, 15), (17, 16), (17, 17), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (17, 30), (17, 31), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (18, 30), (18, 31), (19, 0), (19, 1), (19, 2), (19, 3), (19, 4), (19, 5), (19, 6), (19, 7), (19, 8), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (19, 14), (19, 15), (19, 16), (19, 17), (19, 18), (19, 19), (19, 20), (19, 21), (19, 22), (19, 23), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (19, 30), (19, 31), (20, 0), (20, 1), (20, 2), (20, 3), (20, 4), (20, 5), (20, 6), (20, 7), (20, 8), (20, 9), (20, 10), (20, 11), (20, 12), (20, 13), (20, 14), (20, 15), (20, 16), (20, 17), (20, 18), (20, 19), (20, 20), (20, 21), (20, 22), (20, 23), (20, 24), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (20, 30), (20, 31), (21, 0), (21, 1), (21, 2), (21, 3), (21, 4), (21, 5), (21, 6), (21, 7), (21, 8), (21, 9), (21, 10), (21, 11), (21, 12), (21, 13), (21, 14), (21, 15), (21, 16), (21, 17), (21, 18), (21, 19), (21, 20), (21, 21), (21, 22), (21, 23), (21, 24), (21, 25), (21, 26), (21, 27), (21, 28), (21, 29), (21, 30), (21, 31), (22, 0), (22, 1), (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8), (22, 9), (22, 10), (22, 11), (22, 12), (22, 13), (22, 14), (22, 15), (22, 16), (22, 17), (22, 18), (22, 19), (22, 20), (22, 21), (22, 22), (22, 23), (22, 24), (22, 25), (22, 26), (22, 27), (22, 28), (22, 29), (22, 30), (22, 31), (23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "layers_to_freeze = [layer for layer in range(model.cfg.n_layers)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "035de786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_precomma_ablation_experiment(heads_to_ablate, heads_to_freeze):\n",
    "    comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "    precomma_pos = torch.tensor([1, 2, 3, 11, 12, 13])\n",
    "\n",
    "    # freeze attention patterns\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_attn = partial(freeze_attn_pattern_hook, cache=clean_cache, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_pattern.add_hook(freeze_attn)\n",
    "\n",
    "    # freeze comma attn values\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_commas = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_v\", pos=comma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(freeze_commas)\n",
    "\n",
    "    # freeze comma mlp_out positions\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_mlps = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_mlp_out\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_mlp_out.add_hook(freeze_comma_mlps)\n",
    "\n",
    "    # freeze comma resid_post\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_resid = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_resid_post\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_resid_post.add_hook(freeze_comma_resid)\n",
    "\n",
    "    # ablate pre-comma values\n",
    "    for layer, head in heads_to_ablate:\n",
    "        ablate_precommas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_v\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(ablate_precommas)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return get_logit_diff(ablated_logits, answer_tokens).item(), get_logit_diff(ablated_logits, answer_tokens, per_prompt=True), get_logit_diff(clean_logits, answer_tokens, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a90ba2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.8621\n",
      "Post ablation logit diff: 0.4684\n",
      "Logit diff % change: -45.66%\n"
     ]
    }
   ],
   "source": [
    "ablated_logit_diff, ablated_ld_list, clean_ld_list = run_precomma_ablation_experiment(heads_to_ablate, heads_to_freeze)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {ablated_logit_diff:.4f}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ef1ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95899b3b",
   "metadata": {},
   "source": [
    "##### Resample-Ablating Commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "351e13d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 19), (2, 20), (2, 21), (2, 22), (2, 23), (2, 24), (2, 25), (2, 26), (2, 27), (2, 28), (2, 29), (2, 30), (2, 31), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (3, 24), (3, 25), (3, 26), (3, 27), (3, 28), (3, 29), (3, 30), (3, 31), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24), (4, 25), (4, 26), (4, 27), (4, 28), (4, 29), (4, 30), (4, 31), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (5, 19), (5, 20), (5, 21), (5, 22), (5, 23), (5, 24), (5, 25), (5, 26), (5, 27), (5, 28), (5, 29), (5, 30), (5, 31), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (6, 19), (6, 20), (6, 21), (6, 22), (6, 23), (6, 24), (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30), (6, 31), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (7, 19), (7, 20), (7, 21), (7, 22), (7, 23), (7, 24), (7, 25), (7, 26), (7, 27), (7, 28), (7, 29), (7, 30), (7, 31), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 19), (8, 20), (8, 21), (8, 22), (8, 23), (8, 24), (8, 25), (8, 26), (8, 27), (8, 28), (8, 29), (8, 30), (8, 31), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (9, 24), (9, 25), (9, 26), (9, 27), (9, 28), (9, 29), (9, 30), (9, 31), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (10, 24), (10, 25), (10, 26), (10, 27), (10, 28), (10, 29), (10, 30), (10, 31), (11, 0), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (11, 19), (11, 20), (11, 21), (11, 22), (11, 23), (11, 24), (11, 25), (11, 26), (11, 27), (11, 28), (11, 29), (11, 30), (11, 31), (12, 0), (12, 1), (12, 2), (12, 3), (12, 4), (12, 5), (12, 6), (12, 7), (12, 8), (12, 9), (12, 10), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23), (12, 24), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (12, 30), (12, 31), (13, 0), (13, 1), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 7), (13, 8), (13, 9), (13, 10), (13, 11), (13, 12), (13, 13), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (13, 19), (13, 20), (13, 21), (13, 22), (13, 23), (13, 24), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (13, 30), (13, 31), (14, 0), (14, 1), (14, 2), (14, 3), (14, 4), (14, 5), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 15), (14, 16), (14, 17), (14, 18), (14, 19), (14, 20), (14, 21), (14, 22), (14, 23), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (14, 30), (14, 31), (15, 0), (15, 1), (15, 2), (15, 3), (15, 4), (15, 5), (15, 6), (15, 7), (15, 8), (15, 9), (15, 10), (15, 11), (15, 12), (15, 13), (15, 14), (15, 15), (15, 16), (15, 17), (15, 18), (15, 19), (15, 20), (15, 21), (15, 22), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (15, 30), (15, 31), (16, 0), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 7), (16, 8), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 15), (16, 16), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (16, 30), (16, 31), (17, 0), (17, 1), (17, 2), (17, 3), (17, 4), (17, 5), (17, 6), (17, 7), (17, 8), (17, 9), (17, 10), (17, 11), (17, 12), (17, 13), (17, 14), (17, 15), (17, 16), (17, 17), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (17, 30), (17, 31), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (18, 30), (18, 31), (19, 0), (19, 1), (19, 2), (19, 3), (19, 4), (19, 5), (19, 6), (19, 7), (19, 8), (19, 9), (19, 10), (19, 11), (19, 12), (19, 13), (19, 14), (19, 15), (19, 16), (19, 17), (19, 18), (19, 19), (19, 20), (19, 21), (19, 22), (19, 23), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (19, 30), (19, 31), (20, 0), (20, 1), (20, 2), (20, 3), (20, 4), (20, 5), (20, 6), (20, 7), (20, 8), (20, 9), (20, 10), (20, 11), (20, 12), (20, 13), (20, 14), (20, 15), (20, 16), (20, 17), (20, 18), (20, 19), (20, 20), (20, 21), (20, 22), (20, 23), (20, 24), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (20, 30), (20, 31), (21, 0), (21, 1), (21, 2), (21, 3), (21, 4), (21, 5), (21, 6), (21, 7), (21, 8), (21, 9), (21, 10), (21, 11), (21, 12), (21, 13), (21, 14), (21, 15), (21, 16), (21, 17), (21, 18), (21, 19), (21, 20), (21, 21), (21, 22), (21, 23), (21, 24), (21, 25), (21, 26), (21, 27), (21, 28), (21, 29), (21, 30), (21, 31), (22, 0), (22, 1), (22, 2), (22, 3), (22, 4), (22, 5), (22, 6), (22, 7), (22, 8), (22, 9), (22, 10), (22, 11), (22, 12), (22, 13), (22, 14), (22, 15), (22, 16), (22, 17), (22, 18), (22, 19), (22, 20), (22, 21), (22, 22), (22, 23), (22, 24), (22, 25), (22, 26), (22, 27), (22, 28), (22, 29), (22, 30), (22, 31), (23, 0), (23, 1), (23, 2), (23, 3), (23, 4), (23, 5), (23, 6), (23, 7), (23, 8), (23, 9), (23, 10), (23, 11), (23, 12), (23, 13), (23, 14), (23, 15), (23, 16), (23, 17), (23, 18), (23, 19), (23, 20), (23, 21), (23, 22), (23, 23), (23, 24), (23, 25), (23, 26), (23, 27), (23, 28), (23, 29), (23, 30), (23, 31), (24, 0), (24, 1), (24, 2), (24, 3), (24, 4), (24, 5), (24, 6), (24, 7), (24, 8), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (24, 14), (24, 15), (24, 16), (24, 17), (24, 18), (24, 19), (24, 20), (24, 21), (24, 22), (24, 23), (24, 24), (24, 25), (24, 26), (24, 27), (24, 28), (24, 29), (24, 30), (24, 31), (25, 0), (25, 1), (25, 2), (25, 3), (25, 4), (25, 5), (25, 6), (25, 7), (25, 8), (25, 9), (25, 10), (25, 11), (25, 12), (25, 13), (25, 14), (25, 15), (25, 16), (25, 17), (25, 18), (25, 19), (25, 20), (25, 21), (25, 22), (25, 23), (25, 24), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (25, 30), (25, 31), (26, 0), (26, 1), (26, 2), (26, 3), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 14), (26, 15), (26, 16), (26, 17), (26, 18), (26, 19), (26, 20), (26, 21), (26, 22), (26, 23), (26, 24), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (26, 30), (26, 31), (27, 0), (27, 1), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 14), (27, 15), (27, 16), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 22), (27, 23), (27, 24), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (27, 30), (27, 31), (28, 0), (28, 1), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 14), (28, 15), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (28, 30), (28, 31), (29, 0), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 13), (29, 14), (29, 15), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29), (29, 30), (29, 31), (30, 0), (30, 1), (30, 2), (30, 3), (30, 4), (30, 5), (30, 6), (30, 7), (30, 8), (30, 9), (30, 10), (30, 11), (30, 12), (30, 13), (30, 14), (30, 15), (30, 16), (30, 17), (30, 18), (30, 19), (30, 20), (30, 21), (30, 22), (30, 23), (30, 24), (30, 25), (30, 26), (30, 27), (30, 28), (30, 29), (30, 30), (30, 31), (31, 0), (31, 1), (31, 2), (31, 3), (31, 4), (31, 5), (31, 6), (31, 7), (31, 8), (31, 9), (31, 10), (31, 11), (31, 12), (31, 13), (31, 14), (31, 15), (31, 16), (31, 17), (31, 18), (31, 19), (31, 20), (31, 21), (31, 22), (31, 23), (31, 24), (31, 25), (31, 26), (31, 27), (31, 28), (31, 29), (31, 30), (31, 31)]\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "heads_to_freeze = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "629299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_pos = torch.tensor([4, 10, 14, 20])\n",
    "precomma_pos = torch.tensor([[1, 2, 3, 11, 12, 13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bcc904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comma_ablation_experiment(heads_to_ablate, heads_to_freeze):\n",
    "    comma_pos = torch.tensor([4, 14])\n",
    "    precomma_pos = torch.tensor([[1, 2, 3, 11, 12, 13]])\n",
    "\n",
    "    # freeze attention patterns\n",
    "    for layer, head in heads_to_freeze:\n",
    "        freeze_attn = partial(freeze_attn_pattern_hook, cache=clean_cache, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_pattern.add_hook(freeze_attn)\n",
    "\n",
    "    # freeze precomma attn values\n",
    "    # for layer, head in heads_to_freeze:\n",
    "    #     freeze_precommas = partial(freeze_attn_head_pos_hook, cache=clean_cache, component_type=\"hook_v\", pos=precomma_pos, layer=layer, head_idx=head)\n",
    "    #     model.blocks[layer].attn.hook_v.add_hook(freeze_precommas)\n",
    "\n",
    "    # freeze comma mlp_out positions\n",
    "    # for layer in layers_to_freeze:\n",
    "    #     freeze_comma_mlps = partial(freeze_layer_pos_hook, cache=clean_cache, component_type=\"hook_mlp_out\", pos=comma_pos, layer=layer)\n",
    "    #     model.blocks[layer].hook_mlp_out.add_hook(freeze_comma_mlps)\n",
    "\n",
    "    # ablate comma attn values\n",
    "    for layer, head in heads_to_ablate:\n",
    "        ablate_commas = partial(ablate_attn_head_pos_hook, cache=corrupted_cache, ablation_func=None, component_type=\"hook_v\", pos=comma_pos, layer=layer, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_v.add_hook(ablate_commas)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(clean_tokens)\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return get_logit_diff(ablated_logits, answer_tokens).item(), get_logit_diff(ablated_logits, answer_tokens, per_prompt=True), get_logit_diff(clean_logits, answer_tokens, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd420346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original logit diff: 0.8621\n",
      "Post ablation logit diff: 0.2788\n",
      "Logit diff % change: -67.66%\n"
     ]
    }
   ],
   "source": [
    "ablated_logit_diff, ablated_ld_list, clean_ld_list = run_comma_ablation_experiment(heads_to_ablate, heads_to_freeze)\n",
    "model.reset_hooks()\n",
    "\n",
    "print(f\"Original logit diff: {get_logit_diff(clean_logits, answer_tokens).item():.4f}\")\n",
    "print(f\"Post ablation logit diff: {ablated_logit_diff:.4f}\")\n",
    "print(f\"Logit diff % change: {(ablated_logit_diff - get_logit_diff(clean_logits, answer_tokens).item()) / get_logit_diff(clean_logits, answer_tokens).item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea6b5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
